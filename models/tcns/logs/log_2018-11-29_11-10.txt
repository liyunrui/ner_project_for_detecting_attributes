[[11/29/2018 11:10:44 AM]] 
new run with parameters:
{'USE_CHARS': True,
 'batch_size': 128,
 'causal': False,
 'char_representation_method': 'CNN',
 'checkpoint_dir': 'checkpoints_wo_causal_cnn_char',
 'dim_char': 100,
 'dim_word': 300,
 'early_stopping_steps': 3000,
 'enable_parameter_averaging': False,
 'filter_widths': 3,
 'filter_widths_char': 5,
 'grad_clip': 5,
 'hidden_size_char': 100,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.001,
 'log_dir': 'logs',
 'log_interval': 1,
 'loss_averaging_window': 100,
 'max_seq_len': 36,
 'max_word_length': 54,
 'metric': 'f1',
 'min_steps_to_checkpoint': 500,
 'nchars': 77,
 'ntags': 3,
 'num_channels': [300, 250, 200, 150, 100, 50],
 'num_channels_char': [200],
 'num_restarts': 0,
 'num_training_steps': 15000,
 'num_validation_batches': 10,
 'nwords': 24475,
 'optimizer': 'adam',
 'prediction_dir': 'predictions_wo_causal_cnn_char',
 'reader': <__main__.DataReader object at 0x7f694c2262e8>,
 'regularization_constant': 0.0,
 'trainable_embedding': False,
 'use_evaluation_metric_as_early_stopping': True,
 'warm_start_init_step': 0}
[[11/29/2018 11:10:49 AM]] all parameters:
[[11/29/2018 11:10:49 AM]] [('word_embeddings:0', [4276, 300]),
 ('char_embeddings:0', [77, 100]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/weights:0', [5, 100, 200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/g:0', [200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/biases:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/weights:0', [5, 200, 200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/g:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/biases:0', [200]),
 ('char_level/W_h-in-block-1:0', [1, 100, 200]),
 ('char_level/b_h-in-block-1:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/weights:0', [3, 500, 300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/g:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/biases:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/weights:0', [3, 300, 300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/g:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/biases:0', [300]),
 ('word_level/W_h-in-block-1:0', [1, 500, 300]),
 ('word_level/b_h-in-block-1:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/weights:0', [3, 300, 250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/g:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/biases:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/weights:0', [3, 250, 250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/g:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/biases:0', [250]),
 ('word_level/W_h-in-block-2:0', [1, 300, 250]),
 ('word_level/b_h-in-block-2:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/weights:0', [3, 250, 200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/g:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/biases:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/weights:0', [3, 200, 200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/g:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/biases:0', [200]),
 ('word_level/W_h-in-block-3:0', [1, 250, 200]),
 ('word_level/b_h-in-block-3:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/weights:0', [3, 200, 150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/g:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/biases:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/weights:0', [3, 150, 150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/g:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/biases:0', [150]),
 ('word_level/W_h-in-block-4:0', [1, 200, 150]),
 ('word_level/b_h-in-block-4:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/weights:0', [3, 150, 100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/g:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/biases:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/weights:0', [3, 100, 100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/g:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/biases:0', [100]),
 ('word_level/W_h-in-block-5:0', [1, 150, 100]),
 ('word_level/b_h-in-block-5:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/weights:0', [3, 100, 50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/g:0', [50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/biases:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/weights:0', [3, 50, 50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/g:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/biases:0', [50]),
 ('word_level/W_h-in-block-6:0', [1, 100, 50]),
 ('word_level/b_h-in-block-6:0', [50]),
 ('output-layer/weights:0', [50, 3]),
 ('output-layer/biases:0', [3]),
 ('Variable:0', []),
 ('Variable_1:0', []),
 ('beta1_power:0', []),
 ('beta2_power:0', []),
 ('char_embeddings/Adam:0', [77, 100]),
 ('char_embeddings/Adam_1:0', [77, 100]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/weights/Adam:0', [5, 100, 200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/weights/Adam_1:0',
  [5, 100, 200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/g/Adam:0', [200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/g/Adam_1:0', [200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/biases/Adam:0', [200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/biases/Adam_1:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/weights/Adam:0', [5, 200, 200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/weights/Adam_1:0',
  [5, 200, 200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/g/Adam:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/g/Adam_1:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/biases/Adam:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/biases/Adam_1:0', [200]),
 ('char_level/W_h-in-block-1/Adam:0', [1, 100, 200]),
 ('char_level/W_h-in-block-1/Adam_1:0', [1, 100, 200]),
 ('char_level/b_h-in-block-1/Adam:0', [200]),
 ('char_level/b_h-in-block-1/Adam_1:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/weights/Adam:0', [3, 500, 300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/weights/Adam_1:0',
  [3, 500, 300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/g/Adam:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/g/Adam_1:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/biases/Adam:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/biases/Adam_1:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/weights/Adam:0', [3, 300, 300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/weights/Adam_1:0',
  [3, 300, 300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/g/Adam:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/g/Adam_1:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/biases/Adam:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/biases/Adam_1:0', [300]),
 ('word_level/W_h-in-block-1/Adam:0', [1, 500, 300]),
 ('word_level/W_h-in-block-1/Adam_1:0', [1, 500, 300]),
 ('word_level/b_h-in-block-1/Adam:0', [300]),
 ('word_level/b_h-in-block-1/Adam_1:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/weights/Adam:0', [3, 300, 250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/weights/Adam_1:0',
  [3, 300, 250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/g/Adam:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/g/Adam_1:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/biases/Adam:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/biases/Adam_1:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/weights/Adam:0', [3, 250, 250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/weights/Adam_1:0',
  [3, 250, 250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/g/Adam:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/g/Adam_1:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/biases/Adam:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/biases/Adam_1:0', [250]),
 ('word_level/W_h-in-block-2/Adam:0', [1, 300, 250]),
 ('word_level/W_h-in-block-2/Adam_1:0', [1, 300, 250]),
 ('word_level/b_h-in-block-2/Adam:0', [250]),
 ('word_level/b_h-in-block-2/Adam_1:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/weights/Adam:0', [3, 250, 200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/weights/Adam_1:0',
  [3, 250, 200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/g/Adam:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/g/Adam_1:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/biases/Adam:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/biases/Adam_1:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/weights/Adam:0', [3, 200, 200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/weights/Adam_1:0',
  [3, 200, 200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/g/Adam:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/g/Adam_1:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/biases/Adam:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/biases/Adam_1:0', [200]),
 ('word_level/W_h-in-block-3/Adam:0', [1, 250, 200]),
 ('word_level/W_h-in-block-3/Adam_1:0', [1, 250, 200]),
 ('word_level/b_h-in-block-3/Adam:0', [200]),
 ('word_level/b_h-in-block-3/Adam_1:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/weights/Adam:0', [3, 200, 150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/weights/Adam_1:0',
  [3, 200, 150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/g/Adam:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/g/Adam_1:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/biases/Adam:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/biases/Adam_1:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/weights/Adam:0', [3, 150, 150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/weights/Adam_1:0',
  [3, 150, 150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/g/Adam:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/g/Adam_1:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/biases/Adam:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/biases/Adam_1:0', [150]),
 ('word_level/W_h-in-block-4/Adam:0', [1, 200, 150]),
 ('word_level/W_h-in-block-4/Adam_1:0', [1, 200, 150]),
 ('word_level/b_h-in-block-4/Adam:0', [150]),
 ('word_level/b_h-in-block-4/Adam_1:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/weights/Adam:0', [3, 150, 100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/weights/Adam_1:0',
  [3, 150, 100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/g/Adam:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/g/Adam_1:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/biases/Adam:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/biases/Adam_1:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/weights/Adam:0', [3, 100, 100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/weights/Adam_1:0',
  [3, 100, 100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/g/Adam:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/g/Adam_1:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/biases/Adam:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/biases/Adam_1:0', [100]),
 ('word_level/W_h-in-block-5/Adam:0', [1, 150, 100]),
 ('word_level/W_h-in-block-5/Adam_1:0', [1, 150, 100]),
 ('word_level/b_h-in-block-5/Adam:0', [100]),
 ('word_level/b_h-in-block-5/Adam_1:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/weights/Adam:0', [3, 100, 50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/weights/Adam_1:0',
  [3, 100, 50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/g/Adam:0', [50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/g/Adam_1:0', [50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/biases/Adam:0', [50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/biases/Adam_1:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/weights/Adam:0', [3, 50, 50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/weights/Adam_1:0', [3, 50, 50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/g/Adam:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/g/Adam_1:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/biases/Adam:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/biases/Adam_1:0', [50]),
 ('word_level/W_h-in-block-6/Adam:0', [1, 100, 50]),
 ('word_level/W_h-in-block-6/Adam_1:0', [1, 100, 50]),
 ('word_level/b_h-in-block-6/Adam:0', [50]),
 ('word_level/b_h-in-block-6/Adam_1:0', [50]),
 ('output-layer/weights/Adam:0', [50, 3]),
 ('output-layer/weights/Adam_1:0', [50, 3]),
 ('output-layer/biases/Adam:0', [3]),
 ('output-layer/biases/Adam_1:0', [3])]
[[11/29/2018 11:10:49 AM]] trainable parameters:
[[11/29/2018 11:10:49 AM]] [('char_embeddings:0', [77, 100]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/weights:0', [5, 100, 200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/g:0', [200]),
 ('char_level/weightNorm-cnn-layer-1-in-block-1/biases:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/weights:0', [5, 200, 200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/g:0', [200]),
 ('char_level/weightNorm-cnn-layer-2-in-block-1/biases:0', [200]),
 ('char_level/W_h-in-block-1:0', [1, 100, 200]),
 ('char_level/b_h-in-block-1:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/weights:0', [3, 500, 300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/g:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-1/biases:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/weights:0', [3, 300, 300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/g:0', [300]),
 ('word_level/weightNorm-cnn-layer-2-in-block-1/biases:0', [300]),
 ('word_level/W_h-in-block-1:0', [1, 500, 300]),
 ('word_level/b_h-in-block-1:0', [300]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/weights:0', [3, 300, 250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/g:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-2/biases:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/weights:0', [3, 250, 250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/g:0', [250]),
 ('word_level/weightNorm-cnn-layer-2-in-block-2/biases:0', [250]),
 ('word_level/W_h-in-block-2:0', [1, 300, 250]),
 ('word_level/b_h-in-block-2:0', [250]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/weights:0', [3, 250, 200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/g:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-3/biases:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/weights:0', [3, 200, 200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/g:0', [200]),
 ('word_level/weightNorm-cnn-layer-2-in-block-3/biases:0', [200]),
 ('word_level/W_h-in-block-3:0', [1, 250, 200]),
 ('word_level/b_h-in-block-3:0', [200]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/weights:0', [3, 200, 150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/g:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-4/biases:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/weights:0', [3, 150, 150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/g:0', [150]),
 ('word_level/weightNorm-cnn-layer-2-in-block-4/biases:0', [150]),
 ('word_level/W_h-in-block-4:0', [1, 200, 150]),
 ('word_level/b_h-in-block-4:0', [150]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/weights:0', [3, 150, 100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/g:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-5/biases:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/weights:0', [3, 100, 100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/g:0', [100]),
 ('word_level/weightNorm-cnn-layer-2-in-block-5/biases:0', [100]),
 ('word_level/W_h-in-block-5:0', [1, 150, 100]),
 ('word_level/b_h-in-block-5:0', [100]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/weights:0', [3, 100, 50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/g:0', [50]),
 ('word_level/weightNorm-cnn-layer-1-in-block-6/biases:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/weights:0', [3, 50, 50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/g:0', [50]),
 ('word_level/weightNorm-cnn-layer-2-in-block-6/biases:0', [50]),
 ('word_level/W_h-in-block-6:0', [1, 100, 50]),
 ('word_level/b_h-in-block-6:0', [50]),
 ('output-layer/weights:0', [50, 3]),
 ('output-layer/biases:0', [3])]
[[11/29/2018 11:10:49 AM]] trainable parameter count:
[[11/29/2018 11:10:49 AM]] 2316603
[[11/29/2018 11:11:10 AM]] [[step        0]]     [[train]]     loss: 1.25935078       [[val]]     score: 0.0              
[[11/29/2018 11:11:13 AM]] [[step        1]]     [[train]]     loss: 1.15877664       [[val]]     score: 0.0              
[[11/29/2018 11:11:16 AM]] [[step        2]]     [[train]]     loss: 0.99688756       [[val]]     score: 0.0              
[[11/29/2018 11:11:18 AM]] [[step        3]]     [[train]]     loss: 0.91098711       [[val]]     score: 0.0              
[[11/29/2018 11:11:20 AM]] [[step        4]]     [[train]]     loss: 0.78397498       [[val]]     score: 0.0              
[[11/29/2018 11:11:21 AM]] [[step        5]]     [[train]]     loss: 0.73532431       [[val]]     score: 0.0              
[[11/29/2018 11:11:23 AM]] [[step        6]]     [[train]]     loss: 0.67329546       [[val]]     score: 0.0              
[[11/29/2018 11:11:25 AM]] [[step        7]]     [[train]]     loss: 0.62836915       [[val]]     score: 0.0              
[[11/29/2018 11:11:26 AM]] [[step        8]]     [[train]]     loss: 0.5956281        [[val]]     score: 0.0              
[[11/29/2018 11:11:28 AM]] [[step        9]]     [[train]]     loss: 0.56614386       [[val]]     score: 0.0              
[[11/29/2018 11:11:29 AM]] [[step       10]]     [[train]]     loss: 0.53996123       [[val]]     score: 0.0              
[[11/29/2018 11:11:31 AM]] [[step       11]]     [[train]]     loss: 0.51986438       [[val]]     score: 0.0              
[[11/29/2018 11:11:32 AM]] [[step       12]]     [[train]]     loss: 0.50314024       [[val]]     score: 0.0              
[[11/29/2018 11:11:34 AM]] [[step       13]]     [[train]]     loss: 0.4855698        [[val]]     score: 0.0              
[[11/29/2018 11:11:35 AM]] [[step       14]]     [[train]]     loss: 0.47015648       [[val]]     score: 0.0              
[[11/29/2018 11:11:37 AM]] [[step       15]]     [[train]]     loss: 0.45617113       [[val]]     score: 0.0              
[[11/29/2018 11:11:38 AM]] [[step       16]]     [[train]]     loss: 0.44232983       [[val]]     score: 0.0              
[[11/29/2018 11:11:40 AM]] [[step       17]]     [[train]]     loss: 0.4307861        [[val]]     score: 0.0              
[[11/29/2018 11:11:42 AM]] [[step       18]]     [[train]]     loss: 0.419643         [[val]]     score: 0.0              
[[11/29/2018 11:11:44 AM]] [[step       19]]     [[train]]     loss: 0.4104933        [[val]]     score: 0.0              
[[11/29/2018 11:11:45 AM]] [[step       20]]     [[train]]     loss: 0.4014517        [[val]]     score: 0.0              
[[11/29/2018 11:11:47 AM]] [[step       21]]     [[train]]     loss: 0.3916876        [[val]]     score: 0.00132867       
[[11/29/2018 11:11:48 AM]] [[step       22]]     [[train]]     loss: 0.38326435       [[val]]     score: 0.02293749       
[[11/29/2018 11:11:50 AM]] [[step       23]]     [[train]]     loss: 0.37443469       [[val]]     score: 0.04541258       
[[11/29/2018 11:11:51 AM]] [[step       24]]     [[train]]     loss: 0.36673522       [[val]]     score: 0.06682188       
[[11/29/2018 11:11:53 AM]] [[step       25]]     [[train]]     loss: 0.35949351       [[val]]     score: 0.08563867       
[[11/29/2018 11:11:54 AM]] [[step       26]]     [[train]]     loss: 0.35332392       [[val]]     score: 0.10376134       
[[11/29/2018 11:11:55 AM]] [[step       27]]     [[train]]     loss: 0.34610963       [[val]]     score: 0.12148414       
[[11/29/2018 11:11:57 AM]] [[step       28]]     [[train]]     loss: 0.33837411       [[val]]     score: 0.14046157       
[[11/29/2018 11:11:58 AM]] [[step       29]]     [[train]]     loss: 0.33243969       [[val]]     score: 0.16097452       
[[11/29/2018 11:12:00 AM]] [[step       30]]     [[train]]     loss: 0.32535867       [[val]]     score: 0.18070278       
[[11/29/2018 11:12:01 AM]] [[step       31]]     [[train]]     loss: 0.31878306       [[val]]     score: 0.20038622       
[[11/29/2018 11:12:02 AM]] [[step       32]]     [[train]]     loss: 0.3116218        [[val]]     score: 0.21991767       
