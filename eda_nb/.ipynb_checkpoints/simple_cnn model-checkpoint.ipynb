{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../models/'))\n",
    "sys.path.append('../models')\n",
    "from data_frame import DataFrame\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "denominator = math.exp(3) + math.exp(1) + math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8668133321973349"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(3) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11731042782619835"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(1) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015876239976466765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-1) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999986"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8668+0.11731+0.015876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7297014486341915"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "math.log(0.024) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07257069283483537"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.93) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data = [np.load(os.path.join(data_dir, '{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        #\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print ('shape of whole data : {}'.format(self.test_df.shapes()))\n",
    "        print ('loaded data')\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = int(time.time()))\n",
    "\n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size,shuffle = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of whole data : word_id           (24028, 122)\n",
      "history_length        (24028,)\n",
      "label             (24028, 122)\n",
      "dtype: object\n",
      "loaded data\n",
      "number of training example: 21625\n",
      "number of validating example: 2403\n",
      "number of testing example: 24028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "TRACE_CODE = False\n",
    "reader = DataReader(data_dir ='../models/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = np.load(os.path.join('../models/data/', '{}.npy'.format('word_id')))\n",
    "history_length = np.load(os.path.join('../models/data/', '{}.npy'.format('history_length')))\n",
    "label = np.load(os.path.join('../models/data/', '{}.npy'.format('label')))\n",
    "item_id = np.load(os.path.join('../models/data/','{}.npy'.format('item_id')))\n",
    "eval_set = np.load(os.path.join('../models/data/','{}.npy'.format('eval_set')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24028, 122)\n",
      "(24028, 122)\n",
      "(24028,)\n",
      "(24028,)\n",
      "(24028,)\n"
     ]
    }
   ],
   "source": [
    "print (label.shape)\n",
    "print (word_id.shape)\n",
    "print (history_length.shape)\n",
    "print (item_id.shape)\n",
    "print (eval_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3228,  299,  137, 9411, 9022, 9115, 4120, 6375, 3053, 1141, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
       "       9858], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = word_id[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = label[0]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = history_length[0]\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>is_brand</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>item_id</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>Blackview</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>blackview</td>\n",
       "      <td>2</td>\n",
       "      <td>3228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>BV8000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>bv8000</td>\n",
       "      <td>2</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>Pro</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>pro</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>RAM</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>ram</td>\n",
       "      <td>2</td>\n",
       "      <td>9411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>6GB</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>6gb</td>\n",
       "      <td>2</td>\n",
       "      <td>9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>64GB</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>64gb</td>\n",
       "      <td>2</td>\n",
       "      <td>9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>IP68</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>ip68</td>\n",
       "      <td>2</td>\n",
       "      <td>4120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>Waterproff</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>waterproff</td>\n",
       "      <td>2</td>\n",
       "      <td>6375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>Rugged</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>rugged</td>\n",
       "      <td>2</td>\n",
       "      <td>3053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>smartphone</td>\n",
       "      <td>2</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            item_name      tokens  is_brand  \\\n",
       "9        Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...   Blackview         2   \n",
       "10       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...      BV8000         0   \n",
       "11       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...         Pro         0   \n",
       "12       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...         RAM         0   \n",
       "13       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...         6GB         0   \n",
       "14       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...        64GB         0   \n",
       "15       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...        IP68         0   \n",
       "16       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...  Waterproff         0   \n",
       "17       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...      Rugged         0   \n",
       "18       Blackview BV8000 Pro RAM 6GB 64GB IP68 Wa...  Smartphone         0   \n",
       "\n",
       "   is_valid clean_tokens  item_id  word_id  \n",
       "9     train    blackview        2     3228  \n",
       "10    train       bv8000        2      299  \n",
       "11    train          pro        2      137  \n",
       "12    train          ram        2     9411  \n",
       "13    train          6gb        2     9022  \n",
       "14    train         64gb        2     9115  \n",
       "15    train         ip68        2     4120  \n",
       "16    train   waterproff        2     6375  \n",
       "17    train       rugged        2     3053  \n",
       "18    train   smartphone        2     1141  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/mobile_training_w_word_id.csv')\n",
    "df[df.item_id == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "def shape_of_tensor(tensor, dim=None):\n",
    "    \"\"\"\n",
    "    Get tensor shape/dimension as list/int\n",
    "    ======\n",
    "    Args:\n",
    "        tensor: tensor in tensorflow\n",
    "        dim: int, the dimension of this tensor\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        # return list\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        # t\n",
    "        return tensor.shape.as_list()[dim]\n",
    "    \n",
    "def temporal_convolution_layer(inputs, output_units, convolution_width = 3, dilated = False,\n",
    "                               causal=False, dilation_rate=[1], bias=True, activation=None, \n",
    "                               dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilated: Simple CNN or Dilated CNN\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "        scope: str.\n",
    "        reuse: boolean. If we would like to sharing this weight.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "         if dilated == True:\n",
    "            #-------------------\n",
    "            # dilated CNN\n",
    "            #-------------------\n",
    "            if causal:\n",
    "                # padding zero in the left side of sequence with (k-1) *d\n",
    "                shift = int((convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)) # modified to int\n",
    "                pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "                inputs = tf.concat([pad, inputs], axis=1)\n",
    "            \n",
    "            # filter weight\n",
    "            W = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "            )\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "            # adding bias if True\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "            # adding non-linear output if True\n",
    "            z = activation(z) if activation else z\n",
    "            # adding dropout if True\n",
    "            z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "            # output tensor of this hidden layer\n",
    "            z = z[:, :-shift, :] if causal else z\n",
    "        else:\n",
    "            #-------------------\n",
    "            # simple CNN\n",
    "            #-------------------\n",
    "            \n",
    "            # filter weight\n",
    "            W = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                # shape = spatial_filter_shape + [in_channels, out_channels]\n",
    "                shape=[convolution_width, shape_of_tensor(inputs, 2), output_units]\n",
    "            )\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(input = inputs, \n",
    "                          filter = W, \n",
    "                          padding='SAME', \n",
    "                          )\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "\n",
    "            # adding non-linear output if True\n",
    "            z = activation(z) if activation else z\n",
    "            # adding dropout if True\n",
    "            z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "            # output tensor of this hidden layer\n",
    "            z = z[:, :-shift, :] if causal else z\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    \n",
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "\n",
    "    output_units aka residual_channels: Number of channels to use for residual connections.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[shape_of_tensor(inputs, -1), output_units]\n",
    "        )\n",
    "        # matrix multiplication\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        # doing batch_norm before activation is better for training nn.\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "        #--------\n",
    "        # In practice, activation fisst and then dropout\n",
    "        #----------\n",
    "        # adding non-linear output if True\n",
    "        z = activation(z) if activation else z\n",
    "        # adding dropout if True\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n",
    "\n",
    "def sequence_softmax_loss(y, y_hat, sequence_lengths, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Calculates average softmax cross entropy on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length], which should be index of label.\n",
    "        y_hat: Prediction tensor, [batch size, max_sequence_length, num_class], which should be unscaled score.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        batch_softmax_loss. 0-dimensional tensor.\n",
    "        \n",
    "    Reference:\n",
    "        -Official: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "        -Blog : https://blog.csdn.net/yc461515457/article/details/77861695\n",
    "    \"\"\"\n",
    "    # softmax cross-entropy between y(y_true) and y_hat(y_pred)\n",
    "    softmax_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= y_hat) # (?, max_sequence_length)\n",
    "    # returns a boolean mask tensor for the first N positions of each cell.\n",
    "    sequence_mask = tf.sequence_mask(lengths = sequence_lengths, maxlen=max_sequence_length) # (?, max_sequence_length)\n",
    "    # convert boolean into 1 or 0\n",
    "    sequence_mask = tf.cast(sequence_mask, tf.float32) # (?, max_sequence_length)\n",
    "    # compute sum of loss over each timestep / number of real training example of this batch (aka batch loss)\n",
    "    batch_softmax_loss = tf.reduce_sum(softmax_losses*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return batch_softmax_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, optimizer='adam'):\n",
    "    '''\n",
    "    It's for choosing optimizer given learning rate.\n",
    "    '''\n",
    "    if optimizer == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    elif soptimizer == 'rms':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        # assert is a good way to tell other how to use this function for bug happening.\n",
    "        #-------\n",
    "        # standard way to pring the error\n",
    "        #-------\n",
    "        assert False, 'optimizer must be adam, adagrad, sgd, or rms'\n",
    "\n",
    "def update_parameters(loss, optimizer = 'adam'):\n",
    "    '''\n",
    "    It's for optimizing and logging training parameters\n",
    "    \n",
    "    1.using gradient clipping to avoid gradient explosion and vanishment.\n",
    "    \n",
    "    Gradient clipping is most common in recurrent neural networks. \n",
    "    When gradients are being propagated back in time, they can vanish \n",
    "    because they they are continuously multiplied by numbers less than one.\n",
    "    This is called the vanishing gradient problem. \n",
    "    This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, \n",
    "    This is solved by residual connections. \n",
    "    On the other hand, you can have exploding gradients too. \n",
    "    This is when they get exponentially large from being multiplied by numbers larger \n",
    "    than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    grad_clip = 5 # Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "    regularization_constant = 0.1 # Regularization constant applied to all trainable parameters.\n",
    "    enable_parameter_averaging = False # If true, model saves exponential weighted averages of parameters to separate checkpoint file.\n",
    "    global_step = tf.Variable(0, trainable = False) # Optional Variable to increment by one after the variables have been updated.\n",
    "    learning_rate_var = tf.Variable(0.0, trainable = False)\n",
    "    \n",
    "    #----------------\n",
    "    # for understanding regularization\n",
    "    #----------------\n",
    "    trainable_variables_1 = tf.trainable_variables()[0]\n",
    "    square_1 = tf.square(trainable_variables_1)\n",
    "    sum_1 = tf.reduce_sum(square_1)\n",
    "    sqrt = tf.sqrt(sum_1)\n",
    "    #-----------------\n",
    "    # we can customized our regularization on the parameters we like\n",
    "    #-----------------\n",
    "    if regularization_constant != 0:\n",
    "        # l2_norm: is a 0-D tensor. \n",
    "        # we do l2-norm on each trainable's parameters.\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()]) # Returns list including all variables created with trainable=True\n",
    "        # the smaller the loss is, the better do finish overfitting \n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "    #-----------------\n",
    "    # optimizing\n",
    "    #-----------------\n",
    "    # define the optimizer\n",
    "    optimizer = get_optimizer(learning_rate_var, optimizer=optimizer)\n",
    "    # compute grads: return A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    # standard way to do gradient clipping\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "    step = optimizer.apply_gradients(clipped, global_step = global_step)\n",
    "    print ('step - whtat optimizer.apply_gradients returns', step)\n",
    "    #-----------------\n",
    "    # if using moving average techniques\n",
    "    #-----------------\n",
    "    if enable_parameter_averaging:\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "        maintain_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([step]):\n",
    "            step = tf.group(maintain_averages_op)\n",
    "    else:\n",
    "        step = step\n",
    "    #--------------\n",
    "    # logging\n",
    "    #--------------\n",
    "    logging.info('all parameters:')\n",
    "    logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.global_variables()]))\n",
    "\n",
    "    logging.info('trainable parameters:')\n",
    "    logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.trainable_variables()]))\n",
    "\n",
    "    logging.info('trainable parameter count:')\n",
    "    logging.info(str(np.sum(np.prod(shape_of_tensor(var)) for var in tf.trainable_variables())))\n",
    "    return grads, clipped , step, global_step, learning_rate_var\n",
    "\n",
    "def save(step, averaged=False):\n",
    "    '''\n",
    "    save model\n",
    "    \n",
    "    Args:\n",
    "        step: number checkpoint filenames by passing a value of step\n",
    "        averaged:\n",
    "    '''\n",
    "    global saver\n",
    "    global checkpoint_dir\n",
    "    #---------\n",
    "    # determine using which saver object\n",
    "    #---------\n",
    "    saver = saver_averaged if averaged else saver\n",
    "    checkpoint_dir = checkpoint_dir_averaged if averaged else checkpoint_dir\n",
    "    #---------\n",
    "    #checkpoint_dir\n",
    "    #---------\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        logging.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    model_path = os.path.join(checkpoint_dir, 'model')\n",
    "    print ('========model_path in save========', model_path)\n",
    "    logging.info('saving model to {}'.format(model_path))\n",
    "    #saving\n",
    "    saver.save(sess, model_path, global_step=step)\n",
    "    \n",
    "def restore(step=None, averaged=False):\n",
    "    '''\n",
    "    For using warm start technique\n",
    "    '''\n",
    "    global saver\n",
    "    # checkpoint_dir: Directory where checkpoints are saved\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "\n",
    "    saver = saver_averaged if averaged else saver\n",
    "    checkpoint_dir = checkpoint_dir_averaged if averaged else checkpoint_dir\n",
    "    if not step: \n",
    "        model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        print ('========model_path in restore========', model_path)\n",
    "        logging.info('restoring model parameters from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n",
    "    else:\n",
    "        model_path = os.path.join(\n",
    "            checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "        )\n",
    "\n",
    "        logging.info('restoring model from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCase1:Softmax\\n1.輸出3維度的藍色點點\\n3.Softmax\\n\\n\\nCase2:CRF\\n1.輸出3為度的藍色點點\\n2.CRF\\n\\nPrediction\\n取最大維度的值\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Case1:Softmax\n",
    "1.輸出3維度的藍色點點\n",
    "3.Softmax\n",
    "\n",
    "\n",
    "Case2:CRF\n",
    "1.輸出3為度的藍色點點\n",
    "2.CRF\n",
    "\n",
    "Prediction\n",
    "取最大維度的值\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "sys.path.append('/home/ld-sgdev/yunrui_li/ner_project/brand_recognition_bio_FE/py_model')\n",
    "from utils import init_logging\n",
    "import logging\n",
    "log_dir = 'log/' # log path\n",
    "init_logging(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features : (?, 122, 300)\n",
      "CNN-0 layer : (?, 122, 100)\n",
      "CNN-1 layer : (?, 122, 100)\n",
      "Output layer : (?, 122, 3)\n",
      "y_true : (?, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all parameters:\n",
      "[('word_embeddings:0', [10577, 300]),\n",
      " ('cnn-0/weights:0', [3, 300, 100]),\n",
      " ('cnn-0/biases:0', [100]),\n",
      " ('cnn-1/weights:0', [3, 100, 100]),\n",
      " ('cnn-1/biases:0', [100]),\n",
      " ('output-layer/weights:0', [100, 3]),\n",
      " ('output-layer/biases:0', [3]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('cnn-0/weights/Adam:0', [3, 300, 100]),\n",
      " ('cnn-0/weights/Adam_1:0', [3, 300, 100]),\n",
      " ('cnn-0/biases/Adam:0', [100]),\n",
      " ('cnn-0/biases/Adam_1:0', [100]),\n",
      " ('cnn-1/weights/Adam:0', [3, 100, 100]),\n",
      " ('cnn-1/weights/Adam_1:0', [3, 100, 100]),\n",
      " ('cnn-1/biases/Adam:0', [100]),\n",
      " ('cnn-1/biases/Adam_1:0', [100]),\n",
      " ('output-layer/weights/Adam:0', [100, 3]),\n",
      " ('output-layer/weights/Adam_1:0', [100, 3]),\n",
      " ('output-layer/biases/Adam:0', [3]),\n",
      " ('output-layer/biases/Adam_1:0', [3])]\n",
      "trainable parameters:\n",
      "[('cnn-0/weights:0', [3, 300, 100]),\n",
      " ('cnn-0/biases:0', [100]),\n",
      " ('cnn-1/weights:0', [3, 100, 100]),\n",
      " ('cnn-1/biases:0', [100]),\n",
      " ('output-layer/weights:0', [100, 3]),\n",
      " ('output-layer/biases:0', [3])]\n",
      "trainable parameter count:\n",
      "120503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_word = 300\n",
    "max_seq_length = 122\n",
    "num_layers = 2\n",
    "hidden_size_cnn = 100\n",
    "k = 3\n",
    "ntags = 3 # n_class\n",
    "USE_PRETRAINED = True\n",
    "trainable_embedding = False\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "gc.collect()\n",
    "with tf.Graph().as_default() as g:\n",
    "    ####################################\n",
    "    # Step1: get input_sequences \n",
    "    ####################################\n",
    "\n",
    "    #------------\n",
    "    # 1-D  \n",
    "    #------------\n",
    "    item_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "    #------------   \n",
    "    # 2-D  \n",
    "    #------------\n",
    "    word_id = tf.placeholder(tf.int32, [None, max_seq_length]) \n",
    "    label = tf.placeholder(tf.int32, [None, max_seq_length]) # [batch_size, num_class]\n",
    "\n",
    "    #------------\n",
    "    # boolean parameter\n",
    "    #------------\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #------------\n",
    "    # word_embedding: get char embeddings matrix\n",
    "    #------------\n",
    "    if embeddings is None:\n",
    "        logging.info('WARNING: randomly initializing word vectors')\n",
    "        word_embeddings = tf.get_variable(\n",
    "        shape = [nwords, dim_word],\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32,\n",
    "        )\n",
    "    else:\n",
    "        word_embeddings = tf.get_variable(\n",
    "        initializer = embeddings, # it will hold the embedding\n",
    "        #shape = [word2vec.shape[0], word2vec.shape[1]], # [num_vocabulary, embeddings_dim]\n",
    "        trainable = trainable_embedding,\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32\n",
    "        )\n",
    "    word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = word_id)\n",
    "    x_word = tf.concat([\n",
    "    word_representation,\n",
    "    # tf_idf:for product_name\n",
    "        ], axis=1) # (?, 122, 300)\n",
    "    \n",
    "    ####################################\n",
    "    # Step2: calculate_outputs \n",
    "    ####################################\n",
    "    \n",
    "    #-------------------------\n",
    "    # NN architecuture-Simple CNN\n",
    "    #-------------------------\n",
    "    print ('Original features : {}'.format(x_word.shape))\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            conv = temporal_convolution_layer(x_word, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "        else:\n",
    "            conv = temporal_convolution_layer(conv, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "            \n",
    "        print ('CNN-{} layer : {}'.format(i, conv.shape))\n",
    "    # output layer (linear)\n",
    "    y_hat = time_distributed_dense_layer(conv, ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "    print ('Output layer : {}'.format(y_hat.shape))\n",
    "    print ('y_true : {}'.format(label.shape))\n",
    "    #--------------\n",
    "    # for second-level model\n",
    "    #--------------\n",
    "    prediction_tensors = {\n",
    "        'item_id':item_id,\n",
    "        'word_id':word_id,\n",
    "        'final_states':conv,\n",
    "        'final_predictions':y_hat,\n",
    "    }\n",
    "    \n",
    "    ####################################\n",
    "    # Step3: calculate_loss + optimizing\n",
    "    ####################################\n",
    "    loss,softmax_losses,sequence_mask = sequence_softmax_loss(y = label, y_hat = y_hat, sequence_lengths = history_length, max_sequence_length = max_seq_length)\n",
    "    grads, clipped , step, global_step, learning_rate_var  = update_parameters(loss)\n",
    "    ####################################\n",
    "    # Step4: saving the model \n",
    "    ####################################    \n",
    "    # create saver object\n",
    "    # max_to_keep: indicates the maximum number of recent checkpoint files to keep.\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    if enable_parameter_averaging:\n",
    "        saver_averaged = tf.train.Saver(ema.variables_to_restore(), max_to_keep=1)    \n",
    "\n",
    "    #-------------------------\n",
    "    # standard\n",
    "    #-------------------------\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n-test time_distributed_dense_layer\\n-test sequence_softmax_loss\\n-understanding how to determine grad_clip==> empirical\\n-understanding save and restore and fit function, and make them start training\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "-test time_distributed_dense_layer\n",
    "-test sequence_softmax_loss\n",
    "-understanding how to determine grad_clip==> empirical\n",
    "-understanding save and restore and fit function, and make them start training\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 4.43673849       [[val]]     loss: 4.66601419       \n",
      "[[step        1]]     [[train]]     loss: 4.54418898       [[val]]     loss: 4.54078436       \n",
      "[[step        2]]     [[train]]     loss: 4.56940524       [[val]]     loss: 4.59598207       \n",
      "[[step        3]]     [[train]]     loss: 4.61508822       [[val]]     loss: 4.5304085        \n",
      "[[step        4]]     [[train]]     loss: 4.65484228       [[val]]     loss: 4.57273169       \n",
      "[[step        5]]     [[train]]     loss: 4.662407         [[val]]     loss: 4.58390872       \n",
      "[[step        6]]     [[train]]     loss: 4.68201889       [[val]]     loss: 4.64834118       \n",
      "[[step        7]]     [[train]]     loss: 4.69493777       [[val]]     loss: 4.68718445       \n",
      "[[step        8]]     [[train]]     loss: 4.67208359       [[val]]     loss: 4.7309653        \n",
      "[[step        9]]     [[train]]     loss: 4.67819839       [[val]]     loss: 4.72593021       \n",
      "num_training_steps reached - ending training\n",
      "restoring model parameters from checkpoints/model-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 4.666014\n",
      "train_loss 4.4367385\n",
      "avg_train_loss 4.43673849105835\n",
      "avg_val_loss 4.666014194488525\n",
      "**********step********** 1\n",
      "val_loss 4.4155545\n",
      "train_loss 4.6516395\n",
      "avg_train_loss 4.544188976287842\n",
      "avg_val_loss 4.5407843589782715\n",
      "**********step********** 2\n",
      "val_loss 4.7063775\n",
      "train_loss 4.6198378\n",
      "avg_train_loss 4.569405237833659\n",
      "avg_val_loss 4.595982074737549\n",
      "**********step********** 3\n",
      "val_loss 4.333688\n",
      "train_loss 4.752137\n",
      "avg_train_loss 4.615088224411011\n",
      "avg_val_loss 4.530408501625061\n",
      "**********step********** 4\n",
      "val_loss 4.7420244\n",
      "train_loss 4.8138585\n",
      "avg_train_loss 4.654842281341553\n",
      "avg_val_loss 4.572731685638428\n",
      "**********step********** 5\n",
      "val_loss 4.639794\n",
      "train_loss 4.7002306\n",
      "avg_train_loss 4.662407000859578\n",
      "avg_val_loss 4.583908716837565\n",
      "**********step********** 6\n",
      "val_loss 5.034936\n",
      "train_loss 4.7996902\n",
      "avg_train_loss 4.682018893105643\n",
      "avg_val_loss 4.648341178894043\n",
      "**********step********** 7\n",
      "val_loss 4.9590874\n",
      "train_loss 4.78537\n",
      "avg_train_loss 4.694937765598297\n",
      "avg_val_loss 4.687184453010559\n",
      "**********step********** 8\n",
      "val_loss 5.081212\n",
      "train_loss 4.48925\n",
      "avg_train_loss 4.67208358976576\n",
      "avg_val_loss 4.730965296427409\n",
      "**********step********** 9\n",
      "val_loss 4.6806145\n",
      "train_loss 4.7332315\n",
      "avg_train_loss 4.678198385238647\n",
      "avg_val_loss 4.725930213928223\n",
      "**********step********** 10\n",
      "========model_path in restore======== checkpoints/model-6\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from checkpoints/model-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test_feed_dict {<tf.Tensor 'Placeholder_2:0' shape=(?, 122) dtype=int32>: array([[3228,  299,  137, 9411, 9022, 9115, 4120, 6375, 3053, 1141, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858],\n",
      "       [9432, 1466, 4211, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858],\n",
      "       [7522, 1358, 2823, 9858, 9269, 6383, 9858, 2081, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858, 9858,\n",
      "        9858]], dtype=int32), <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>: array([10,  0,  0], dtype=int8), <tf.Tensor 'Placeholder_3:0' shape=(?, 122) dtype=int32>: array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8), <tf.Variable 'Variable_1:0' shape=() dtype=float32_ref>: 0.001}\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype int32 and shape [?]\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: cnn-1/add/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_79_cnn-1/add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-132-3a61314b85b9>\", line 25, in <module>\n    item_id = tf.placeholder(tf.int32, [None])\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int32 and shape [?]\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: cnn-1/add/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_79_cnn-1/add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype int32 and shape [?]\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: cnn-1/add/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_79_cnn-1/add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-5f8aa8bd62ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         np_tensors = sess.run(\n\u001b[1;32m    209\u001b[0m             \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_feed_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         )  # return list\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype int32 and shape [?]\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: cnn-1/add/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_79_cnn-1/add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-132-3a61314b85b9>\", line 25, in <module>\n    item_id = tf.placeholder(tf.int32, [None])\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int32 and shape [?]\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: cnn-1/add/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_79_cnn-1/add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque # for computing Train/validation losses are averaged over the last loss_averaging_window\n",
    "\n",
    "warm_start_init_step = 0 # If nonzero, model will resume training a restored model beginning at warm_start_init_step.\n",
    "batch_size = 128\n",
    "loss_averaging_window = 10\n",
    "num_validation_batches = 1\n",
    "num_training_steps = 10\n",
    "learning_rate=0.001\n",
    "log_interval = 1\n",
    "min_steps_to_checkpoint =5\n",
    "early_stopping_steps = 10\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    ####################################\n",
    "    # 1. fit\n",
    "    ####################################\n",
    "    if warm_start_init_step:\n",
    "        # continue the optimization at a recent checkpoint instead of having to restart the optimization from the beginning\n",
    "        restore(warm_start_init_step)\n",
    "        step = warm_start_init_step\n",
    "    else:\n",
    "        # start the optimization from the beginning\n",
    "        sess.run(init) # Run the initializer\n",
    "        step = 0\n",
    "        \n",
    "    sess.run(init)\n",
    "    #--------------\n",
    "    # create generator for batch training\n",
    "    #--------------\n",
    "    train_generator = reader.train_batch_generator(batch_size) # it will yield our customized dataframe\n",
    "    val_generator = reader.val_batch_generator(num_validation_batches*batch_size)\n",
    "    \n",
    "    # create deque object (list-like): for only considering last 100 training steps\n",
    "    train_loss_history = deque(maxlen = loss_averaging_window)\n",
    "    val_loss_history = deque(maxlen= loss_averaging_window)\n",
    "\n",
    "    best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "    restarts = 0\n",
    "    while step < num_training_steps:\n",
    "        #-----------------------\n",
    "        # validating \n",
    "        #-----------------------\n",
    "        val_batch_df = next(val_generator) # it's also a generator.\n",
    "\n",
    "        val_feed_dict = {}\n",
    "\n",
    "            \n",
    "        for placeholder_name, data in val_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                if placeholder_name in vars():\n",
    "                    val_feed_dict.update({globals()[placeholder_name]: data})\n",
    "        val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "\n",
    "    \n",
    "        if keep_prob is not None:\n",
    "            val_feed_dict.update({keep_prob: 1.0}) # 在validating 和testing時, keep_prob:1.0\n",
    "\n",
    "        if is_training is not None:\n",
    "            val_feed_dict.update({is_training: False})\n",
    "        # feeding data into the graph we designed\n",
    "        [val_loss] = sess.run(\n",
    "            fetches=[loss],\n",
    "            feed_dict=val_feed_dict\n",
    "        )\n",
    "        print ('val_loss', val_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "     \n",
    "        #-----------------------\n",
    "        # training\n",
    "        #-----------------------\n",
    "        train_batch_df = next(train_generator)\n",
    "        train_feed_dict = {}\n",
    "        \n",
    "        for placeholder_name, data in train_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                #print ('placeholder_name === train', placeholder_name)\n",
    "                if placeholder_name in vars():\n",
    "                    #print ('placeholder_name === train',globals()[placeholder_name])\n",
    "                    train_feed_dict.update({globals()[placeholder_name]: data})\n",
    "\n",
    "        train_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        if keep_prob is not None:\n",
    "            train_feed_dict.update({keep_prob: 1.0})\n",
    "        if is_training is not None:\n",
    "            train_feed_dict.update({is_training: True})\n",
    "\n",
    "        [train_loss] = sess.run(\n",
    "            fetches=[loss],\n",
    "            feed_dict=train_feed_dict\n",
    "        )\n",
    "        print ('train_loss', train_loss)\n",
    "        train_loss_history.append(train_loss)\n",
    "    \n",
    "        #----------\n",
    "        # logging: log the training and validating loss every log_interval training steps\n",
    "        #----------\n",
    "        if step % log_interval == 0:\n",
    "            # compute average batch training loss over the last loss_averaging_window steps\n",
    "            avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "            print ('avg_train_loss', avg_train_loss)\n",
    "            avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "            print ('avg_val_loss', avg_val_loss)\n",
    "            metric_log = (\n",
    "                \"[[step {:>8}]]     \"\n",
    "                \"[[train]]     loss: {:<12}     \"\n",
    "                \"[[val]]     loss: {:<12}     \"\n",
    "            ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "            logging.info(metric_log)\n",
    "\n",
    "            #------------------\n",
    "            # early stopping\n",
    "            #------------------\n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                # update the best_validation_loss\n",
    "                best_validation_loss = avg_val_loss\n",
    "                best_validation_tstep = step # best_validation_tstep: for recording the model_path in restore function\n",
    "                if step > min_steps_to_checkpoint:\n",
    "                    # saving \n",
    "                    save(step)\n",
    "                    if enable_parameter_averaging:\n",
    "                        save(step, averaged=True)\n",
    "\n",
    "            if step - best_validation_tstep > early_stopping_steps:\n",
    "                #----------------\n",
    "                # handling loss plateaus:  halving earning rate will be repeated more times. \n",
    "                #----------------\n",
    "                if num_restarts is None or restarts >=  num_restarts:\n",
    "                    # stop training\n",
    "                    logging.info('best validation loss of {} at training step {}'.format(\n",
    "                        best_validation_loss, best_validation_tstep))\n",
    "                    logging.info('early stopping - ending training.')\n",
    "\n",
    "                if restarts < num_restarts:\n",
    "                    # 1.the best checkpoint will be restored.\n",
    "                    # 2.keep traning but halving the learning rate and early_stopping_steps\n",
    "                    restore(best_validation_tstep)\n",
    "                    logging.info('halving learning rate')\n",
    "                    learning_rate /= 2.0\n",
    "                    early_stopping_steps /= 2\n",
    "                    step = best_validation_tstep # start from the best_validation_tstep\n",
    "                    restarts += 1\n",
    "        # for terminating while\n",
    "        step += 1\n",
    "        print ('**********step**********', step)\n",
    "    #----------------\n",
    "    # for the case: num_training_steps < = min_steps_to_checkpoint(Basically, it won't happen)\n",
    "    #----------------\n",
    "    if step <= min_steps_to_checkpoint:\n",
    "        best_validation_tstep = step\n",
    "        save(step)\n",
    "        if enable_parameter_averaging:\n",
    "            save(step, averaged=True)\n",
    "\n",
    "    logging.info('num_training_steps reached - ending training')\n",
    "\n",
    "    ####################################\n",
    "    # 2.restore\n",
    "    ####################################\n",
    "    restore()\n",
    "    ####################################\n",
    "    # 3.predict\n",
    "    ####################################\n",
    "    \n",
    "    chunk_size = 3\n",
    "    prediction_dir = 'predictions'\n",
    "    if not os.path.isdir(prediction_dir):\n",
    "        os.makedirs(prediction_dir)\n",
    "    \n",
    "#     #--------------------\n",
    "#     # for input of 2nd-level\n",
    "#     #--------------------\n",
    "#     prediction_dict = {}\n",
    "#     for tensor_name, value in prediction_tensors.items():\n",
    "#         prediction_dict.update({tensor_name: []})\n",
    "        \n",
    "    test_generator = reader.test_batch_generator(chunk_size)\n",
    "    for i, test_batch_df in enumerate(test_generator):\n",
    "        if i % 100 == 0:\n",
    "            print (i*chunk_size)\n",
    "\n",
    "        #--------------\n",
    "        # preparing feed_dict\n",
    "        #--------------\n",
    "        test_feed_dict = {}\n",
    "        for placeholder_name, data in test_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                #print ('placeholder_name === val', placeholder_name)\n",
    "                if placeholder_name in vars():\n",
    "                    #print ('placeholder_name === val',globals()[placeholder_name])\n",
    "                    test_feed_dict.update({globals()[placeholder_name]: data})\n",
    "        test_feed_dict.update({learning_rate_var: learning_rate})\n",
    "\n",
    "        if keep_prob in vars():\n",
    "            # Pracitcally, After finishing the training, it is important to turn off the dropout during development and testing.\n",
    "            # Otherwise, the prediction of this model is not stable since dropout add uncertainties to it.\n",
    "            test_feed_dict.update({keep_prob: 1.0})\n",
    "        if is_training in vars():\n",
    "            test_feed_dict.update({is_training: False})\n",
    "        print ('test_feed_dict',test_feed_dict)\n",
    "        #-----------\n",
    "        # feeding the data into session\n",
    "        #-----------\n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items()) \n",
    "        np_tensors = sess.run(\n",
    "            fetches = list(tf_tensors),\n",
    "            feed_dict=test_feed_dict\n",
    "        )  # return list\n",
    "        \n",
    "        for tensor_name, tensor in zip(list(tensor_names), np_tensors):\n",
    "            print ('=====tensor_name in test =====', tensor_name)\n",
    "            print ('=====tensor in test =====', tensor.shape, tensor)\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "    #-------------\n",
    "    # save the prediction result\n",
    "    #-------------\n",
    "    for tensor_name, tensor in prediction_dict.items():\n",
    "        # tensor: list of array w shape of (batch_size, )\n",
    "        # tensor_name: str\n",
    "        # \n",
    "        np_tensor = np.concatenate(tensor, axis = 0)\n",
    "        save_file = os.path.join(prediction_dir, '{}.npy'.format(tensor_name))\n",
    "        print ('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "        logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "        np.save(save_file, np_tensor)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.design a customized plot funtion to observe training/validation loss versus epoch\n",
    "# 2.build tensorflow base model\n",
    "# 3.suvey about how to tune deep learning model using Bayesian Optimization\n",
    "### reference: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以透過is_training 傳入variable決定這個batch要怎麼運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 4:\n",
    "    print (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
