{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Nov 5 2018\n",
    "\n",
    "This is an implementation of An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling in TensorFlow.\n",
    "\n",
    "Reference:\n",
    "    -Code: https://github.com/YuanTingHsieh/TF_TCN\n",
    "    -Figure: https://github.com/philipperemy/keras-tcn\n",
    "@author: Ray\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "sys.path.append('../models')\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel # for building our customized\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data_test)\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "        self.train_df = DataFrame(columns=data_cols, data=data_train)\n",
    "        self.val_df = DataFrame(columns=data_cols, data=data_val)\n",
    "\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = 3)\n",
    "\n",
    "        \n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        '''All row in our dataframe need to predicted as input of second-level model'''\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    \n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 36), (2, 36), (2,), (2, 36, 54), (2, 36))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id = np.load(os.path.join('../models/data/train/', '{}.npy'.format('word_id')))\n",
    "history_length = np.load(os.path.join('../models/data/train/', '{}.npy'.format('history_length')))\n",
    "label = np.load(os.path.join('../models/data/train/', '{}.npy'.format('label')))\n",
    "item_id = np.load(os.path.join('../models/data/train/','{}.npy'.format('item_id')))\n",
    "char_id = np.load(os.path.join('../models/data/train/','{}.npy'.format('char_id')))\n",
    "word_lengths = np.load(os.path.join('../models/data/train/','{}.npy'.format('word_length')))\n",
    "sentence = word_id[:2]\n",
    "y_true = label[:2]\n",
    "length = history_length[:2]\n",
    "char_id_____ = char_id[:2]\n",
    "word_lengths______ = word_lengths[:2]\n",
    "\n",
    "sentence.shape, y_true.shape, length.shape, char_id_____.shape, word_lengths______.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, optimizer='adam'):\n",
    "    '''\n",
    "    It's for choosing optimizer given learning rate.\n",
    "    '''\n",
    "    if optimizer == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    elif soptimizer == 'rms':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        # assert is a good way to tell other how to use this function for bug happening.\n",
    "        #-------\n",
    "        # standard way to pring the error\n",
    "        #-------\n",
    "        assert False, 'optimizer must be adam, adagrad, sgd, or rms'\n",
    "\n",
    "def update_parameters(loss, optimizer = 'adam'):\n",
    "    '''\n",
    "    It's for optimizing and logging training parameters\n",
    "    \n",
    "    1.using gradient clipping to avoid gradient explosion and vanishment.\n",
    "    \n",
    "    Gradient clipping is most common in recurrent neural networks. \n",
    "    When gradients are being propagated back in time, they can vanish \n",
    "    because they they are continuously multiplied by numbers less than one.\n",
    "    This is called the vanishing gradient problem. \n",
    "    This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, \n",
    "    This is solved by residual connections. \n",
    "    On the other hand, you can have exploding gradients too. \n",
    "    This is when they get exponentially large from being multiplied by numbers larger \n",
    "    than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    grad_clip = 5 # Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "    regularization_constant = 0.1 # Regularization constant applied to all trainable parameters.\n",
    "    enable_parameter_averaging = False # If true, model saves exponential weighted averages of parameters to separate checkpoint file.\n",
    "    global_step = tf.Variable(0, trainable = False) # Optional Variable to increment by one after the variables have been updated.\n",
    "    learning_rate_var = tf.Variable(0.0, trainable = False)\n",
    "    \n",
    "    #----------------\n",
    "    # for understanding regularization\n",
    "    #----------------\n",
    "    trainable_variables_1 = tf.trainable_variables()[0]\n",
    "    square_1 = tf.square(trainable_variables_1)\n",
    "    sum_1 = tf.reduce_sum(square_1)\n",
    "    sqrt = tf.sqrt(sum_1)\n",
    "    #-----------------\n",
    "    # we can customized our regularization on the parameters we like\n",
    "    #-----------------\n",
    "    if regularization_constant != 0:\n",
    "        # l2_norm: is a 0-D tensor. \n",
    "        # we do l2-norm on each trainable's parameters.\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()]) # Returns list including all variables created with trainable=True\n",
    "        # the smaller the loss is, the better do finish overfitting \n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "    #-----------------\n",
    "    # optimizing\n",
    "    #-----------------\n",
    "    # define the optimizer\n",
    "    optimizer = get_optimizer(learning_rate_var, optimizer=optimizer)\n",
    "    # compute grads: return A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    # standard way to do gradient clipping\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "    step = optimizer.apply_gradients(clipped, global_step = global_step)\n",
    "    print ('step - whtat optimizer.apply_gradients returns', step)\n",
    "    #-----------------\n",
    "    # if using moving average techniques\n",
    "    #-----------------\n",
    "    if enable_parameter_averaging:\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "        maintain_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([step]):\n",
    "            step = tf.group(maintain_averages_op)\n",
    "    else:\n",
    "        step = step\n",
    "    return learning_rate_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_utils import TemporalConvNet\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from tf_utils import sequence_evaluation_metric\n",
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "from tf_utils import shape_of_tensor\n",
    "\n",
    "\n",
    "dim_word = 300 # lstm on word embeddings\n",
    "hidden_size_char = 100 # lstm on chars\n",
    "\n",
    "trainable_embedding = False\n",
    "USE_PRETRAINED = True\n",
    "USE_CHARS = True\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "filename_chars_voc = \"../models/data/wordvec/chars_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "nchars = len(load_vocab_and_return_word_to_id_dict(filename_chars_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "\n",
    "#----------\n",
    "# for debugging\n",
    "#-------------\n",
    "max_seq_length = 36\n",
    "max_word_length = 54\n",
    "num_layers = 2\n",
    "hidden_size_cnn = 300\n",
    "dim_char = 100\n",
    "k = 3\n",
    "ntags = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features : (?, 36, 500)\n",
      "Output layer : (?, 36, 3)\n",
      "y_true : (?, 36)\n",
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    ####################################\n",
    "    # Step1: get input_sequences \n",
    "    ####################################\n",
    "\n",
    "    #------------\n",
    "    # 1-D  \n",
    "    #------------\n",
    "    item_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "    #------------   \n",
    "    # 2-D  \n",
    "    #------------\n",
    "    word_id = tf.placeholder(tf.int32, [None, max_seq_length]) \n",
    "    label = tf.placeholder(tf.int32, [None, max_seq_length]) # [batch_size, num_class]\n",
    "    if USE_CHARS:\n",
    "        word_lengths = tf.placeholder(tf.int32, shape=[None, max_seq_length])\n",
    "    #------------   \n",
    "    # 3-D  \n",
    "    #------------\n",
    "    if USE_CHARS:\n",
    "        char_ids = tf.placeholder(tf.int32, shape=[None, max_seq_length, max_word_length]) # [batch_size, max_seq_length, max_word_length]\n",
    "\n",
    "    #------------\n",
    "    # boolean parameter\n",
    "    #------------\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #------------\n",
    "    # word_embedding: get word embeddings matrix\n",
    "    #------------\n",
    "    if embeddings is None:\n",
    "        logging.info('WARNING: randomly initializing word vectors')\n",
    "        word_embeddings = tf.get_variable(\n",
    "        shape = [nwords, dim_word],\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32,\n",
    "        )\n",
    "    else:\n",
    "        word_embeddings = tf.get_variable(\n",
    "        initializer = embeddings, # it will hold the embedding\n",
    "        trainable = trainable_embedding,\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32\n",
    "        )\n",
    "    word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = word_id)\n",
    "    #------------\n",
    "    # char_embedding: get char embeddings matrix\n",
    "    #------------\n",
    "    if USE_CHARS:\n",
    "        # get char embeddings matrix\n",
    "        char_embeddings = tf.get_variable(\n",
    "                shape=[nchars, dim_char],\n",
    "                name=\"char_embeddings\",\n",
    "                trainable = True,\n",
    "                dtype=tf.float32,\n",
    "        )\n",
    "        # get char_representation, 4-D, [batch_size, max_seq_length, max_word_length, dim_char]\n",
    "        char_representation = tf.nn.embedding_lookup(params = char_embeddings, ids = char_ids) \n",
    "        # convert 4-D into 3-D: put the timestep on axis=1 and should be charater-level axis\n",
    "        s = tf.shape(char_representation) # 1-D tensor, (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "        char_representation = tf.reshape(char_representation, shape=[ s[0]*s[1], s[-2], dim_char]) # [batch_size * max_seq_length, max_word_length, dim_char]\n",
    "        # for computing bi lstm on chars\n",
    "        word_lengths_ = tf.reshape(word_lengths, shape=[s[0]*s[1]]) # 1-D tensor\n",
    "        # bi lstm on chars\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(hidden_size_char,\n",
    "                state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(hidden_size_char,\n",
    "                state_is_tuple=True)\n",
    "        _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw, \n",
    "                cell_bw, \n",
    "                inputs = char_representation,\n",
    "                sequence_length = word_lengths_, \n",
    "                dtype=tf.float32) \n",
    "        \"\"\"\n",
    "        Return A tuple (outputs, output_states) \n",
    "          -outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output\n",
    "               1.output_fw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char].For example, (72, 54, 100)\n",
    "               2.output_bw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char]\n",
    "          -output_states: A tuple (output_state_fw, output_state_bw) containing the forward and the backward final states of bidirectional rnn.\n",
    "               1.final_state_output_fw with shape of [batch_szie*max_word_lenght, hidden_size_char]. For instance, (72, 100)\n",
    "               2.final_state_output_bw with shape of [batch_szie*max_word_lenght, hidden_size_char]\n",
    "        \n",
    "        \"\"\"\n",
    "        # get word level representation from characters embeddings\n",
    "        _, ((_, output_fw_final_state), (_, output_bw_final_state)) = _output\n",
    "        output = tf.concat([output_fw_final_state, output_bw_final_state], axis=-1)# concat on char_embedding level, [batch_szie*max_word_lenght, 2*hidden_size_char]\n",
    "        # reshape to word level representation\n",
    "        word_representation_extracted_from_char = tf.reshape(output, shape=[s[0], s[1], 2* hidden_size_char]) # [batch_size, max_seq_length, 2*hidden_size_char]\n",
    "\n",
    "    x = tf.concat([\n",
    "    word_representation,\n",
    "    word_representation_extracted_from_char\n",
    "        ], axis = 2) # (?, 122, 300)\n",
    "    \n",
    "    ####################################\n",
    "    # Step2: calculate_outputs \n",
    "    ####################################\n",
    "    \n",
    "    #-------------------------\n",
    "    # NN architecuture-Simple CNN\n",
    "    #-------------------------\n",
    "    print ('Original features : {}'.format(x.shape))\n",
    "    num_channels = [300, 250, 50]\n",
    "    tcn = TemporalConvNet(inputs=x, num_channels = num_channels,convolution_width=k)\n",
    "    print ('final hiddel layer : {}'.format(y_hat.shape))\n",
    "    y_hat = time_distributed_dense_layer(tcn, ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "    print ('Output layer : {}'.format(y_hat.shape))\n",
    "    print ('y_true : {}'.format(label.shape))\n",
    "    #--------------\n",
    "    # for second-level model\n",
    "    #--------------\n",
    "    prediction_tensors = {\n",
    "        'item_id':item_id,\n",
    "        'word_id':word_id,\n",
    "        'final_states':tcn, # 修改不要全部max_seq_lenghth都存, 只存到history_length的長度(save memory)\n",
    "        'final_predictions':y_hat,\n",
    "    }\n",
    "    \n",
    "    ####################################\n",
    "    # Step3: calculate_loss +evaluation score+ optimizing\n",
    "    ####################################\n",
    "    loss = sequence_softmax_loss(y = label, y_hat = y_hat, sequence_lengths = history_length, max_sequence_length = max_seq_length)\n",
    "    learning_rate_var  = update_parameters(loss)\n",
    "    \n",
    "    \n",
    "    labels_pred = tf.cast(tf.argmax(y_hat, axis= 2),tf.int32) # (?, max_seq_length)\n",
    "    score = sequence_evaluation_metric(y = label, y_hat = labels_pred, sequence_lengths = history_length, max_sequence_length = max_seq_length)['f1']\n",
    "    #score,p,r,y_,correct_preds,total_correct,total_preds,acc,sequence_mask,y = sequence_evaluation_metric(y = label, y_hat = labels_pred, sequence_lengths = history_length)\n",
    "    #y_ture_input, y_pred_input, correct_preds,sequence_mask, p,r,f1,acc = sequence_evaluation_metric_1(y = label, y_hat = labels_pred, sequence_lengths = history_length)\n",
    "\n",
    "#     ####################################\n",
    "#     # Step4: saving the model \n",
    "#     ####################################    \n",
    "#     # create saver object\n",
    "#     # max_to_keep: indicates the maximum number of recent checkpoint files to keep.\n",
    "#     saver = tf.train.Saver(max_to_keep = 1)\n",
    "#     if enable_parameter_averaging:\n",
    "#         saver_averaged = tf.train.Saver(ema.variables_to_restore(), max_to_keep=1)    \n",
    "\n",
    "    #-------------------------\n",
    "    # standard\n",
    "    #-------------------------\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque # for computing Train/validation losses are averaged over the last loss_averaging_window\n",
    "import tensorflow as tf\n",
    "warm_start_init_step = 0 # If nonzero, model will resume training a restored model beginning at warm_start_init_step.\n",
    "batch_size = 128\n",
    "loss_averaging_window = 10\n",
    "num_validation_batches = 1\n",
    "num_training_steps = 10\n",
    "learning_rate=0.001\n",
    "log_interval = 1\n",
    "min_steps_to_checkpoint =1\n",
    "early_stopping_steps = 10\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    ####################################\n",
    "    # 1. fit\n",
    "    ####################################\n",
    "    if warm_start_init_step:\n",
    "        # continue the optimization at a recent checkpoint instead of having to restart the optimization from the beginning\n",
    "        restore(warm_start_init_step)\n",
    "        step = warm_start_init_step\n",
    "    else:\n",
    "        # start the optimization from the beginning\n",
    "        sess.run(init) # Run the initializer\n",
    "        step = 0\n",
    "     \n",
    "    word_lengths__, _output_from_bilstm, word_representation_from_char, word_representation, wtf_ = sess.run(fetches = [word_lengths_, _output, output, x, _], feed_dict = {\n",
    "                                                 word_id:sentence,\n",
    "                                                 label: y_true,\n",
    "                                                 history_length: length,\n",
    "                                                 char_ids: char_id_____,\n",
    "                                                 word_lengths:word_lengths______\n",
    "                \n",
    "                                                    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
