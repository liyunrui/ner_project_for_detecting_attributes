{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "sys.path.append('../models')\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel # for building our customized\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'char_id',\n",
    "            'word_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data_test)\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "        self.train_df = DataFrame(columns=data_cols, data=data_train)\n",
    "        self.val_df = DataFrame(columns=data_cols, data=data_val)\n",
    "    \n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = 3)\n",
    "\n",
    "        print ('shape of whole data : {}'.format(self.test_df.shapes()))\n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        '''All row in our dataframe need to predicted as input of second-level model'''\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    \n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "shape of whole data : item_id                  (198709,)\n",
      "word_id               (198709, 36)\n",
      "history_length           (198709,)\n",
      "char_id           (198709, 36, 54)\n",
      "word_length           (198709, 36)\n",
      "label                 (198709, 36)\n",
      "dtype: object\n",
      "number of training example: 167741\n",
      "number of validating example: 18632\n",
      "number of testing example: 198709\n"
     ]
    }
   ],
   "source": [
    "TRACE_CODE = False\n",
    "dr = DataReader(data_dir ='../models/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dr.train_df:\n",
    "#     print (i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('../data/processed/mobile_training_v2.csv').groupby('item_name').mean()\n",
    "#pd.read_csv('/data/ner_task/mobile/mobile_ID_attribute_brand.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_training_w_word_id = pd.read_csv('../data/processed/mobile_training_w_word_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1757852\n",
       "2     199039\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_training_w_word_id.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobile_training_w_word_id.groupby('item_name').label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>item_id</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [item_name, tokens, label, eval_set, clean_tokens, item_id, word_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_training_w_word_id[mobile_training_w_word_id.item_name == 'zyrex zs278 zs 278 dual sim garansi 1 tahun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_utils import temporal_convolution_layer\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from tf_utils import sequence_evaluation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_cnn(TFBaseModel):\n",
    "    \n",
    "    def __init__(self,max_seq_len,filter_widths,hidden_size_cnn,num_hidden_layers,\n",
    "                 ntags,trainable_embedding,dim_word,nwords,metric,use_chars,\n",
    "                 dim_char,max_word_length,nchars,hidden_size_char,**kwargs):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.filter_widths = filter_widths\n",
    "        self.hidden_size_cnn = hidden_size_cnn\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.ntags = ntags # n_class\n",
    "        self.dim_word = dim_word\n",
    "        self.nwords = nwords\n",
    "        self.trainable_embedding = trainable_embedding\n",
    "        self.metric = metric\n",
    "        self.USE_CHARS = use_chars\n",
    "        if self.USE_CHARS:\n",
    "            try:\n",
    "                self.dim_char = dim_char\n",
    "                self.max_word_length = max_word_length\n",
    "                self.nchars = nchars\n",
    "                self.hidden_size_char = hidden_size_char\n",
    "            except:\n",
    "                assert False, 'Please assing dim_char, max_word_length, and nchars as arguments'\n",
    "        super(simple_cnn, self).__init__(**kwargs)\n",
    "    def get_input_sequences(self):\n",
    "        ####################################\n",
    "        # Step1: get input_sequences \n",
    "        ####################################\n",
    "        #------------\n",
    "        # 1-D  \n",
    "        #------------\n",
    "        self.item_id = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "        #------------   \n",
    "        # 2-D  \n",
    "        #------------\n",
    "        self.word_id = tf.placeholder(tf.int32, [None, self.max_seq_len]) \n",
    "        self.label = tf.placeholder(tf.int32, [None, self.max_seq_len]) # [batch_size, num_class]\n",
    "        if self.USE_CHARS:\n",
    "            self.word_length = tf.placeholder(tf.int32, shape=[None, self.max_seq_len])\n",
    "        #------------   \n",
    "        # 3-D  \n",
    "        #------------\n",
    "        if self.USE_CHARS:\n",
    "            self.char_id = tf.placeholder(tf.int32, shape=[None, self.max_seq_len, self.max_word_length]) # [batch_size, max_seq_length, max_word_length]\n",
    "\n",
    "\n",
    "        #------------\n",
    "        # boolean parameter\n",
    "        #------------\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #------------\n",
    "        # word_embedding: get embeddings matrix\n",
    "        #------------\n",
    "        if embeddings is None:\n",
    "            logging.info('WARNING: randomly initializing word vectors')\n",
    "            word_embeddings = tf.get_variable(\n",
    "            shape = [nwords, dim_word],\n",
    "            name = 'word_embeddings',\n",
    "            dtype = tf.float32,\n",
    "            )\n",
    "        else:\n",
    "            word_embeddings = tf.get_variable(\n",
    "            initializer = embeddings, # it will hold the embedding\n",
    "            #shape = [word2vec.shape[0], word2vec.shape[1]], # [num_vocabulary, embeddings_dim]\n",
    "            trainable = trainable_embedding,\n",
    "            name = 'word_embeddings',\n",
    "            dtype = tf.float32\n",
    "            )\n",
    "        word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = self.word_id)\n",
    "        #------------\n",
    "        # char_embedding: get char embeddings matrix\n",
    "        #------------\n",
    "        if self.USE_CHARS:\n",
    "            # get char embeddings matrix\n",
    "            char_embeddings = tf.get_variable(\n",
    "                    shape=[self.nchars, self.dim_char],\n",
    "                    name=\"char_embeddings\",\n",
    "                    trainable = True,\n",
    "                    dtype=tf.float32,\n",
    "            )\n",
    "            # get char_representation, 4-D, [batch_size, max_seq_length, max_word_length, dim_char]\n",
    "            char_representation = tf.nn.embedding_lookup(params = char_embeddings, ids = self.char_id) \n",
    "            # convert 4-D into 3-D: put the timestep on axis=1 and should be charater-level axis\n",
    "            s = tf.shape(char_representation) # 1-D tensor, (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "            char_representation = tf.reshape(char_representation, shape=[ s[0]*s[1], s[-2], self.dim_char]) # [batch_size * max_seq_length, max_word_length, dim_char]\n",
    "            # for computing bi lstm on chars\n",
    "            word_lengths = tf.reshape(self.word_length, shape=[s[0]*s[1]]) # 1-D tensor\n",
    "            # bi lstm on chars\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size_char,\n",
    "                    state_is_tuple=True)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size_char,\n",
    "                    state_is_tuple=True)\n",
    "            _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, \n",
    "                    cell_bw, \n",
    "                    inputs = char_representation,\n",
    "                    sequence_length = word_lengths, \n",
    "                    dtype=tf.float32) \n",
    "            \"\"\"\n",
    "            Return A tuple (outputs, output_states) \n",
    "              -outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output\n",
    "                   1.output_fw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char].For example, (72, 54, 100)\n",
    "                   2.output_bw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char]\n",
    "              -output_states: A tuple (output_state_fw, output_state_bw) containing the forward and the backward final states of bidirectional rnn.\n",
    "                   1.final_state_output_fw with shape of [batch_szie*max_word_lenght, hidden_size_char]. For instance, (72, 100)\n",
    "                   2.final_state_output_bw with shape of [batch_szie*max_word_lenght, hidden_size_char]\n",
    "\n",
    "            \"\"\"\n",
    "            # get word level representation from characters embeddings\n",
    "            _, ((_, output_fw_final_state), (_, output_bw_final_state)) = _output\n",
    "            output = tf.concat([output_fw_final_state, output_bw_final_state], axis=-1)# concat on char_embedding level, [batch_szie*max_word_lenght, 2*hidden_size_char]\n",
    "            # reshape to word level representation\n",
    "            word_representation_extracted_from_char = tf.reshape(output, shape=[s[0], s[1], 2* hidden_size_char]) # [batch_size, max_seq_length, 2*hidden_size_char]\n",
    "        \n",
    "        if self.USE_CHARS:\n",
    "            x = tf.concat([\n",
    "            word_representation,\n",
    "            word_representation_extracted_from_char\n",
    "                ], axis = 2) # (?, 36, 500 == 300+200)  \n",
    "        else:\n",
    "            x = tf.concat([\n",
    "            word_representation,\n",
    "                ], axis=2) # (?, 36, 300)\n",
    "        #print ('Original features : {}'.format(x.shape))\n",
    "        return x\n",
    "    def calculate_outputs(self, x):\n",
    "        ####################################\n",
    "        # Step2: calculate_outputs \n",
    "        ####################################\n",
    "        #-------------------------\n",
    "        # Model-Simple CNN\n",
    "        #-------------------------\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            if i == 0:\n",
    "                self.conv = temporal_convolution_layer(x, \n",
    "                                           output_units = self.hidden_size_cnn,\n",
    "                                           convolution_width = self.filter_widths,\n",
    "                                           dilated = False,\n",
    "                                           causal = False,\n",
    "                                           bias=True,\n",
    "                                           activation=None, \n",
    "                                           dropout=None,\n",
    "                                           scope='cnn-{}'.format(i),\n",
    "                                           reuse = False,\n",
    "                                          )\n",
    "            else:\n",
    "                self.conv = temporal_convolution_layer(self.conv, \n",
    "                                           output_units = self.hidden_size_cnn,\n",
    "                                           convolution_width = self.filter_widths,\n",
    "                                           dilated = False,\n",
    "                                           causal = False,\n",
    "                                           bias=True,\n",
    "                                           activation=None, \n",
    "                                           dropout=None,\n",
    "                                           scope='cnn-{}'.format(i),\n",
    "                                           reuse = False,\n",
    "                                          )\n",
    "\n",
    "            #print ('CNN-{} layer : {}'.format(i, conv.shape))\n",
    "        # output layer (linear)\n",
    "        y_hat = time_distributed_dense_layer(self.conv, self.ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "        #--------------\n",
    "        # for second-level model: \n",
    "        #--------------\n",
    "        # method1: using final_temporal_idx, it's for only get the final step. -->That's not here task I want\n",
    "        # method2: Only saving history_length rather tahn saving max_seq_lenght --> By far, don't know how to do that.\n",
    "        self.prediction_tensors = {\n",
    "            'item_id':self.item_id, # (?, )\n",
    "            'word_id':self.word_id, # (?, max_seq_length)\n",
    "            'history_length':self.history_length,\n",
    "            'final_states':self.conv, # (?, max_seq_length, num_hidden_units)\n",
    "            'final_predictions':y_hat, # (?, max_seq_length, ntags)\n",
    "        }\n",
    "        return y_hat\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        self.preds = self.calculate_outputs(x)\n",
    "        loss = sequence_softmax_loss(y = self.label, \n",
    "                                     y_hat = self.preds, \n",
    "                                     sequence_lengths = self.history_length, \n",
    "                                     max_sequence_length = self.max_seq_len)\n",
    "        return loss\n",
    "    \n",
    "    def calculate_evaluation_metric(self):\n",
    "        labels_pred = tf.cast(tf.argmax(self.preds, axis= 2),tf.int32) # (?, max_seq_length)\n",
    "        score = sequence_evaluation_metric(y = self.label,\n",
    "                                           y_hat = labels_pred, \n",
    "                                           sequence_lengths = self.history_length,\n",
    "                                           max_sequence_length = self.max_seq_len\n",
    "                                          )[self.metric]\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_dict = {'B-B': 2, 'I-B': 1, 'O': 0}\n",
    "# tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_chunks = set(get_chunks(seq = [2,1,1], tags = tag_dict))\n",
    "# lab_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_pred_chunks = set(get_chunks(seq = [0,2,0], tags = tag_dict))\n",
    "# lab_pred_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab_chunks & lab_pred_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(lab_chunks & lab_pred_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(lab_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "\n",
    "dim_word = 300\n",
    "trainable_embedding = False\n",
    "USE_PRETRAINED = True\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "filename_chars_voc = \"../models/data/wordvec/chars_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "USE_CHARS = True\n",
    "if USE_CHARS:\n",
    "    hidden_size_char = 100\n",
    "    nchars = len(load_vocab_and_return_word_to_id_dict(filename_chars_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomorrow To do list\n",
    "# 2. 搭建dilated CNN w/wo\n",
    "# 1. Add char_embedding + parameter tuning\n",
    "# 3.嘗試各種model\n",
    "# 5. crf loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.2265625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20125 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new run with parameters:\n",
      "{'USE_CHARS': True,\n",
      " 'batch_size': 128,\n",
      " 'checkpoint_dir': 'checkpoints',\n",
      " 'dim_char': 100,\n",
      " 'dim_word': 300,\n",
      " 'early_stopping_steps': 3000,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'filter_widths': 3,\n",
      " 'grad_clip': 5,\n",
      " 'hidden_size_char': 100,\n",
      " 'hidden_size_cnn': 300,\n",
      " 'keep_prob_scalar': 1.0,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_dir': 'logs',\n",
      " 'log_interval': 1,\n",
      " 'loss_averaging_window': 100,\n",
      " 'max_seq_len': 36,\n",
      " 'max_word_length': 54,\n",
      " 'metric': 'f1',\n",
      " 'min_steps_to_checkpoint': 50,\n",
      " 'nchars': 77,\n",
      " 'ntags': 3,\n",
      " 'num_hidden_layers': 2,\n",
      " 'num_restarts': 0,\n",
      " 'num_training_steps': 50000,\n",
      " 'num_validation_batches': 25,\n",
      " 'nwords': 24475,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': 'predictions',\n",
      " 'reader': <__main__.DataReader object at 0x7f32642ecef0>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'trainable_embedding': False,\n",
      " 'use_evaluation_metric_as_early_stopping': True,\n",
      " 'warm_start_init_step': 0}\n",
      "all parameters:\n",
      "[('word_embeddings:0', [24475, 300]),\n",
      " ('char_embeddings:0', [77, 100]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/kernel:0', [200, 400]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/bias:0', [400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/kernel:0', [200, 400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/bias:0', [400]),\n",
      " ('cnn-0/weights:0', [3, 500, 300]),\n",
      " ('cnn-0/biases:0', [300]),\n",
      " ('cnn-1/weights:0', [3, 300, 300]),\n",
      " ('cnn-1/biases:0', [300]),\n",
      " ('output-layer/weights:0', [300, 3]),\n",
      " ('output-layer/biases:0', [3]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('char_embeddings/Adam:0', [77, 100]),\n",
      " ('char_embeddings/Adam_1:0', [77, 100]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/kernel/Adam:0', [200, 400]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/kernel/Adam_1:0', [200, 400]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/bias/Adam:0', [400]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/bias/Adam_1:0', [400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/kernel/Adam:0', [200, 400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/kernel/Adam_1:0', [200, 400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/bias/Adam:0', [400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/bias/Adam_1:0', [400]),\n",
      " ('cnn-0/weights/Adam:0', [3, 500, 300]),\n",
      " ('cnn-0/weights/Adam_1:0', [3, 500, 300]),\n",
      " ('cnn-0/biases/Adam:0', [300]),\n",
      " ('cnn-0/biases/Adam_1:0', [300]),\n",
      " ('cnn-1/weights/Adam:0', [3, 300, 300]),\n",
      " ('cnn-1/weights/Adam_1:0', [3, 300, 300]),\n",
      " ('cnn-1/biases/Adam:0', [300]),\n",
      " ('cnn-1/biases/Adam_1:0', [300]),\n",
      " ('output-layer/weights/Adam:0', [300, 3]),\n",
      " ('output-layer/weights/Adam_1:0', [300, 3]),\n",
      " ('output-layer/biases/Adam:0', [3]),\n",
      " ('output-layer/biases/Adam_1:0', [3])]\n",
      "trainable parameters:\n",
      "[('char_embeddings:0', [77, 100]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/kernel:0', [200, 400]),\n",
      " ('bidirectional_rnn/fw/lstm_cell/bias:0', [400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/kernel:0', [200, 400]),\n",
      " ('bidirectional_rnn/bw/lstm_cell/bias:0', [400]),\n",
      " ('cnn-0/weights:0', [3, 500, 300]),\n",
      " ('cnn-0/biases:0', [300]),\n",
      " ('cnn-1/weights:0', [3, 300, 300]),\n",
      " ('cnn-1/biases:0', [300]),\n",
      " ('output-layer/weights:0', [300, 3]),\n",
      " ('output-layer/biases:0', [3])]\n",
      "trainable parameter count:\n",
      "890003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n",
      "built graph\n"
     ]
    }
   ],
   "source": [
    "base_dir = ''\n",
    "nn = simple_cnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "    optimizer='adam',\n",
    "    max_seq_len = 36,\n",
    "    learning_rate =0.001,\n",
    "    hidden_size_cnn = 300,\n",
    "    filter_widths= 3,\n",
    "    num_hidden_layers = 2,\n",
    "    ntags = 3,\n",
    "    batch_size = 128,\n",
    "    dim_word = 300,\n",
    "    nwords = nwords,\n",
    "    trainable_embedding = False,\n",
    "    metric = 'f1',\n",
    "    use_chars = USE_CHARS,\n",
    "    nchars = nchars,\n",
    "    hidden_size_char = hidden_size_char,\n",
    "    max_word_length = 54,\n",
    "    dim_char = 100,\n",
    "    num_training_steps = 50000,\n",
    "    early_stopping_steps = 3000,\n",
    "    loss_averaging_window = 100,\n",
    "    use_evaluation_metric_as_early_stopping = True,\n",
    "    warm_start_init_step = 0, # for some case, we don't want to train the model from the beginning\n",
    "    regularization_constant = 0.0,\n",
    "    keep_prob = 1.0,\n",
    "    enable_parameter_averaging = False,\n",
    "    num_restarts = 0,\n",
    "    min_steps_to_checkpoint = 500, # The value of this need to larger than best_validation_tstep. Otherwise, we won't save our best model\n",
    "    log_interval = 1,\n",
    "    num_validation_batches = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22860 / 22939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1:這麼高還需要post-processing麼? 先不要\n",
    "# Q2:先doubt這個分數, 去看test的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 3.66481662       [[val]]     score: 0.02726281       \n",
      "[[step        1]]     [[train]]     loss: 2.65904021       [[val]]     score: 0.35076207       \n",
      "[[step        2]]     [[train]]     loss: 1.93723869       [[val]]     score: 0.43084073       \n",
      "[[step        3]]     [[train]]     loss: 1.53070506       [[val]]     score: 0.44269053       \n",
      "[[step        4]]     [[train]]     loss: 1.40815645       [[val]]     score: 0.49123503       \n",
      "[[step        5]]     [[train]]     loss: 1.29330376       [[val]]     score: 0.54408339       \n",
      "[[step        6]]     [[train]]     loss: 1.15708091       [[val]]     score: 0.59039469       \n",
      "[[step        7]]     [[train]]     loss: 1.05477159       [[val]]     score: 0.62481545       \n",
      "[[step        8]]     [[train]]     loss: 0.9781244        [[val]]     score: 0.65211877       \n",
      "[[step        9]]     [[train]]     loss: 0.91040561       [[val]]     score: 0.67526738       \n",
      "[[step       10]]     [[train]]     loss: 0.86975382       [[val]]     score: 0.6950631        \n"
     ]
    }
   ],
   "source": [
    "##### \n",
    "nn.fit()  # why f1-score: 1.04117423, 又在更新了v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.restore() # 幹為什麼最好的step是420, 卻restore是200\n",
    "nn.evaluate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.restore() # 幹為什麼最好的step是420, 卻restore是200\n",
    "nn.evaluate() \n",
    "\n",
    "#nn.evaluate() \n",
    "# What my expected is output of nn.evaluate() should be the same as the best_validation_loss\n",
    "# if the best_validation_loss proudced from whole validating set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading testing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# lodaing features\n",
    "#-----------------------\n",
    "test = pd.read_csv('../data/processed/mobile_training_w_word_id.csv')\n",
    "item_id = np.load('predictions/item_id.npy')\n",
    "word_id = np.load('predictions/word_id.npy')\n",
    "final_predictions = np.load('predictions/final_predictions.npy')\n",
    "final_states = np.load('predictions/final_states.npy')\n",
    "history_length = np.load('predictions/history_length.npy')\n",
    "# label = np.load('data/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_id_df = pd.DataFrame(word_id, columns = ['timestep_{}'.format(i+1) for i in range(word_id.shape[1])])\n",
    "# word_id_df\n",
    "word_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.item_id == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "# merge testing result from Neural Network\n",
    "#----------------\n",
    "i_ids = []\n",
    "w_ids = []\n",
    "predictions = []\n",
    "hidden_features = []\n",
    "for sent, hidden_feature, sent_length, i_id, w_id in zip(final_predictions, final_states, history_length, item_id, word_id):\n",
    "#     print ('i_id', i_id)\n",
    "#     print ('sent_length',sent_length)\n",
    "    for w, y_pred, h_f in zip(w_id[:sent_length], sent[:sent_length], hidden_feature[:sent_length]):\n",
    "#         print ('w_id',w)\n",
    "#         print ('y_pred_index',np.argmax(y_pred))\n",
    "#         print ('h_f', len(h_f))\n",
    "        i_ids.append(i_id)\n",
    "        w_ids.append(w)\n",
    "        predictions.append(np.argmax(y_pred))\n",
    "        hidden_features.append(h_f)\n",
    "hidden_features = np.array(hidden_features) # (history_lengh * num_samples, num_hidden_units)\n",
    "#----------------\n",
    "# output\n",
    "#----------------\n",
    "df1 = pd.DataFrame({\n",
    "    'item_id':i_ids,\n",
    "    'word_id':w_ids,\n",
    "    'predictions':predictions,\n",
    "})\n",
    "df2 = pd.DataFrame(hidden_features, columns = ['simple_cnn_{}'.format(i+1) for i in range(hidden_features.shape[1])])\n",
    "test = pd.concat([df1,df2], axis = 1)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze\n",
    "# 1.check evaluation metric of train, val and test.\n",
    "# 2.eheck training/validting history using tensorboard\n",
    "# 3.observe those items we cannot detect attribute.\n",
    "# 4.make our imbalanced attribute balanecd. For example, in brand attribute, Xiomi, Samsung, and Apple are top3 brand, the other brand is less. We hope our proposed method can tag unseen or rare attributes as possible as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = pd.merge(mobile_training_w_word_id, test[['item_id','word_id','predictions']], \n",
    "         on = ['item_id','word_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.head(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performaance_dashboard(df):\n",
    "    observe\n",
    "    for eval_set in ['train', 'val', 'test']:\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for i_id, groupby_df in df[df.eval_set == eval_set].groupby('item_id'):\n",
    "            y_true = list(groupby_df.label)\n",
    "            y_pred = list(groupby_df.predictions)\n",
    "        #     print ('item_id', i_id)\n",
    "        #     print ('y_true', y_true)\n",
    "        #     print ('y_pred', y_pred)\n",
    "            if all(v == 0 for v in y_true):\n",
    "                pass\n",
    "            else:\n",
    "                total_correct += 1.0\n",
    "                if y_true == y_pred:\n",
    "                    correct_preds += 1.0\n",
    "            if all(v == 0 for v in y_pred):\n",
    "                pass\n",
    "            else:\n",
    "                total_preds += 1.0\n",
    "        #----------\n",
    "        # output\n",
    "        #----------\n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        print('{}-f1: {}'.format(eval_set,f1))\n",
    "        print('{}-precision: {}'.format(eval_set,p))\n",
    "        print('{}-recall: {}'.format(eval_set,r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performaance_dashboard(analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true == y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performaance_dashboard(df):\n",
    "    correct = None\n",
    "    count_helper = list(df[df.item_id == 1].label == df[df.item_id == 1].predictions)\n",
    "    if count_helper.count(True) == len(count_helper):\n",
    "        correct = 1\n",
    "    else:\n",
    "        correct = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = analyze[analyze.item_id == 1].label == analyze[analyze.item_id == 1].predictions\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.tolist().count(True) == len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.learn how to use tensorboard?\n",
    "### Reference: https://www.tensorflow.org/guide/summaries_and_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# #-----------------------\n",
    "# # lodaing hidden features\n",
    "# #-----------------------\n",
    "# item_id = np.load('predictions/item_id.npy')\n",
    "# word_id = np.load('predictions/word_id.npy')\n",
    "# final_states = np.load('predictions/final_states.npy')\n",
    "# final_predictions = np.load('predictions/final_predictions.npy')\n",
    "# #label = np.load('data/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2048 / 2/2/2/2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
