{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-D convolution\n",
    "### https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    k = tf.constant([\n",
    "        [1, 0, 1],\n",
    "        [2, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=tf.float32, name='k')\n",
    "    i = tf.constant([\n",
    "        [4, 3, 1, 0],\n",
    "        [2, 1, 0, 1],\n",
    "        [1, 2, 4, 1],\n",
    "        [3, 1, 0, 2]\n",
    "    ], dtype=tf.float32, name='i')\n",
    "    # [filter_height, filter_width, in_channels, out_channels] --> [1,5, in, out]\n",
    "    kernel = tf.reshape(k, [3, 3, 1, 1], name='kernel') \n",
    "    # [batch, in_height, in_width, in_channels] --> (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "\n",
    "    image  = tf.reshape(i, [1, 4, 4, 1], name='image') \n",
    "    o = tf.nn.conv2d(image, kernel, [1, 1, 1, 1], \"SAME\")\n",
    "    res = tf.squeeze(o)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "with tf.Session(graph=g,  config = config) as sess:\n",
    "    sess.run(init)\n",
    "    res,kernel,o = sess.run([res,kernel,o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 4, 1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.],\n",
       "        [11.],\n",
       "        [ 8.],\n",
       "        [ 2.]],\n",
       "\n",
       "       [[ 7.],\n",
       "        [14.],\n",
       "        [ 6.],\n",
       "        [ 2.]],\n",
       "\n",
       "       [[ 3.],\n",
       "        [ 6.],\n",
       "        [12.],\n",
       "        [ 9.]],\n",
       "\n",
       "       [[ 5.],\n",
       "        [12.],\n",
       "        [ 5.],\n",
       "        [ 6.]]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.,  6.],\n",
       "       [ 6., 12.]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先把code接起來\n",
    "# Q2: gated mechanism 不懂, 但先實驗, work在想為什麼。\n",
    "# Q3: weight normalization: 畫圖, 在確認一下what he did\n",
    "# Q4: residual connections or highway: 在實驗哪個比較好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention mechanism reference:https://jalammar.github.io/illustrated-transformer/?fbclid=IwAR3O7uOysR3j1CxhW0vvNs-INOSf1S0lLskBEgjtoWu4m69aSbx4pYxocns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightNorm_temporal_convolution_layer(inputs, output_units, convolution_width = 3, causal = True, \n",
    "                                          dilation_rate=[1], bias=True, activation = tf.nn.relu, dropout=None, \n",
    "                                          scope='weightNorm-temporal-convolution-layer', reuse=False, \n",
    "                                          gated = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    ----------\n",
    "    Reference:\n",
    "        - Implement Weight Normarlization [Salimans 16]\n",
    "        - Implement Spatial Dropout[LeCun 15]\n",
    "            It's a special dropout used in convolutional layer.\n",
    "        - Implement Gating Mechanisms [Dauphin 17], (Not in TCN paper)\n",
    "    \"\"\"\n",
    "     with tf.variable_scope(scope, reuse=reuse):\n",
    "        if gated:\n",
    "            output_units = output_units * 2\n",
    "        \n",
    "        if causal:\n",
    "            # comput how many zeros we need padded given dilation rate(d) and convolution_width(k)\n",
    "            shift = (convolution_width - 1) * dilation_rate[0]\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            # pad zeros over temporal axis at the left side of inputs\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "            # filter weight\n",
    "            V = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                shape=[convolution_width, shape_of_tensor(inputs, 2), output_units]\n",
    "            )\n",
    "            g = tf.get_variable('g', shape=[output_units], dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.), trainable=True)\n",
    "            # use weight normalization (Salimans & Kingma, 2016)\n",
    "            W = tf.reshape(g, [1, 1, output_units]) * tf.nn.l2_normalize(V, [0, 1])\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(inputs, W, padding='VALID', dilation_rate=dilation_rate)\n",
    "            # adding bias if True\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "            # adding non-linear output if True\n",
    "            if gated:\n",
    "                # use gated linear units (Dauphin 2016) as activation\n",
    "                split0, split1 = tf.split(z, num_or_size_splits=2, axis=2)\n",
    "                split1 = tf.sigmoid(split1)\n",
    "                z = tf.multiply(split0, split1)\n",
    "            else:\n",
    "                z = activation(z) if activation else z\n",
    "            # adding spatial dropout if True\n",
    "            if dropout is not None:\n",
    "                # at each training step, a whole channel is zeroed out.(feature- axis)\n",
    "                # Here, what spatial dropout did is like feature fraction in tree-based model: random select part of features duriing training step.\n",
    "                noise_shape = (tf.shape(z)[0], tf.constant(1), tf.shape(z)[2]) #[batch_size,1, output_units]\n",
    "                z = tf.nn.dropout(z, dropout, noise_shape)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # filter weight\n",
    "            V = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                shape=[convolution_width, shape_of_tensor(inputs, 2), output_units]\n",
    "            )\n",
    "            g = tf.get_variable('g', shape=[output_units], dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.), trainable=True)\n",
    "            # use weight normalization (Salimans & Kingma, 2016)\n",
    "            W = tf.reshape(g, [1, 1, output_units]) * tf.nn.l2_normalize(V, [0, 1])\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "            # adding bias if True\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "            # adding non-linear output if True\n",
    "            if gated:\n",
    "                # use gated linear units (Dauphin 2016) as activation\n",
    "                split0, split1 = tf.split(z, num_or_size_splits=2, axis=2)\n",
    "                split1 = tf.sigmoid(split1)\n",
    "                z = tf.multiply(split0, split1)\n",
    "            else:\n",
    "                z = activation(z) if activation else z\n",
    "            # adding spatial dropout if True\n",
    "            if dropout is not None:\n",
    "                # at each training step, a whole channel is zeroed out.(feature- axis)\n",
    "                # Here, what spatial dropout did is like feature fraction in tree-based model: random select part of features duriing training step.\n",
    "                noise_shape = (tf.shape(z)[0], tf.constant(1), tf.shape(z)[2]) #[batch_size,1, output_units]\n",
    "                z = tf.nn.dropout(z, dropout, noise_shape)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    return z\n",
    "\n",
    "def attentionBlock(x, counters, dropout):\n",
    "    \"\"\"self attention block\n",
    "    \n",
    "    # Arguments\n",
    "        x: Tensor of shape [batch size, max sequence length, Cin] # [batch size, max sequence length, num_features]\n",
    "        counters: to keep track of names\n",
    "        dropout: add dropout after attention\n",
    "    # Returns\n",
    "        A tensor of shape [batch size, L, Cin]\n",
    "        \n",
    "    Reference:\n",
    "        - Scaled Dot-Product Attention in Attention is All You Need [Vaswani 17]\n",
    "    \"\"\"\n",
    "\n",
    "    k_size = x.get_shape()[-1].value # Cin, number of features\n",
    "    v_size = x.get_shape()[-1].value # Cin, number of features\n",
    "\n",
    "    name = get_name('attention_block', counters)\n",
    "    with tf.variable_scope(name):\n",
    "        #---------------\n",
    "        # step1: create query, key, and value vector first through a linear transformation of inputs( aka dense-connected layer)\n",
    "        #---------------\n",
    "        # [batch size, max sequence length, k_size]\n",
    "        key = tf.layers.dense(x, units=k_size, activation=None, use_bias=False,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0, 0.01))\n",
    "        key = tf.nn.dropout(key, 1.0 - dropout)\n",
    "        # [batch size, max sequence length, k_size]\n",
    "        query = tf.layers.dense(x, units=k_size, activation=None, use_bias=False,\n",
    "                                kernel_initializer=tf.random_normal_initializer(0, 0.01))\n",
    "        query = tf.nn.dropout(query, 1.0 - dropout)\n",
    "        value = tf.layers.dense(x, units=v_size, activation=None, use_bias=False,\n",
    "                                kernel_initializer=tf.random_normal_initializer(0, 0.01))\n",
    "        value = tf.nn.dropout(value, 1.0 - dropout)\n",
    "        #---------------\n",
    "        # simple matrix multiplication between query and key vector\n",
    "        #---------------\n",
    "        # The second step in calculating self-attention is to calculate a score for each timestep.\n",
    "        # compute the dot products of the query with all keys\n",
    "        logits = tf.matmul(query, key, transpose_b=True)\n",
    "        # scaling prevents dot-product from becoming large, divide the key of dimension\n",
    "        logits = logits / np.sqrt(k_size)\n",
    "        # apply softmax function to obtain the weights on the values\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        # And this weights will be applied to similarities of encoder and decoder states\n",
    "        output = tf.matmul(weights, value)\n",
    "\n",
    "    return output\n",
    "\n",
    "def ResidualBlock(input_layer, out_channels, convolution_width, dilation_rate,\n",
    "                  dropout, atten=False, use_highway=False, gated=False, scope='residual-block', \n",
    "                  reuse=False):\n",
    "    \"\"\"\n",
    "    Residual Block in TCN (Bai 2018)\n",
    "    \n",
    "    Here, we also implement another short connections, alternative to Residual Network, \n",
    "    called Highway Networks [Srivastava 15]\n",
    "    ----------------\n",
    "    Arguments\n",
    "        input_layer: A tensor of shape [N, L, Cin]\n",
    "        out_channels: output dimension\n",
    "        convolution_width: Number of kernel to use in convolution layer.\n",
    "        stride: same as what's need in conv. function\n",
    "        dilation_rate: holes inbetween\n",
    "        dropout: prob. to drop weights\n",
    "        atten: (not in original paper) add self attention block after Conv.\n",
    "        use_highway: (not in original paper) use highway as residual connection\n",
    "        gated: (not in original paper) use gated linear unit as activation\n",
    "    Returns\n",
    "        A Tensor of shape [batch size, max sequence length, out_channels].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # for short connetions\n",
    "        in_channels = input_layer.get_shape()[-1]\n",
    "        #-------------------\n",
    "        # Layer1: Dilated Causal Conv + WeightNorm\n",
    "        #-------------------\n",
    "        conv1 = weightNorm_temporal_convolution_layer(\n",
    "                inputs = input_layer, \n",
    "                output_units = out_channels,\n",
    "                convolution_width = convolution_width,\n",
    "                dilation_rate = [dilation_rate],\n",
    "                causal = True,\n",
    "                bias=True,\n",
    "                activation=None, \n",
    "                dropout=None,\n",
    "                scope='weightNorm-cnn-layer-1',\n",
    "                reuse=False,\n",
    "                gated=gated,\n",
    "                        )\n",
    "        if atten:\n",
    "            conv1 = attentionBlock(conv1, dropout)\n",
    "        #-------------------\n",
    "        # Layer2: Dilated Causal Conv + WeightNorm\n",
    "        #-------------------\n",
    "        conv2 = weightNorm_temporal_convolution_layer(\n",
    "                inputs = conv1, \n",
    "                output_units = out_channels,\n",
    "                convolution_width = convolution_width,\n",
    "                dilation_rate = [dilation_rate],\n",
    "                causal = True,\n",
    "                bias=True,\n",
    "                activation=None, \n",
    "                dropout=None,\n",
    "                scope='weightNorm-cnn-layer-2',\n",
    "                reuse=False,\n",
    "                gated=gated,\n",
    "                        )\n",
    "        if atten:\n",
    "            conv2 = attentionBlock(conv2, dropout)\n",
    "\n",
    "\n",
    "        # highway connetions or residual connection\n",
    "        residual = None\n",
    "        if use_highway:\n",
    "            #---------------\n",
    "            # use highway network as short connections\n",
    "            #---------------\n",
    "            W_h = tf.get_variable('W_h', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_h = tf.get_variable('b_h', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            H = tf.nn.bias_add(tf.nn.convolution(input_layer, W_h, 'SAME'), b_h)\n",
    "\n",
    "            W_t = tf.get_variable('W_t', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_t = tf.get_variable('b_t', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            T = tf.nn.bias_add(tf.nn.convolution(input_layer, W_t, 'SAME'), b_t)\n",
    "            T = tf.nn.sigmoid(T)\n",
    "            residual = H*T + input_layer * (1.0 - T)\n",
    "        elif in_channels != out_channels:\n",
    "            #---------------\n",
    "            # use residual network as short connections\n",
    "            #---------------\n",
    "            # we only use 1 filter aka convolution_width = 1 from the perspective of 1-D convolutions.\n",
    "            W_h = tf.get_variable('W_h', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_h = tf.get_variable('b_h', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            residual = tf.nn.bias_add(tf.nn.convolution(input_layer, W_h, 'SAME'), b_h)\n",
    "        else:\n",
    "            print(\"There is no need to use additional 1x1 convolution to solve discrepant input-output widths\")\n",
    "\n",
    "        res = input_layer if residual is None else residual\n",
    "\n",
    "        return tf.nn.relu(out2 + res)\n",
    "\n",
    "def TemporalConvNet(inputs, num_channels, convolution_width=3,\n",
    "                    dropout=tf.constant(0.0, dtype=tf.float32),\n",
    "                    atten=False, use_highway=False, use_gated=False):\n",
    "    \"\"\"\n",
    "    A stacked dilated CNN architecture described in Bai 2018\n",
    "    ------------\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        num_channels: List of output channels aka output unit for each convolution layer. \n",
    "        convolution_width: Number of kernel to use in convolution layer.\n",
    "        dropout: A scalar Tensor, keep prob.\n",
    "        atten: (not in original paper) add self attention block after Conv.\n",
    "        use_highway: (not in original paper) use highway as residual connection\n",
    "        gated: (not in original paper) use gated linear unit as activation\n",
    "    Returns\n",
    "        A tensor of shape [batch size, max sequence length, num_channels[-1]]\n",
    "    Notice:\n",
    "        len(num_channels) = how many residual block we used in TCN.\n",
    "    ------------\n",
    "    Reference:\n",
    "        - https://github.com/YuanTingHsieh\n",
    "    \"\"\"\n",
    "    num_residual_block = len(num_channels)\n",
    "    for i in range(num_residual_block):\n",
    "        dilation_factor = 2 ** i # each residual block use same convolution_width and dilation factor \n",
    "        out_channels = num_channels[i]\n",
    "        inputs = ResidualBlock(\n",
    "                            inputs, \n",
    "                            out_channels, \n",
    "                            convolution_width, \n",
    "                            stride=1, \n",
    "                            dilation_rate=dilation_factor,\n",
    "                            dropout=dropout, \n",
    "                            atten=atten, \n",
    "                            use_highway = use_highway,\n",
    "                            gated=use_gated\n",
    "                                   )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    #-----------------------\n",
    "    # definition of temporalblock\n",
    "    #-----------------------\n",
    "    # 1.TemporalConvNet\n",
    "    # 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spatial dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
    "    c = tf.constant(1)\n",
    "    dropout = tf.layers.Dropout(0.5, noise_shape=[x.shape[0], tf.constant(1), x.shape[2]], seed = 19921030)\n",
    "    output = dropout(x, training=True)\n",
    "    # \n",
    "    noise_shape = (tf.shape(x)[0], tf.shape(x)[1], tf.constant(1)) #[batch_size,1, output_units]\n",
    "    z = tf.nn.dropout(x, 0.5, noise_shape, seed = 19921030)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g,  config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res,c, z = sess.run([output, c, z])\n",
    "    print(res.shape)   \n",
    "#     print(res[0, :, :])\n",
    "#     print(res[1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  0.        ,  0.        , -2.8364923 ],\n",
       "       [-0.        ,  0.        , -0.        ,  0.64728487],\n",
       "       [ 0.        ,  0.        , -0.        , -0.02653886],\n",
       "       [-0.        ,  0.        , -0.        ,  1.0989823 ],\n",
       "       [-0.        ,  0.        , -0.        , -0.9989221 ],\n",
       "       [-0.        ,  0.        ,  0.        , -1.0415276 ],\n",
       "       [-0.        ,  0.        , -0.        , -3.3377316 ],\n",
       "       [-0.        ,  0.        , -0.        , -0.63642293],\n",
       "       [-0.        , -0.        , -0.        ,  1.1216874 ],\n",
       "       [-0.        , -0.        ,  0.        ,  0.07789448]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  0.        ,  0.        , -0.        ],\n",
       "       [-0.        ,  0.        , -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.        , -0.        ],\n",
       "       [-0.48166808,  1.7931372 , -1.6821005 ,  1.0989823 ],\n",
       "       [-0.        ,  0.        , -0.        , -0.        ],\n",
       "       [-2.1448581 ,  0.40315533,  2.7007635 , -1.0415276 ],\n",
       "       [-0.11056013,  0.27259856, -1.6276945 , -3.3377316 ],\n",
       "       [-0.        ,  0.        , -0.        , -0.        ],\n",
       "       [-1.4779317 , -0.46940562, -2.3209996 ,  1.1216874 ],\n",
       "       [-0.7861813 , -0.35664263,  1.6412584 ,  0.07789448]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  2.7841222 ,  0.3337404 ,  0.        ],\n",
       "       [ 0.        ,  0.6900487 ,  1.2543238 , -0.        ],\n",
       "       [-0.        , -1.3224872 , -2.0129004 ,  0.        ],\n",
       "       [ 0.        , -0.3802217 ,  1.9132335 ,  0.        ],\n",
       "       [-0.        , -1.7185893 , -3.0538573 ,  0.        ],\n",
       "       [-0.        ,  1.0133408 ,  0.5718828 ,  0.        ],\n",
       "       [-0.        , -3.2928517 , -2.1893296 ,  0.        ],\n",
       "       [ 0.        , -1.9574913 , -1.9909762 ,  0.        ],\n",
       "       [-0.        , -1.6311072 ,  0.03458405, -0.        ],\n",
       "       [-0.        , -1.3467032 , -0.29242575,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  2.7841222 ,  0.3337404 ,  0.        ],\n",
       "       [ 0.        ,  0.6900487 ,  1.2543238 , -0.        ],\n",
       "       [-0.        , -1.3224872 , -2.0129004 ,  0.        ],\n",
       "       [ 0.        , -0.3802217 ,  1.9132335 ,  0.        ],\n",
       "       [-0.        , -1.7185893 , -3.0538573 ,  0.        ],\n",
       "       [-0.        ,  1.0133408 ,  0.5718828 ,  0.        ],\n",
       "       [-0.        , -3.2928517 , -2.1893296 ,  0.        ],\n",
       "       [ 0.        , -1.9574913 , -1.9909762 ,  0.        ],\n",
       "       [-0.        , -1.6311072 ,  0.03458405, -0.        ],\n",
       "       [-0.        , -1.3467032 , -0.29242575,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.layers.dense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((2, 5, 3)) # (batch_size, length, channel)\n",
    "    x_size = x.get_shape()[-1]\n",
    "    #\n",
    "    y = tf.layers.dense(x, 2, activation= None)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "# with tf.Session(graph=g,  config = config) as sess:\n",
    "#     # Run the initializer\n",
    "#     sess.run(init)\n",
    "#     x, y, x_size = sess.run([x, \n",
    "#                              y, \n",
    "#                              x_size\n",
    "#                             ])\n",
    "#     print(x.shape)   \n",
    "#     print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_size.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.7393266 , -0.57685965],\n",
       "        [ 0.2382004 ,  0.03669056],\n",
       "        [ 2.0757465 ,  2.1956925 ],\n",
       "        [ 1.4333581 ,  0.8679571 ],\n",
       "        [ 0.7828558 ,  0.04249547]],\n",
       "\n",
       "       [[-0.33551183,  0.36804572],\n",
       "        [ 1.2380608 , -0.04142523],\n",
       "        [-1.6662457 , -1.0496262 ],\n",
       "        [-0.8827214 ,  0.01979199],\n",
       "        [-0.4699133 , -0.6493114 ]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight normalization on 1D convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g1:\n",
    "    #b = tf.convert_to_tensor(np.arange(10), dtype=tf.float32)\n",
    "    #a = tf.convert_to_tensor(np.arange(10), dtype=tf.float32)\n",
    "    num_filters = 2\n",
    "    #V = tf.ones(shape = [1,2,num_filters], dtype = tf.float32)\n",
    "    V = tf.convert_to_tensor(np.arange(12).reshape(2,3,num_filters), dtype=tf.float32)\n",
    "    #g1 = tf.constant(2,shape = [num_filters], dtype = tf.float32)\n",
    "    g = tf.convert_to_tensor(np.arange(2), dtype = tf.float32)\n",
    "    g = tf.reshape(g, [1, 1, num_filters])\n",
    "    #z1 = a+b\n",
    "    #z2 = tf.nn.bias_add(value = a, bias = b)\n",
    "    z3 = tf.nn.l2_normalize(V, dim = [0,1]) # why doing normalizaing along input channels\n",
    "    #z4 = tf.nn.l2_normalize(V, dim = [0])\n",
    "    W = g*z3\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g1, config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    V, z3, W = sess.run([V, z3, W])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.05913124],\n",
       "        [0.        , 0.17739372],\n",
       "        [0.        , 0.2956562 ]],\n",
       "\n",
       "       [[0.        , 0.41391867],\n",
       "        [0.        , 0.53218114],\n",
       "        [0.        , 0.6504436 ]]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.],\n",
       "        [ 2.,  3.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 6.,  7.],\n",
       "        [ 8.,  9.],\n",
       "        [10., 11.]]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.05913124],\n",
       "        [0.13483998, 0.17739372],\n",
       "        [0.26967996, 0.2956562 ]],\n",
       "\n",
       "       [[0.40451995, 0.41391867],\n",
       "        [0.5393599 , 0.53218114],\n",
       "        [0.67419994, 0.6504436 ]]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090910345151304"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i**2 for i in z3[0][:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090910747005774"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i**2 for i in z3[1][:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001781520904"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i**2 for i in z3[0][:,0]]) + sum([i**2 for i in z3[1][:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999590473934"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i**2 for i in z3[0][:,1]]) + sum([i**2 for i in z3[1][:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8776223328347825"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function l2_normalize in module tensorflow.python.ops.nn_impl:\n",
      "\n",
      "l2_normalize(x, dim, epsilon=1e-12, name=None)\n",
      "    Normalizes along dimension `dim` using an L2 norm.\n",
      "    \n",
      "    For a 1-D tensor with `dim = 0`, computes\n",
      "    \n",
      "        output = x / sqrt(max(sum(x**2), epsilon))\n",
      "    \n",
      "    For `x` with more dimensions, independently normalizes each 1-D slice along\n",
      "    dimension `dim`.\n",
      "    \n",
      "    Args:\n",
      "      x: A `Tensor`.\n",
      "      dim: Dimension along which to normalize.  A scalar or a vector of\n",
      "        integers.\n",
      "      epsilon: A lower bound value for the norm. Will use `sqrt(epsilon)` as the\n",
      "        divisor if `norm < sqrt(epsilon)`.\n",
      "      name: A name for this operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with the same shape as `x`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.l2_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090910103200199"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.13483998**2 + 0.26967996**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4472135954999579"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 / np.sqrt(4+16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8944271909999159"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / np.sqrt(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "[[0.26726124 0.5345225  0.8017837 ]\n",
      " [0.45584232 0.5698029  0.6837635 ]\n",
      " [0.5025707  0.5743665  0.64616233]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_data = tf.constant([[1.0,2,3],[4.0,5,6],[7.0,8,9]])\n",
    "\n",
    "output = tf.nn.l2_normalize(input_data, dim = 1)\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(input_data))\n",
    "    print (sess.run(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g1:\n",
    "    num_units = 2 # output_channel\n",
    "    input_size = 3\n",
    "    V = tf.convert_to_tensor(np.arange(6).reshape(input_size,num_units), dtype=tf.float32)\n",
    "    V_norm = tf.nn.l2_normalize(V, dim = [0]) \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g1, config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    V,V_norm = sess.run([V, V_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [2., 3.],\n",
       "       [4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.16903085],\n",
       "       [0.4472136 , 0.50709254],\n",
       "       [0.8944272 , 0.8451542 ]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(1+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000201248"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4472136**2 + 0.8944272**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998941530142"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.16903085**2 + 0.50709254**2 + 0.8451542**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3713906763541037"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/np.sqrt(4+25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000845878612"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.24253564**2+0.97014254**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    t1 = np.array([[[1, 2], [2, 3]], [[4, 4], [5, 3]]])\n",
    "    t2 = np.array([[[7, 4], [8, 4]], [[2, 10], [15, 11]]])\n",
    "    x = tf.concat([t1, t2], -1)\n",
    "    c = tf.concat([t1, t2], 2)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    x,c = sess.run([x, c])\n",
    "\n",
    "print ('x', x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7,  4],\n",
       "        [ 8,  4]],\n",
       "\n",
       "       [[ 2, 10],\n",
       "        [15, 11]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  7,  4],\n",
       "        [ 2,  3,  8,  4]],\n",
       "\n",
       "       [[ 4,  4,  2, 10],\n",
       "        [ 5,  3, 15, 11]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 2), (2, 2, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape, t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.sequence_mask(lengths, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    #-------------------\n",
    "    # 1-D mask tensor\n",
    "    #-------------------\n",
    "    # for the N first point\n",
    "    z = tf.sequence_mask(lengths = [1, 3, 2], maxlen = 5)\n",
    "    z_ = tf.cast(z, tf.float32) # (3, 5) [len(input), maxlen]\n",
    "    #-------------------\n",
    "    # 2-D mask tensor\n",
    "    #-------------------\n",
    "    \n",
    "    x = tf.sequence_mask(lengths = [[1,10,3],[2,0,3]], maxlen = 5) # [batch_size, len(input), maxlen]\n",
    "    x_ = tf.cast(x, tf.float32)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    z,z_,x_ = sess.run([z, z_, x_])\n",
    "\n",
    "print ('z',z.shape)\n",
    "print ('z_',z_.shape)\n",
    "print ('x_',x_.shape)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.nn.softmax_cross_entropy_with_logits(labels = y, logits= y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with tf.Graph().as_default() as g:\n",
    "    #---------\n",
    "    # input\n",
    "    #---------\n",
    "    y_hat = np.arange(12).reshape(2,2,3) # [batch_size, max_seq_lenght, num_class]\n",
    "    y = np.array([\n",
    "    [[2,1,0],[2,0,0]],\n",
    "    [[2,1,0],[2,0,0]]\n",
    "                    ])\n",
    "    y_hat = tf.convert_to_tensor(value = y_hat, dtype = tf.float32)\n",
    "    y = tf.convert_to_tensor(value = y, dtype = tf.float32)\n",
    "    \n",
    "    softmax_losses = tf.nn.softmax_cross_entropy_with_logits(labels = y, logits= y_hat) # [batch_size, max_seq_lenght]\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    softmax_losses, y, y_hat = sess.run([softmax_losses, y, y_hat])\n",
    "    \n",
    "    print('softmax_losses', softmax_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what tf.nn.softmax_cross_entropy_with_logits(labels = y, logits= y_hat) do \n",
    "# Step1:softmax of y_hat\n",
    "# Step2:compute cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "with tf.Graph().as_default() as g:\n",
    "    #our NN's output\n",
    "    logits = tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "    #step1:do softmax\n",
    "    y = tf.nn.softmax(logits)\n",
    "    #true label\n",
    "    y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])\n",
    "    #step2:do cross_entropy\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #do cross_entropy just one step\n",
    "    softmax_loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y_)\n",
    "    cross_entropy2 = tf.reduce_sum(softmax_loss)#dont forget tf.reduce_sum()!!\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    y_hat, softmax_loss, y_true =sess.run([y, softmax_loss, y_])\n",
    "    c_e = sess.run(cross_entropy)\n",
    "    c_e2 = sess.run(cross_entropy2)\n",
    "    print(\"step1:softmax result=\")\n",
    "    print(y_true)\n",
    "    print(\"step2:cross_entropy result=\")\n",
    "    print(c_e)\n",
    "    print(\"Function(softmax_cross_entropy_with_logits) result=\")\n",
    "    print(c_e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_loss # (1, 3) [batch_size, max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "\n",
    "    labels = np.array([[0.2,0.3,0.5],\n",
    "              [0.1,0.6,0.3]]).reshape(1,2,3)\n",
    "    logits = np.array([[2,0.5,1],\n",
    "              [0.1,1,3]]).reshape(1,2,3)\n",
    "    logits_scaled = tf.nn.softmax(logits)\n",
    "\n",
    "    result1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    result2 = -tf.reduce_sum(labels*tf.log(logits_scaled),1)\n",
    "    result3 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits_scaled)\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    result1 = sess.run(result1)\n",
    "    result2 = sess.run(result2)\n",
    "    result3 = sess.run(result3)\n",
    "    print ('result1', result1)\n",
    "    print ('result2', result2)\n",
    "    print ('result3', result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= y_hat) do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    #batch_size = 2\n",
    "    labels = tf.constant([[0, 0, 0, 1],[0, 1, 0, 0]])\n",
    "    logits = tf.constant([[-3.4, 2.5, -1.2, 5.5],[-3.4, 2.5, -1.2, 5.5]])\n",
    "    labels_sparse = tf.argmax(labels,1)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss_s = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_sparse, logits=logits)\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:  \n",
    "    print (\"softmax loss:\", sess.run(loss))\n",
    "    print (\"sparse softmax loss:\", sess.run(loss_s))\n",
    "    print ('labels_sparse : ', sess.run(labels_sparse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    #batch_size = 2\n",
    "    labels_ = tf.constant([[0, 0, 1],[0, 1, 0]])\n",
    "    labels = tf.reshape(labels_, [1,2,3])\n",
    "    logits = tf.constant([[-3.4, 2.5, -1.2],[-0.5, 2.5, -1.2]])\n",
    "    logits = tf.reshape(logits, [1,2,3])\n",
    "    \n",
    "    \n",
    "    labels_sparse = tf.argmax(labels_,1)\n",
    "    labels_sparse = tf.reshape(labels_sparse, [1,2])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss_s = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_sparse, logits=logits)\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:  \n",
    "    loss = sess.run(loss)\n",
    "    loss_s = sess.run(loss_s)\n",
    "    labels_sparse = sess.run(labels_sparse)\n",
    "    logits = sess.run(logits)\n",
    "    print (\"softmax loss:\", loss)\n",
    "    print (\"sparse softmax loss:\", loss_s)\n",
    "    print ('labels_sparse : ', labels_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "denominator = math.exp(-3.4) + math.exp(2.5) + math.exp(-1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(-0.5) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(2.5) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(-1.2) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "math.log(0.024062693412026315) * -1 # 我們預測是這類的機率很小, 但真實卻是這類, loss就很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(0.9306562492765156) * -1 # 反之, 我們預測是這類的機率很大, 但真實卻是這類, loss就很小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    batch_size = 3\n",
    "    x = tf.range(batch_size) # (3,)\n",
    "    history_length = tf.constant(1, shape = [batch_size])\n",
    "    stacked = tf.stack( [x, history_length], axis = 1)\n",
    "    params = tf.reshape(tf.range(start = 0, limit =15, dtype = tf.int32),[3,5,1])\n",
    "    final_states = tf.gather_nd(params = params, indices = stacked)\n",
    "with tf.Session(graph=g, config = config) as sess:  \n",
    "    x, history_length,stacked,params, final_states = sess.run([x, history_length,stacked,params, final_states])\n",
    "    print ('x : ', x)\n",
    "    print ('history_length', history_length)\n",
    "    print ('stacked', stacked)\n",
    "    print ('final_states',final_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gather_nd(params, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_temporal_idx = np.array(\n",
    "    [[ 0, 11],\n",
    "     [ 1, 11],\n",
    "     [ 2, 11]]\n",
    ")\n",
    "\n",
    "y_hat_ = tf.gather_nd(params = y_hat, indices = final_temporal_idx)\n",
    "with tf.Session(config = config) as sess1:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess1.run(init)\n",
    "    y_hat_ = sess1.run(y_hat_)\n",
    "print ('y_hat',y_hat_.shape)\n",
    "y_hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.argmax(input,axis=None,name=None,dimension=None,output_type=tf.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.arange(12).reshape(1,4,3)\n",
    "test = np.arange(4)\n",
    "\n",
    "y_hat_ = tf.cast(tf.argmax(preds, axis=2),tf.int32)\n",
    "test_ = tf.expand_dims(test, axis = 1)\n",
    "with tf.Session(config = config) as sess1:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess1.run(init)\n",
    "    y_hat_,test_ = sess1.run([y_hat_, test_])\n",
    "print ('y_hat',y_hat_.shape)\n",
    "print ('test_',test_.shape)\n",
    "y_hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.arange(12).reshape(1,4,3)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict. dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "        chunk_start: it represents this tag start with index of this list.\n",
    "        chunk_end:it represents this tag end with index of this list + 1.\n",
    "    Example 1:\n",
    "        seq = [2,0,0]\n",
    "        tags = {'B-B': 2, 'I-B': 1, 'O': 0}\n",
    "        result = [('B', 0, 1)]\n",
    "    Example 2:\n",
    "        seq = [2,1,0]\n",
    "        tags = {'B-B': 2, 'I-B': 1, 'O': 0}\n",
    "        result = [('B', 0, 2)]\n",
    "    Example 3:\n",
    "        seq = [0,2,1]\n",
    "        tags = {'B-B': 2, 'I-B': 1, 'O': 0}\n",
    "        result = [('B', 1, 3)]\n",
    "    Example 4:\n",
    "        seq = [0,0,0]\n",
    "        tags = {'B-B': 2, 'I-B': 1, 'O': 0}\n",
    "        result = []\n",
    "    \"\"\"\n",
    "    # setting\n",
    "    NONE = \"O\" # for get_chunks function\n",
    "\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(8).reshape(4,2)\n",
    "e = np.arange(8).reshape(4,2)\n",
    "f = np.array([1,2,1,3])\n",
    "\n",
    "for a,b,c in zip(d,e,f):\n",
    "    print (a)\n",
    "    #print (b)\n",
    "    #print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {'B-B': 2, 'I-B': 1, 'O': 0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_preds () 1.0\n",
      "total_correct () 3.0\n",
      "total_preds () 2.0\n",
      "p 0.5\n",
      "r 0.33333334\n",
      "f1 0.4\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[2,1,0,0], [2,1,0,0],[0,2,0,0], [0,0,0,0],[0,0,0,0]]) # (5,4)\n",
    "y_pred = np.array([[2,0,0,0],[2,1,0,0],[0,0,0,0], [0,0,0,0],[0,0,0,0]])\n",
    "# make y_true where tagging of each timestep is zero become False. Otherwise, True.\n",
    "y_true_ = tf.reduce_sum(y_true, axis = 1)\n",
    "y_true_ = tf.not_equal(y_true_, tf.zeros(5, dtype = tf.int64)) # batch_szie\n",
    "\n",
    "# compare if y_ture is equal to y_pred element-wise\n",
    "correct_preds = tf.equal(y_true, y_pred) # Returns boolean tensor where the truth value of (x == y) element-wise.\n",
    "# \n",
    "correct_preds = tf.cast(tf.logical_and(y_true_, tf.reduce_all(correct_preds, axis = 1)), tf.float32)\n",
    "correct_preds = tf.cast(tf.count_nonzero(correct_preds, axis = None),tf.float32)\n",
    "\n",
    "total_correct = tf.cast(tf.count_nonzero(tf.count_nonzero(y_true, axis = 1), axis = None),tf.float32)\n",
    "total_preds = tf.cast(tf.count_nonzero(tf.count_nonzero(y_pred, axis = 1), axis = None), tf.float32) # 0-D, number of our rediction which is non-zero\n",
    "p = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_preds, lambda: tf.zeros(shape=[]))\n",
    "r = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_correct, lambda: tf.zeros(shape=[]))\n",
    "f1 = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : 2 * p * r / (p + r), lambda: tf.zeros(shape=[]))\n",
    "\n",
    "c = tf.greater(4,3)\n",
    "d = tf.zeros(shape = [])\n",
    "with tf.Session(config = config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    correct_preds, total_correct,total_preds, p,r,f1= sess.run([correct_preds, total_correct,total_preds,p,r,f1])\n",
    "    y_true_ = sess.run(y_true_)\n",
    "#print ('correct_preds_1',correct_preds_1.shape,correct_preds_1)\n",
    "#print ('correct_preds_2', correct_preds_2.shape, correct_preds_2)\n",
    "print ('correct_preds',correct_preds.shape, correct_preds)\n",
    "print ('total_correct',total_correct.shape, total_correct)\n",
    "print ('total_preds',total_preds.shape, total_preds)\n",
    "print ('p', p)\n",
    "print ('r', r)\n",
    "print ('f1', f1)\n",
    "#print ('y_true_', y_true_)\n",
    "# print ('y_pred_', y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# output\n",
    "#---------------\n",
    "p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.3333333333333333, 0.4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,r,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.constant([[True,  True, True], [False, False, True]])\n",
    "    a = tf.reduce_all(x)  # False\n",
    "    b = tf.reduce_all(x, 0)  # [False, False]\n",
    "    c = tf.reduce_all(x, 1)  # [True, False]\n",
    "with tf.Session(graph = g,config = config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a,b,c = sess.run([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.583333333333334"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "175 * 5 /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    t_a = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
    "    t = tf.constant(t_a)\n",
    "    c = tf.shape(t)  # [2, 2, 3]\n",
    "with tf.Session(graph = g,config = config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    c,t = sess.run([c, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8359abf235d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
