{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Oct 2 2018\n",
    "\n",
    "Updated on Oct 24 2018\n",
    "\n",
    "Prepare data for the following tensorflow model.\n",
    "\n",
    "Noticed:\n",
    "    It may take around 13.935157557328543 mins, the real number depends on ur machine.\n",
    "\n",
    "@author: Ray\n",
    "\n",
    "\n",
    "Reference:\n",
    "\t- LSTM character embedding : https://github.com/cristianoBY/Sequence-Tagging-NER-TensorFlow:\n",
    "    - character embedding: https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "sys.path.append('/home/ld-sgdev/yunrui_li/ner_project/brand_recognition_bio_FE/preprocessing')\n",
    "sys.path.append('/home/ld-sgdev/yunrui_li/ner_project/brand_recognition_bio_FE/py_model')\n",
    "sys.path.append('../models/')\n",
    "from clean_helpers import clean_name_for_word_embedding\n",
    "from utils import init_logging\n",
    "from data_utils import get_glove_vocab\n",
    "from data_utils import write_vocab\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "from data_utils import export_glove_vectors\n",
    "from data_utils import get_char_vocab\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "\n",
    "def pad_1d(array, max_len, word_padding = True):\n",
    "    if word_padding == True:\n",
    "        array = array[:max_len]\n",
    "        length = len(array)\n",
    "        padded = array + [9858]*(max_len - len(array)) # padded index of unknown.\n",
    "    else:\n",
    "        array = array[:max_len]\n",
    "        length = len(array)\n",
    "        padded = array + [0]*(max_len - len(array)) # padded with zero.\n",
    "    return padded, length\n",
    "\n",
    "def encode_word_to_idx(word, word_to_id, vocabulary_set, lowercase = True, allow_unknown = True):\n",
    "    '''encode a word (string) into id'''\n",
    "\n",
    "    # 1. preprocess word\n",
    "    if lowercase:\n",
    "        word = word.lower()\n",
    "    if word.isdigit():\n",
    "        word = NUM\n",
    "\n",
    "    # 2. get id of word\n",
    "    if word in vocabulary_set:\n",
    "        return word_to_id[word]\n",
    "    else:\n",
    "        if allow_unknown:\n",
    "            return word_to_id[UNK]\n",
    "        else:\n",
    "            raise Exception(\"Unknow key is not allowed. Check that your vocab (tags?) is correct\")\n",
    "\n",
    "#--------------------\n",
    "# setting\n",
    "#--------------------\n",
    "TRACE_CODE = False # for tracing funtionality and developing quickly\n",
    "TRUNCATED = False # for reducing memory \n",
    "USE_CHARS = True # for character embedding\n",
    "LOWERCASE = True\n",
    "ALLOW_UNKNOWN = True\n",
    "dim_word = 300\n",
    "UNK = \"$UNK$\" # for the word in our own courpus which is unknown in embedding\n",
    "NUM = \"$NUM$\" # for the word which is number\n",
    "\n",
    "\n",
    "pre_trained_word_embedding_path = \"/data/ID_largewv_300_2.txt\"\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "log_dir = 'log/' # log path\n",
    "init_logging(log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lips', 'face', 'mobile', 'dress', 'women_top']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[:-13] for i in os.listdir('../data/processed/') if 'training.' in i ][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------category----------- lips\n",
      "# sku : 44031\n",
      "# validation ratio : train    0.899918\n",
      "val      0.100082\n",
      "Name: eval_set, dtype: float64\n",
      "distribution of data train    31660\n",
      "test      8850\n",
      "val       3521\n",
      "Name: eval_set, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 0.04522428512573242 mins on preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 6629250 tokens\n",
      "Writing vocab...\n",
      "- done. 7889 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 13.189097519715627 mins on word embedding \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 40 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_seq_length : 27\n",
      "max_word_length : 71\n",
      "number of sequences : 31660\n",
      "100%|██████████| 31660/31660 [00:21<00:00, 1478.65it/s]\n",
      "number of sequences : 3521\n",
      "100%|██████████| 3521/3521 [00:02<00:00, 1532.89it/s]\n",
      "number of sequences : 44029\n",
      "100%|██████████| 44029/44029 [00:28<00:00, 1543.60it/s]\n",
      "shape of df : (347497, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path ../data/processed/lips_w_word_id.csv\n",
      "-----------category----------- face\n",
      "# sku : 73826\n",
      "# validation ratio : train    0.899877\n",
      "val      0.100123\n",
      "Name: eval_set, dtype: float64\n",
      "distribution of data train    52848\n",
      "test     15098\n",
      "val       5880\n",
      "Name: eval_set, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 0.07405883073806763 mins on preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 6629250 tokens\n",
      "Writing vocab...\n",
      "- done. 9408 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 13.0612610856692 mins on word embedding \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 41 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_seq_length : 28\n",
      "max_word_length : 48\n",
      "number of sequences : 52848\n",
      "100%|██████████| 52848/52848 [00:35<00:00, 1495.59it/s]\n",
      "number of sequences : 5880\n",
      "100%|██████████| 5880/5880 [00:03<00:00, 1474.54it/s]\n",
      "number of sequences : 73824\n",
      "100%|██████████| 73824/73824 [00:48<00:00, 1516.08it/s]\n",
      "shape of df : (642031, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path ../data/processed/face_w_word_id.csv\n",
      "-----------category----------- mobile\n",
      "# sku : 186349\n",
      "# validation ratio : train    0.899843\n",
      "val      0.100157\n",
      "Name: eval_set, dtype: float64\n",
      "distribution of data train    143740\n",
      "test      26610\n",
      "val       15999\n",
      "Name: eval_set, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 0.21230234305063883 mins on preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 6629250 tokens\n",
      "Writing vocab...\n",
      "- done. 24106 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 12.600360731283823 mins on word embedding \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 41 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_seq_length : 32\n",
      "max_word_length : 54\n",
      "number of sequences : 143740\n",
      "100%|██████████| 143740/143740 [01:33<00:00, 1545.32it/s]\n",
      "number of sequences : 15999\n",
      "100%|██████████| 15999/15999 [00:11<00:00, 1425.68it/s]\n",
      "number of sequences : 186344\n",
      "100%|██████████| 186344/186344 [01:59<00:00, 1559.49it/s]\n",
      "shape of df : (1844485, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path ../data/processed/mobile_w_word_id.csv\n",
      "-----------category----------- dress\n",
      "# sku : 3845\n",
      "# validation ratio : train    0.899142\n",
      "val      0.100858\n",
      "Name: eval_set, dtype: float64\n",
      "distribution of data train    3040\n",
      "test      464\n",
      "val       341\n",
      "Name: eval_set, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 0.005031935373942057 mins on preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 6629250 tokens\n",
      "Writing vocab...\n",
      "- done. 2023 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 12.721617607275645 mins on word embedding \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 39 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_seq_length : 19\n",
      "max_word_length : 78\n",
      "number of sequences : 3040\n",
      "100%|██████████| 3040/3040 [00:02<00:00, 1395.53it/s]\n",
      "number of sequences : 341\n",
      "100%|██████████| 341/341 [00:00<00:00, 1339.01it/s]\n",
      "number of sequences : 3845\n",
      "100%|██████████| 3845/3845 [00:02<00:00, 1476.05it/s]\n",
      "shape of df : (42558, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path ../data/processed/dress_w_word_id.csv\n",
      "-----------category----------- women_top\n",
      "# sku : 7807\n",
      "# validation ratio : train    0.900231\n",
      "val      0.099769\n",
      "Name: eval_set, dtype: float64\n",
      "distribution of data train    5838\n",
      "test     1322\n",
      "val       647\n",
      "Name: eval_set, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 0.008811986446380616 mins on preprocessing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 6629250 tokens\n",
      "Writing vocab...\n",
      "- done. 4276 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it spend 12.774761120478312 mins on word embedding \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 39 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_seq_length : 20\n",
      "max_word_length : 28\n",
      "number of sequences : 5838\n",
      "100%|██████████| 5838/5838 [00:03<00:00, 1508.23it/s]\n",
      "number of sequences : 647\n",
      "100%|██████████| 647/647 [00:00<00:00, 1393.20it/s]\n",
      "number of sequences : 7807\n",
      "100%|██████████| 7807/7807 [00:05<00:00, 1467.75it/s]\n",
      "shape of df : (78717, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path ../data/processed/women_top_w_word_id.csv\n"
     ]
    }
   ],
   "source": [
    "#-------------------\n",
    "# loading data\n",
    "#-------------------\n",
    "\n",
    "for category in [i[:-13] for i in os.listdir('../data/processed/') if 'training.' in i ][:]:\n",
    "    print ('-----------category-----------', category)\n",
    "    # setting\n",
    "    base_dir = \"../models/data/wordvec/{}\".format(category)\n",
    "    filename_words_voc = \"../models/data/wordvec/{}/words_vocab.txt\".format(category)\n",
    "    filename_chars_voc = \"../models/data/wordvec/{}/chars_vocab.txt\".format(category)\n",
    "    # loading data\n",
    "    date_path = '../data/processed/{}_training.csv'.format(category)\n",
    "    df = pd.read_csv(date_path)\n",
    "    # analyze\n",
    "    sku_df = df.groupby('item_name').eval_set.value_counts().to_frame('counts').reset_index()\n",
    "    print ('# sku : {}'.format(len(sku_df)))\n",
    "    print ('# validation ratio : {}'.format( sku_df[sku_df.eval_set != 'test'].eval_set.value_counts(normalize=True)))\n",
    "    print ('distribution of data', sku_df.eval_set.value_counts())\n",
    "    del sku_df\n",
    "    # preprocessing\n",
    "    s = time.time()\n",
    "    \n",
    "    df.dropna(subset = ['tokens'], inplace = True)\n",
    "    \n",
    "    df['clean_tokens'] = df.tokens.apply(lambda x: clean_name_for_word_embedding(x) if type(x)==str else x)\n",
    "    if LOWERCASE:\n",
    "        df['clean_tokens'] = df.clean_tokens.apply(lambda x: x.lower() if type(x)==str else x)\n",
    "    df['clean_tokens'] = df.clean_tokens.astype(str)\n",
    "\n",
    "    # item_id\n",
    "    item_dict = {}\n",
    "    for i, i_n in enumerate(df.item_name.unique().tolist()):\n",
    "        item_dict[i_n] = i+1\n",
    "    df['item_id'] = [item_dict[i_n] for i_n in df.item_name.tolist()]\n",
    "\n",
    "    e = time.time()\n",
    "    logging.info('it spend {} mins on preprocessing'.format( (e-s) / 60.0))\n",
    "    #-------------------\n",
    "    # word embedding \n",
    "    #-------------------\n",
    "    s = time.time()\n",
    "    # Build Word vocab (vocabulary set) for our customized task\n",
    "    vocab_glove = get_glove_vocab(pre_trained_word_embedding_path) # word set from pre-trained word_embedding\n",
    "    vocab_words = set(df.clean_tokens.tolist()) # word set from our own whole corpurs including train, dev, and test\n",
    "    vocab_set = vocab_words & vocab_glove # 這裡面的字, 肯定都有相對應的vector, 不是zero vector(The reason we did是可以節省我們使用的embbedding大小, 不用沒用到的pre-trained字也佔memory)\n",
    "    vocab_set.add(UNK)\n",
    "    vocab_set.add(NUM)\n",
    "\n",
    "    # Save vocab\n",
    "    write_vocab(vocab_set, base_dir, filename_words_voc)\n",
    "    # create dictionary mapping word to index\n",
    "    word_to_id_dict = load_vocab_and_return_word_to_id_dict(filename_words_voc)\n",
    "    # save word embedding matrix \n",
    "    export_glove_vectors(word_to_id_dict, glove_filename = pre_trained_word_embedding_path,\n",
    "                         output_filename = filename_words_vec, dim = dim_word)\n",
    "    # encode a word (string) into id\n",
    "    df['word_id'] = df.clean_tokens.apply( lambda x: encode_word_to_idx(x, word_to_id_dict, vocab_set, LOWERCASE, ALLOW_UNKNOWN))\n",
    "\n",
    "    e = time.time()\n",
    "    logging.info('it spend {} mins on word embedding '.format( (e-s) / 60.0)) # it spend 16.531146574020386 mins on word embedding over 37788 words in volcabulary.\n",
    "\n",
    "    #-------------------\n",
    "    # chracter embedding \n",
    "    #-------------------\n",
    "    if USE_CHARS == True:\n",
    "        dim_char = 100\n",
    "        # Build Char vocab (vocabulary set) \n",
    "        \n",
    "        vocab_chars = get_char_vocab([w for w in df.tokens if type(w)!= float])\n",
    "        # Save Char vocab\n",
    "        write_vocab(vocab_chars, base_dir, filename_chars_voc)\n",
    "        # create dictionary mapping char to index\n",
    "        chars_to_id_dict = load_vocab_and_return_word_to_id_dict(filename_chars_voc)\n",
    "        # get max_word_length for padding later\n",
    "        word_len_distribution = [len(w) for w in df.tokens if type(w)!= float]\n",
    "        max_word_length = max(word_len_distribution)\n",
    "    \n",
    "    # max_seq_length\n",
    "    seq_len_distribution = df.groupby('item_name').tokens.apply( lambda x : len(x.tolist())).to_frame('seq_len').reset_index()\n",
    "\n",
    "    if TRUNCATED == False:\n",
    "        if TRACE_CODE == True:\n",
    "            max_seq_length = 122\n",
    "        else:\n",
    "            max_seq_length = seq_len_distribution.seq_len.max()\n",
    "    else:\n",
    "        max_seq_length = 100\n",
    "\n",
    "    logging.info('max_seq_length : {}'.format( max_seq_length)) # max length of sentence\n",
    "    logging.info('max_word_length : {}'.format( max_word_length)) # max length of word\n",
    "\n",
    "    #-------------------\n",
    "    # output\n",
    "    #-------------------\n",
    "\n",
    "    for i in range(3):\n",
    "        # \n",
    "        if i == 0:\n",
    "            name = 'train'\n",
    "            output = df[df.eval_set == 'train']\n",
    "        elif i == 1:\n",
    "            name = 'val'\n",
    "            output = df[df.eval_set == 'val']\n",
    "        else:\n",
    "            name = 'test'\n",
    "            output = df.copy()\n",
    "        # setting\n",
    "        num_sentences = output['item_name'].nunique()\n",
    "        logging.info('number of sequences : {}'.format(num_sentences))\n",
    "        # 1-D\n",
    "        eval_set = np.zeros(shape=[num_sentences], dtype='S5')\n",
    "        item_id = np.zeros(shape=[num_sentences], dtype=np.int32) # for recording\n",
    "        history_length = np.zeros(shape=[num_sentences], dtype=np.int32) # length of sentence\n",
    "        # 2-D\n",
    "        word_id = np.zeros(shape=[num_sentences, max_seq_length], dtype=np.int32)\n",
    "        label = np.zeros(shape=[num_sentences, max_seq_length], dtype=np.int32)\n",
    "        word_length = np.zeros(shape=[num_sentences, max_seq_length], dtype=np.int32) # length of words \n",
    "        # 3-D\n",
    "        char_id = np.zeros(shape=[num_sentences, max_seq_length, max_word_length], dtype=np.int32)\n",
    "        i = 0\n",
    "        for ix, df_ in tqdm(output.groupby('item_name')):\n",
    "            #logging.info('item_id : {}'.format(i))\n",
    "            # 1-D\n",
    "            eval_set[i] = df_['eval_set'].iloc[0]\n",
    "            item_id[i] = df_['item_id'].iloc[0]\n",
    "            # 2-D\n",
    "            word_id[i, :], history_length[i] = pad_1d(list(map(int, df_['word_id'])), max_len = max_seq_length, word_padding = True)\n",
    "            label[i, :], _ = pad_1d(list(map(int, df_['label'])), max_len = max_seq_length, word_padding = False)\n",
    "            word_length[i, :], _ = pad_1d([len([char for char in w]) for w in df_['tokens'].tolist()], max_len = max_seq_length, word_padding = False)\n",
    "            if USE_CHARS == True:\n",
    "                # 3-D\n",
    "                for i_word_axis, w in enumerate(df_['tokens'].tolist()):\n",
    "                    char_id[i, i_word_axis, : ], _ = pad_1d([chars_to_id_dict[char] for char in w], max_len = max_word_length, word_padding = False)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #--------------------------\n",
    "        # save\n",
    "        #--------------------------\n",
    "        base_path = '/data/ner_task/data_for_brand_detection_model/{}/{}'.format(category, name)\n",
    "        if not os.path.isdir(base_path):\n",
    "            os.makedirs(base_path)    \n",
    "\n",
    "        if TRACE_CODE == True:\n",
    "            np.save(os.path.join(base_path, 'eval_set_0.npy'), eval_set)\n",
    "            np.save(os.path.join(base_path, 'word_id_0.npy'), word_id)\n",
    "            np.save(os.path.join(base_path, 'history_length_0.npy'), history_length)\n",
    "            np.save(os.path.join(base_path, 'label_0.npy'), label)\n",
    "            np.save(os.path.join(base_path, 'item_id_0.npy'), item_id)\n",
    "            if USE_CHARS == True:\n",
    "                np.save(os.path.join(base_path, 'char_id_0.npy'), char_id)\n",
    "                np.save(os.path.join(base_path, 'word_length_0.npy'), word_length)\n",
    "        else:\n",
    "            np.save(os.path.join(base_path, 'eval_set.npy'), eval_set)\n",
    "            np.save(os.path.join(base_path, 'word_id.npy'), word_id)\n",
    "            np.save(os.path.join(base_path, 'history_length.npy'), history_length)\n",
    "            np.save(os.path.join(base_path, 'label.npy'), label)\n",
    "            np.save(os.path.join(base_path, 'item_id.npy'), item_id)\n",
    "            if USE_CHARS == True:\n",
    "                np.save(os.path.join(base_path, 'char_id.npy'), char_id)\n",
    "                np.save(os.path.join(base_path, 'word_length.npy'), word_length)\n",
    "\n",
    "\n",
    "    logging.info('shape of df : {}'.format(df.shape))\n",
    "    save_path = '../data/processed/{}_w_word_id.csv'.format(category)\n",
    "    print ('save_path', save_path)\n",
    "    df.to_csv(save_path, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = [w for w in df.tokens if type(w) == float]\n",
    "# df[df.tokens.isin(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lips': {'max_seq_length': 27, 'max_word_length': 71}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_category = {\n",
    "    'lips':{'max_seq_length':27,'max_word_length':71},\n",
    "    'face':{'max_seq_length':28,'max_word_length':48},\n",
    "    'mobile':{'max_seq_length':32,'max_word_length':54},\n",
    "    'face':{'max_seq_length':28,'max_word_length':48},\n",
    "    'face':{'max_seq_length':28,'max_word_length':48},\n",
    "\n",
    "}\n",
    "dict_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
