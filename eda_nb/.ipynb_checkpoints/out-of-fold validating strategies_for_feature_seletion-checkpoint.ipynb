{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "sys.path.append('../models/blend/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# setting\n",
    "#---------------------\n",
    "BINARY_SCENARIO = None\n",
    "#---------------------\n",
    "# load features\n",
    "#---------------------\n",
    "feature_dir = '../features/lazada_and_amazon/all_features.h5'\n",
    "df = pd.read_hdf(feature_dir)\n",
    "#---------------------\n",
    "# label post-processing\n",
    "#---------------------\n",
    "if df.label.nunique() == 2: \n",
    "    BINARY_SCENARIO = True\n",
    "    # binary class\n",
    "    df['label'] = df.label.apply(lambda x: 1 if x == 2 else 0) # for customized f1 score inference of lgb\n",
    "else:\n",
    "    # multi-class(B, I or O)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238668, 575)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>item_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>if_tokens_has_numbers_in_the_str</th>\n",
       "      <th>if_start_with_capital_chars</th>\n",
       "      <th>percentage_of_upper_chars_in_token</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_292</th>\n",
       "      <th>dim_293</th>\n",
       "      <th>dim_294</th>\n",
       "      <th>dim_295</th>\n",
       "      <th>dim_296</th>\n",
       "      <th>dim_297</th>\n",
       "      <th>dim_298</th>\n",
       "      <th>dim_299</th>\n",
       "      <th>dim_300</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>samsung</td>\n",
       "      <td>1</td>\n",
       "      <td>7054</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.158214</td>\n",
       "      <td>-3.077862</td>\n",
       "      <td>4.290862</td>\n",
       "      <td>4.385282</td>\n",
       "      <td>4.017592</td>\n",
       "      <td>3.681826</td>\n",
       "      <td>-4.147452</td>\n",
       "      <td>7.351630</td>\n",
       "      <td>4.044105</td>\n",
       "      <td>0.169688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White</td>\n",
       "      <td>Galaxy</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>1</td>\n",
       "      <td>537</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766951</td>\n",
       "      <td>-3.085364</td>\n",
       "      <td>4.416570</td>\n",
       "      <td>2.265839</td>\n",
       "      <td>4.203361</td>\n",
       "      <td>0.191773</td>\n",
       "      <td>-1.871406</td>\n",
       "      <td>9.093953</td>\n",
       "      <td>2.210504</td>\n",
       "      <td>0.194254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White</td>\n",
       "      <td>J1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>j1</td>\n",
       "      <td>1</td>\n",
       "      <td>10138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.633618</td>\n",
       "      <td>1.691233</td>\n",
       "      <td>-3.085515</td>\n",
       "      <td>-1.096208</td>\n",
       "      <td>0.355705</td>\n",
       "      <td>0.258805</td>\n",
       "      <td>-0.344883</td>\n",
       "      <td>0.036787</td>\n",
       "      <td>1.097038</td>\n",
       "      <td>0.377184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White</td>\n",
       "      <td>Ace</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>ace</td>\n",
       "      <td>1</td>\n",
       "      <td>10469</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.824487</td>\n",
       "      <td>0.047915</td>\n",
       "      <td>-1.890348</td>\n",
       "      <td>0.103570</td>\n",
       "      <td>0.449590</td>\n",
       "      <td>1.808830</td>\n",
       "      <td>0.450412</td>\n",
       "      <td>0.646871</td>\n",
       "      <td>0.522857</td>\n",
       "      <td>0.402201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>9108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603993</td>\n",
       "      <td>0.499419</td>\n",
       "      <td>0.748781</td>\n",
       "      <td>1.345417</td>\n",
       "      <td>-1.621760</td>\n",
       "      <td>0.530300</td>\n",
       "      <td>0.549422</td>\n",
       "      <td>1.383292</td>\n",
       "      <td>1.529812</td>\n",
       "      <td>0.362081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 575 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          item_name   tokens  label is_valid  \\\n",
       "0   Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White  Samsung      1    train   \n",
       "1   Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White   Galaxy      0    train   \n",
       "2   Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White       J1      0    train   \n",
       "3   Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White      Ace      0    train   \n",
       "4   Samsung Galaxy J1 Ace 2016 SM-J111F 8GB - White     2016      0    train   \n",
       "\n",
       "  clean_tokens  item_id  word_id  if_tokens_has_numbers_in_the_str  \\\n",
       "0      samsung        1     7054                                 0   \n",
       "1       galaxy        1      537                                 0   \n",
       "2           j1        1    10138                                 1   \n",
       "3          ace        1    10469                                 0   \n",
       "4         2016        1     9108                                 1   \n",
       "\n",
       "   if_start_with_capital_chars  percentage_of_upper_chars_in_token    ...     \\\n",
       "0                            1                            0.142857    ...      \n",
       "1                            1                            0.166667    ...      \n",
       "2                            1                            0.500000    ...      \n",
       "3                            1                            0.333333    ...      \n",
       "4                            0                            0.000000    ...      \n",
       "\n",
       "    dim_292   dim_293   dim_294   dim_295   dim_296   dim_297   dim_298  \\\n",
       "0 -1.158214 -3.077862  4.290862  4.385282  4.017592  3.681826 -4.147452   \n",
       "1 -0.766951 -3.085364  4.416570  2.265839  4.203361  0.191773 -1.871406   \n",
       "2 -2.633618  1.691233 -3.085515 -1.096208  0.355705  0.258805 -0.344883   \n",
       "3  2.824487  0.047915 -1.890348  0.103570  0.449590  1.808830  0.450412   \n",
       "4 -0.603993  0.499419  0.748781  1.345417 -1.621760  0.530300  0.549422   \n",
       "\n",
       "    dim_299   dim_300    tf_idf  \n",
       "0  7.351630  4.044105  0.169688  \n",
       "1  9.093953  2.210504  0.194254  \n",
       "2  0.036787  1.097038  0.377184  \n",
       "3  0.646871  0.522857  0.402201  \n",
       "4  1.383292  1.529812  0.362081  \n",
       "\n",
       "[5 rows x 575 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique() #=====> binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'val'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_valid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_name',\n",
       " 'tokens',\n",
       " 'label',\n",
       " 'is_valid',\n",
       " 'clean_tokens',\n",
       " 'item_id',\n",
       " 'word_id',\n",
       " 'if_tokens_has_numbers_in_the_str',\n",
       " 'if_start_with_capital_chars',\n",
       " 'percentage_of_upper_chars_in_token',\n",
       " 'check_if_english_word',\n",
       " 'len_of_token',\n",
       " 'is_all_character_captilized',\n",
       " 'is_all_character_lowercase',\n",
       " 'consist_only_of_digits',\n",
       " 'is_first_character_digit',\n",
       " 'is_first_character_uppercase',\n",
       " 'do_consist_hyphen',\n",
       " 'if_it_is_and',\n",
       " 'is_second_character_uppercase',\n",
       " 'if_it_is_a_sale_word',\n",
       " 'if_it_is_by',\n",
       " 'the_preceding_w_1-hasNumbers',\n",
       " 'the_preceding_w_1-consist_only_of_digits',\n",
       " 'the_preceding_w_1-do_consist_hyphen',\n",
       " 'the_preceding_w_1-if_it_is_and',\n",
       " 'the_preceding_w_1-if_start_with_capital_chars',\n",
       " 'the_preceding_w_1-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_1-check_if_english_word',\n",
       " 'the_preceding_w_1-len_of_token',\n",
       " 'the_preceding_w_1-is_all_character_captilized',\n",
       " 'the_preceding_w_1-is_all_character_lowercase',\n",
       " 'the_preceding_w_1-is_first_character_digit',\n",
       " 'the_preceding_w_1-is_first_character_uppercase',\n",
       " 'the_preceding_w_1-is_second_character_uppercase',\n",
       " 'the_preceding_w_1-position_of_the_tokens',\n",
       " 'the_preceding_w_1-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_1-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_1-token_freq',\n",
       " 'the_preceding_w_1-if_it_is_by',\n",
       " 'the_preceding_w_1-if_it_is_a_sale_word',\n",
       " 'the_preceding_w_2-hasNumbers',\n",
       " 'the_preceding_w_2-consist_only_of_digits',\n",
       " 'the_preceding_w_2-do_consist_hyphen',\n",
       " 'the_preceding_w_2-if_it_is_and',\n",
       " 'the_preceding_w_2-if_start_with_capital_chars',\n",
       " 'the_preceding_w_2-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_2-check_if_english_word',\n",
       " 'the_preceding_w_2-len_of_token',\n",
       " 'the_preceding_w_2-is_all_character_captilized',\n",
       " 'the_preceding_w_2-is_all_character_lowercase',\n",
       " 'the_preceding_w_2-is_first_character_digit',\n",
       " 'the_preceding_w_2-is_first_character_uppercase',\n",
       " 'the_preceding_w_2-is_second_character_uppercase',\n",
       " 'the_preceding_w_2-position_of_the_tokens',\n",
       " 'the_preceding_w_2-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_2-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_2-token_freq',\n",
       " 'the_preceding_w_2-if_it_is_by',\n",
       " 'the_preceding_w_2-if_it_is_a_sale_word',\n",
       " 'the_preceding_w_3-hasNumbers',\n",
       " 'the_preceding_w_3-consist_only_of_digits',\n",
       " 'the_preceding_w_3-do_consist_hyphen',\n",
       " 'the_preceding_w_3-if_it_is_and',\n",
       " 'the_preceding_w_3-if_start_with_capital_chars',\n",
       " 'the_preceding_w_3-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_3-check_if_english_word',\n",
       " 'the_preceding_w_3-len_of_token',\n",
       " 'the_preceding_w_3-is_all_character_captilized',\n",
       " 'the_preceding_w_3-is_all_character_lowercase',\n",
       " 'the_preceding_w_3-is_first_character_digit',\n",
       " 'the_preceding_w_3-is_first_character_uppercase',\n",
       " 'the_preceding_w_3-is_second_character_uppercase',\n",
       " 'the_preceding_w_3-position_of_the_tokens',\n",
       " 'the_preceding_w_3-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_3-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_3-token_freq',\n",
       " 'the_preceding_w_3-if_it_is_by',\n",
       " 'the_preceding_w_3-if_it_is_a_sale_word',\n",
       " 'the_preceding_w_4-hasNumbers',\n",
       " 'the_preceding_w_4-consist_only_of_digits',\n",
       " 'the_preceding_w_4-do_consist_hyphen',\n",
       " 'the_preceding_w_4-if_it_is_and',\n",
       " 'the_preceding_w_4-if_start_with_capital_chars',\n",
       " 'the_preceding_w_4-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_4-check_if_english_word',\n",
       " 'the_preceding_w_4-len_of_token',\n",
       " 'the_preceding_w_4-is_all_character_captilized',\n",
       " 'the_preceding_w_4-is_all_character_lowercase',\n",
       " 'the_preceding_w_4-is_first_character_digit',\n",
       " 'the_preceding_w_4-is_first_character_uppercase',\n",
       " 'the_preceding_w_4-is_second_character_uppercase',\n",
       " 'the_preceding_w_4-position_of_the_tokens',\n",
       " 'the_preceding_w_4-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_4-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_4-token_freq',\n",
       " 'the_preceding_w_4-if_it_is_by',\n",
       " 'the_preceding_w_4-if_it_is_a_sale_word',\n",
       " 'the_preceding_w_5-hasNumbers',\n",
       " 'the_preceding_w_5-consist_only_of_digits',\n",
       " 'the_preceding_w_5-do_consist_hyphen',\n",
       " 'the_preceding_w_5-if_it_is_and',\n",
       " 'the_preceding_w_5-if_start_with_capital_chars',\n",
       " 'the_preceding_w_5-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_5-check_if_english_word',\n",
       " 'the_preceding_w_5-len_of_token',\n",
       " 'the_preceding_w_5-is_all_character_captilized',\n",
       " 'the_preceding_w_5-is_all_character_lowercase',\n",
       " 'the_preceding_w_5-is_first_character_digit',\n",
       " 'the_preceding_w_5-is_first_character_uppercase',\n",
       " 'the_preceding_w_5-is_second_character_uppercase',\n",
       " 'the_preceding_w_5-position_of_the_tokens',\n",
       " 'the_preceding_w_5-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_5-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_5-token_freq',\n",
       " 'the_preceding_w_5-if_it_is_by',\n",
       " 'the_preceding_w_5-if_it_is_a_sale_word',\n",
       " 'the_preceding_w_6-hasNumbers',\n",
       " 'the_preceding_w_6-consist_only_of_digits',\n",
       " 'the_preceding_w_6-do_consist_hyphen',\n",
       " 'the_preceding_w_6-if_it_is_and',\n",
       " 'the_preceding_w_6-if_start_with_capital_chars',\n",
       " 'the_preceding_w_6-percentage_of_upper_chars_in_token',\n",
       " 'the_preceding_w_6-check_if_english_word',\n",
       " 'the_preceding_w_6-len_of_token',\n",
       " 'the_preceding_w_6-is_all_character_captilized',\n",
       " 'the_preceding_w_6-is_all_character_lowercase',\n",
       " 'the_preceding_w_6-is_first_character_digit',\n",
       " 'the_preceding_w_6-is_first_character_uppercase',\n",
       " 'the_preceding_w_6-is_second_character_uppercase',\n",
       " 'the_preceding_w_6-position_of_the_tokens',\n",
       " 'the_preceding_w_6-position_of_the_tokens_from_the_bottom',\n",
       " 'the_preceding_w_6-position_of_the_tokens_in_ratio',\n",
       " 'the_preceding_w_6-token_freq',\n",
       " 'the_preceding_w_6-if_it_is_by',\n",
       " 'the_preceding_w_6-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_1-hasNumbers',\n",
       " 'the_succeeding_w_1-consist_only_of_digits',\n",
       " 'the_succeeding_w_1-do_consist_hyphen',\n",
       " 'the_succeeding_w_1-if_it_is_and',\n",
       " 'the_succeeding_w_1-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_1-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_1-check_if_english_word',\n",
       " 'the_succeeding_w_1-len_of_token',\n",
       " 'the_succeeding_w_1-is_all_character_captilized',\n",
       " 'the_succeeding_w_1-is_all_character_lowercase',\n",
       " 'the_succeeding_w_1-is_first_character_digit',\n",
       " 'the_succeeding_w_1-is_first_character_uppercase',\n",
       " 'the_succeeding_w_1-is_second_character_uppercase',\n",
       " 'the_succeeding_w_1-position_of_the_tokens',\n",
       " 'the_succeeding_w_1-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_1-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_1-token_freq',\n",
       " 'the_succeeding_w_1-if_it_is_by',\n",
       " 'the_succeeding_w_1-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_2-hasNumbers',\n",
       " 'the_succeeding_w_2-consist_only_of_digits',\n",
       " 'the_succeeding_w_2-do_consist_hyphen',\n",
       " 'the_succeeding_w_2-if_it_is_and',\n",
       " 'the_succeeding_w_2-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_2-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_2-check_if_english_word',\n",
       " 'the_succeeding_w_2-len_of_token',\n",
       " 'the_succeeding_w_2-is_all_character_captilized',\n",
       " 'the_succeeding_w_2-is_all_character_lowercase',\n",
       " 'the_succeeding_w_2-is_first_character_digit',\n",
       " 'the_succeeding_w_2-is_first_character_uppercase',\n",
       " 'the_succeeding_w_2-is_second_character_uppercase',\n",
       " 'the_succeeding_w_2-position_of_the_tokens',\n",
       " 'the_succeeding_w_2-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_2-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_2-token_freq',\n",
       " 'the_succeeding_w_2-if_it_is_by',\n",
       " 'the_succeeding_w_2-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_3-hasNumbers',\n",
       " 'the_succeeding_w_3-consist_only_of_digits',\n",
       " 'the_succeeding_w_3-do_consist_hyphen',\n",
       " 'the_succeeding_w_3-if_it_is_and',\n",
       " 'the_succeeding_w_3-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_3-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_3-check_if_english_word',\n",
       " 'the_succeeding_w_3-len_of_token',\n",
       " 'the_succeeding_w_3-is_all_character_captilized',\n",
       " 'the_succeeding_w_3-is_all_character_lowercase',\n",
       " 'the_succeeding_w_3-is_first_character_digit',\n",
       " 'the_succeeding_w_3-is_first_character_uppercase',\n",
       " 'the_succeeding_w_3-is_second_character_uppercase',\n",
       " 'the_succeeding_w_3-position_of_the_tokens',\n",
       " 'the_succeeding_w_3-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_3-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_3-token_freq',\n",
       " 'the_succeeding_w_3-if_it_is_by',\n",
       " 'the_succeeding_w_3-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_4-hasNumbers',\n",
       " 'the_succeeding_w_4-consist_only_of_digits',\n",
       " 'the_succeeding_w_4-do_consist_hyphen',\n",
       " 'the_succeeding_w_4-if_it_is_and',\n",
       " 'the_succeeding_w_4-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_4-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_4-check_if_english_word',\n",
       " 'the_succeeding_w_4-len_of_token',\n",
       " 'the_succeeding_w_4-is_all_character_captilized',\n",
       " 'the_succeeding_w_4-is_all_character_lowercase',\n",
       " 'the_succeeding_w_4-is_first_character_digit',\n",
       " 'the_succeeding_w_4-is_first_character_uppercase',\n",
       " 'the_succeeding_w_4-is_second_character_uppercase',\n",
       " 'the_succeeding_w_4-position_of_the_tokens',\n",
       " 'the_succeeding_w_4-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_4-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_4-token_freq',\n",
       " 'the_succeeding_w_4-if_it_is_by',\n",
       " 'the_succeeding_w_4-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_5-hasNumbers',\n",
       " 'the_succeeding_w_5-consist_only_of_digits',\n",
       " 'the_succeeding_w_5-do_consist_hyphen',\n",
       " 'the_succeeding_w_5-if_it_is_and',\n",
       " 'the_succeeding_w_5-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_5-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_5-check_if_english_word',\n",
       " 'the_succeeding_w_5-len_of_token',\n",
       " 'the_succeeding_w_5-is_all_character_captilized',\n",
       " 'the_succeeding_w_5-is_all_character_lowercase',\n",
       " 'the_succeeding_w_5-is_first_character_digit',\n",
       " 'the_succeeding_w_5-is_first_character_uppercase',\n",
       " 'the_succeeding_w_5-is_second_character_uppercase',\n",
       " 'the_succeeding_w_5-position_of_the_tokens',\n",
       " 'the_succeeding_w_5-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_5-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_5-token_freq',\n",
       " 'the_succeeding_w_5-if_it_is_by',\n",
       " 'the_succeeding_w_5-if_it_is_a_sale_word',\n",
       " 'the_succeeding_w_6-hasNumbers',\n",
       " 'the_succeeding_w_6-consist_only_of_digits',\n",
       " 'the_succeeding_w_6-do_consist_hyphen',\n",
       " 'the_succeeding_w_6-if_it_is_and',\n",
       " 'the_succeeding_w_6-if_start_with_capital_chars',\n",
       " 'the_succeeding_w_6-percentage_of_upper_chars_in_token',\n",
       " 'the_succeeding_w_6-check_if_english_word',\n",
       " 'the_succeeding_w_6-len_of_token',\n",
       " 'the_succeeding_w_6-is_all_character_captilized',\n",
       " 'the_succeeding_w_6-is_all_character_lowercase',\n",
       " 'the_succeeding_w_6-is_first_character_digit',\n",
       " 'the_succeeding_w_6-is_first_character_uppercase',\n",
       " 'the_succeeding_w_6-is_second_character_uppercase',\n",
       " 'the_succeeding_w_6-position_of_the_tokens',\n",
       " 'the_succeeding_w_6-position_of_the_tokens_from_the_bottom',\n",
       " 'the_succeeding_w_6-position_of_the_tokens_in_ratio',\n",
       " 'the_succeeding_w_6-token_freq',\n",
       " 'the_succeeding_w_6-if_it_is_by',\n",
       " 'the_succeeding_w_6-if_it_is_a_sale_word',\n",
       " 'succeeding_2_gram_given_current_token',\n",
       " 'preceding_2_gram_given_current_token',\n",
       " 'succeeding_3_gram_given_current_token',\n",
       " 'preceding_3_gram_given_current_token',\n",
       " 'succeeding_4_gram_given_current_token',\n",
       " 'preceding_4_gram_given_current_token',\n",
       " 'succeeding_5_gram_given_current_token',\n",
       " 'preceding_5_gram_given_current_token',\n",
       " 'succeeding_6_gram_given_current_token',\n",
       " 'preceding_6_gram_given_current_token',\n",
       " 'succeeding_d_2_k_2_given_current_token',\n",
       " 'preceding_d_2_k_2_given_current_token',\n",
       " 'succeeding_d_3_k_2_given_current_token',\n",
       " 'preceding_d_3_k_2_given_current_token',\n",
       " 'succeeding_d_4_k_2_given_current_token',\n",
       " 'preceding_d_4_k_2_given_current_token',\n",
       " 'succeeding_d_5_k_2_given_current_token',\n",
       " 'preceding_d_5_k_2_given_current_token',\n",
       " 'succeeding_d_6_k_2_given_current_token',\n",
       " 'preceding_d_6_k_2_given_current_token',\n",
       " 'succeeding_d_2_k_3_given_current_token',\n",
       " 'preceding_d_2_k_3_given_current_token',\n",
       " 'succeeding_d_3_k_3_given_current_token',\n",
       " 'preceding_d_3_k_3_given_current_token',\n",
       " 'dim_1',\n",
       " 'dim_2',\n",
       " 'dim_3',\n",
       " 'dim_4',\n",
       " 'dim_5',\n",
       " 'dim_6',\n",
       " 'dim_7',\n",
       " 'dim_8',\n",
       " 'dim_9',\n",
       " 'dim_10',\n",
       " 'dim_11',\n",
       " 'dim_12',\n",
       " 'dim_13',\n",
       " 'dim_14',\n",
       " 'dim_15',\n",
       " 'dim_16',\n",
       " 'dim_17',\n",
       " 'dim_18',\n",
       " 'dim_19',\n",
       " 'dim_20',\n",
       " 'dim_21',\n",
       " 'dim_22',\n",
       " 'dim_23',\n",
       " 'dim_24',\n",
       " 'dim_25',\n",
       " 'dim_26',\n",
       " 'dim_27',\n",
       " 'dim_28',\n",
       " 'dim_29',\n",
       " 'dim_30',\n",
       " 'dim_31',\n",
       " 'dim_32',\n",
       " 'dim_33',\n",
       " 'dim_34',\n",
       " 'dim_35',\n",
       " 'dim_36',\n",
       " 'dim_37',\n",
       " 'dim_38',\n",
       " 'dim_39',\n",
       " 'dim_40',\n",
       " 'dim_41',\n",
       " 'dim_42',\n",
       " 'dim_43',\n",
       " 'dim_44',\n",
       " 'dim_45',\n",
       " 'dim_46',\n",
       " 'dim_47',\n",
       " 'dim_48',\n",
       " 'dim_49',\n",
       " 'dim_50',\n",
       " 'dim_51',\n",
       " 'dim_52',\n",
       " 'dim_53',\n",
       " 'dim_54',\n",
       " 'dim_55',\n",
       " 'dim_56',\n",
       " 'dim_57',\n",
       " 'dim_58',\n",
       " 'dim_59',\n",
       " 'dim_60',\n",
       " 'dim_61',\n",
       " 'dim_62',\n",
       " 'dim_63',\n",
       " 'dim_64',\n",
       " 'dim_65',\n",
       " 'dim_66',\n",
       " 'dim_67',\n",
       " 'dim_68',\n",
       " 'dim_69',\n",
       " 'dim_70',\n",
       " 'dim_71',\n",
       " 'dim_72',\n",
       " 'dim_73',\n",
       " 'dim_74',\n",
       " 'dim_75',\n",
       " 'dim_76',\n",
       " 'dim_77',\n",
       " 'dim_78',\n",
       " 'dim_79',\n",
       " 'dim_80',\n",
       " 'dim_81',\n",
       " 'dim_82',\n",
       " 'dim_83',\n",
       " 'dim_84',\n",
       " 'dim_85',\n",
       " 'dim_86',\n",
       " 'dim_87',\n",
       " 'dim_88',\n",
       " 'dim_89',\n",
       " 'dim_90',\n",
       " 'dim_91',\n",
       " 'dim_92',\n",
       " 'dim_93',\n",
       " 'dim_94',\n",
       " 'dim_95',\n",
       " 'dim_96',\n",
       " 'dim_97',\n",
       " 'dim_98',\n",
       " 'dim_99',\n",
       " 'dim_100',\n",
       " 'dim_101',\n",
       " 'dim_102',\n",
       " 'dim_103',\n",
       " 'dim_104',\n",
       " 'dim_105',\n",
       " 'dim_106',\n",
       " 'dim_107',\n",
       " 'dim_108',\n",
       " 'dim_109',\n",
       " 'dim_110',\n",
       " 'dim_111',\n",
       " 'dim_112',\n",
       " 'dim_113',\n",
       " 'dim_114',\n",
       " 'dim_115',\n",
       " 'dim_116',\n",
       " 'dim_117',\n",
       " 'dim_118',\n",
       " 'dim_119',\n",
       " 'dim_120',\n",
       " 'dim_121',\n",
       " 'dim_122',\n",
       " 'dim_123',\n",
       " 'dim_124',\n",
       " 'dim_125',\n",
       " 'dim_126',\n",
       " 'dim_127',\n",
       " 'dim_128',\n",
       " 'dim_129',\n",
       " 'dim_130',\n",
       " 'dim_131',\n",
       " 'dim_132',\n",
       " 'dim_133',\n",
       " 'dim_134',\n",
       " 'dim_135',\n",
       " 'dim_136',\n",
       " 'dim_137',\n",
       " 'dim_138',\n",
       " 'dim_139',\n",
       " 'dim_140',\n",
       " 'dim_141',\n",
       " 'dim_142',\n",
       " 'dim_143',\n",
       " 'dim_144',\n",
       " 'dim_145',\n",
       " 'dim_146',\n",
       " 'dim_147',\n",
       " 'dim_148',\n",
       " 'dim_149',\n",
       " 'dim_150',\n",
       " 'dim_151',\n",
       " 'dim_152',\n",
       " 'dim_153',\n",
       " 'dim_154',\n",
       " 'dim_155',\n",
       " 'dim_156',\n",
       " 'dim_157',\n",
       " 'dim_158',\n",
       " 'dim_159',\n",
       " 'dim_160',\n",
       " 'dim_161',\n",
       " 'dim_162',\n",
       " 'dim_163',\n",
       " 'dim_164',\n",
       " 'dim_165',\n",
       " 'dim_166',\n",
       " 'dim_167',\n",
       " 'dim_168',\n",
       " 'dim_169',\n",
       " 'dim_170',\n",
       " 'dim_171',\n",
       " 'dim_172',\n",
       " 'dim_173',\n",
       " 'dim_174',\n",
       " 'dim_175',\n",
       " 'dim_176',\n",
       " 'dim_177',\n",
       " 'dim_178',\n",
       " 'dim_179',\n",
       " 'dim_180',\n",
       " 'dim_181',\n",
       " 'dim_182',\n",
       " 'dim_183',\n",
       " 'dim_184',\n",
       " 'dim_185',\n",
       " 'dim_186',\n",
       " 'dim_187',\n",
       " 'dim_188',\n",
       " 'dim_189',\n",
       " 'dim_190',\n",
       " 'dim_191',\n",
       " 'dim_192',\n",
       " 'dim_193',\n",
       " 'dim_194',\n",
       " 'dim_195',\n",
       " 'dim_196',\n",
       " 'dim_197',\n",
       " 'dim_198',\n",
       " 'dim_199',\n",
       " 'dim_200',\n",
       " 'dim_201',\n",
       " 'dim_202',\n",
       " 'dim_203',\n",
       " 'dim_204',\n",
       " 'dim_205',\n",
       " 'dim_206',\n",
       " 'dim_207',\n",
       " 'dim_208',\n",
       " 'dim_209',\n",
       " 'dim_210',\n",
       " 'dim_211',\n",
       " 'dim_212',\n",
       " 'dim_213',\n",
       " 'dim_214',\n",
       " 'dim_215',\n",
       " 'dim_216',\n",
       " 'dim_217',\n",
       " 'dim_218',\n",
       " 'dim_219',\n",
       " 'dim_220',\n",
       " 'dim_221',\n",
       " 'dim_222',\n",
       " 'dim_223',\n",
       " 'dim_224',\n",
       " 'dim_225',\n",
       " 'dim_226',\n",
       " 'dim_227',\n",
       " 'dim_228',\n",
       " 'dim_229',\n",
       " 'dim_230',\n",
       " 'dim_231',\n",
       " 'dim_232',\n",
       " 'dim_233',\n",
       " 'dim_234',\n",
       " 'dim_235',\n",
       " 'dim_236',\n",
       " 'dim_237',\n",
       " 'dim_238',\n",
       " 'dim_239',\n",
       " 'dim_240',\n",
       " 'dim_241',\n",
       " 'dim_242',\n",
       " 'dim_243',\n",
       " 'dim_244',\n",
       " 'dim_245',\n",
       " 'dim_246',\n",
       " 'dim_247',\n",
       " 'dim_248',\n",
       " 'dim_249',\n",
       " 'dim_250',\n",
       " 'dim_251',\n",
       " 'dim_252',\n",
       " 'dim_253',\n",
       " 'dim_254',\n",
       " 'dim_255',\n",
       " 'dim_256',\n",
       " 'dim_257',\n",
       " 'dim_258',\n",
       " 'dim_259',\n",
       " 'dim_260',\n",
       " 'dim_261',\n",
       " 'dim_262',\n",
       " 'dim_263',\n",
       " 'dim_264',\n",
       " 'dim_265',\n",
       " 'dim_266',\n",
       " 'dim_267',\n",
       " 'dim_268',\n",
       " 'dim_269',\n",
       " 'dim_270',\n",
       " 'dim_271',\n",
       " 'dim_272',\n",
       " 'dim_273',\n",
       " 'dim_274',\n",
       " 'dim_275',\n",
       " 'dim_276',\n",
       " 'dim_277',\n",
       " 'dim_278',\n",
       " 'dim_279',\n",
       " 'dim_280',\n",
       " 'dim_281',\n",
       " 'dim_282',\n",
       " 'dim_283',\n",
       " 'dim_284',\n",
       " 'dim_285',\n",
       " 'dim_286',\n",
       " 'dim_287',\n",
       " 'dim_288',\n",
       " 'dim_289',\n",
       " 'dim_290',\n",
       " 'dim_291',\n",
       " 'dim_292',\n",
       " 'dim_293',\n",
       " 'dim_294',\n",
       " 'dim_295',\n",
       " 'dim_296',\n",
       " 'dim_297',\n",
       " 'dim_298',\n",
       " 'dim_299',\n",
       " 'dim_300',\n",
       " 'tf_idf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from utils import customized_eval\n",
    "\n",
    "\n",
    "class KFoldValidation():\n",
    "    \n",
    "    def __init__(self, data, group_by = 'item_id',n_splits=5):\n",
    "        ''''''\n",
    "        unique_vis = np.array(sorted(data[group_by].astype(str).unique()))\n",
    "        folds = GroupKFold(n_splits)\n",
    "        ids = np.arange(data.shape[0]) # index of row for whole data\n",
    "        \n",
    "        self.fold_ids = [] # saving training and validating index of row for each fold\n",
    "        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "            # trn_vis: 1-D array with index of training row\n",
    "            # val_vis: 1-D array with index of validating row\n",
    "            self.fold_ids.append([\n",
    "                    ids[data[group_by].astype(str).isin(unique_vis[trn_vis])],\n",
    "                    ids[data[group_by].astype(str).isin(unique_vis[val_vis])]\n",
    "                ])\n",
    "\n",
    "    \n",
    "    def validate(self, train, test, features, target_col, use_which_model ='lgb', \n",
    "                 name=\"\", prepare_stacking=False, \n",
    "                 fit_params={\"early_stopping_rounds\": 50, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n",
    "        '''\n",
    "        test: Only needed, if prepare_stacking is True.\n",
    "        name: Only needed, if prepare_stacking is True.\n",
    "        target_col: str.\n",
    "        '''\n",
    "        col_need_for_computing_f1 = ['item_id']\n",
    "        \n",
    "        #----------------------\n",
    "        # initialization\n",
    "        #----------------------\n",
    "#         model.FeatImp = pd.DataFrame(index=features) # Feature Importance \n",
    "        full_score = 0 # Final Meaure of Performance\n",
    "        \n",
    "        if prepare_stacking:\n",
    "            test[name] = 0\n",
    "            train[name] = np.NaN\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(self.fold_ids):\n",
    "            #---------------------\n",
    "            # train-val split\n",
    "            #---------------------\n",
    "            devel = train[features].iloc[trn_idx]\n",
    "            y_devel = train[target_col].iloc[trn_idx]\n",
    "            valid = train[features].iloc[val_idx]\n",
    "            y_valid = train[target_col].iloc[val_idx]\n",
    "            \n",
    "            # for custom_f1\n",
    "            #global val_for_f1\n",
    "            # get 1-D array with shape of (num_samples,)\n",
    "            item_id_for_f1 = train[col_need_for_computing_f1].iloc[val_idx].values.reshape(len(val_idx))                \n",
    "            print(\"Fold \", fold_id, \":\")            \n",
    "            #--------------------\n",
    "            # covert pd.DataFrame into lgb.Dataset\n",
    "            #--------------------\n",
    "            if use_which_model == 'lgb':\n",
    "                dtrain = lgb.Dataset(devel, label= y_devel, free_raw_data = False)\n",
    "                dvalid = lgb.Dataset(valid, label= y_valid, free_raw_data = False, reference= dtrain)\n",
    "                        \n",
    "                #evals_result = {} # for saving the evaluation metric of validating set during training\n",
    "                model = lgb.train(params = fit_params, \n",
    "                                      train_set = dtrain, \n",
    "                                      num_boost_round = 10,\n",
    "                                      valid_sets = dvalid, \n",
    "                                      #evals_result = evals_result,\n",
    "                                      feval = customized_eval(data = item_id_for_f1,threshold = 0.5, verbose = True), \n",
    "                                     )\n",
    "            \n",
    "#             if len(model.feature_importances_) == len(features):  # some bugs in catboost?\n",
    "#                 model.FeatImp['fold' + str(fold_id)] = model.feature_importances_ / model.feature_importances_.sum()\n",
    "\n",
    "\n",
    "            #----------------------\n",
    "            # compute the score of each fold\n",
    "            #----------------------\n",
    "            predictions = model.predict(valid) # 1-D array with shape of (num_samples,)\n",
    "            fold_score = model.best_score['valid_0']['f1-score on sentence-level']\n",
    "            print(\"Fold \", fold_id, \" f1-score : \", fold_score) \n",
    "            \n",
    "            #----------------------\n",
    "            # compute final measure of performance(Average)\n",
    "            #----------------------\n",
    "           \n",
    "            full_score += fold_score / len(self.fold_ids) # len(self.fold_ids) == n_splits\n",
    "\n",
    "            if prepare_stacking:\n",
    "                train[name].iloc[val] = predictions\n",
    "                \n",
    "                test_predictions = model.predict(test[features])\n",
    "                test_predictions[test_predictions < 0] = 0\n",
    "                test[name] += test_predictions / len(self.fold_ids)\n",
    "                \n",
    "        print(\"Final score: \", full_score)\n",
    "        return full_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfolder = KFoldValidation(df, group_by = 'item_id', n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Kfolder.fold_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'label'\n",
    "features = df.columns.tolist()[7:]\n",
    "# parameters\n",
    "CPU_USE_RATE = 0.5\n",
    "NUM_LEAVES = 31\n",
    "COLSAMPLE_BYTREE = 1.0\n",
    "SUBSAMPLE = 1.0\n",
    "SUBSAMPLE_FREQ = 0\n",
    "MAX_DEPTH = -1\n",
    "REG_LAMBDA = 0.0\n",
    "REG_LAMBDA = 0.0\n",
    "REG_ALPHA = 0.0\n",
    "MIN_SPLIT_GAIN = 0\n",
    "MIN_CHILD_WEIGHT = 0.001\n",
    "MAX_BIN = 255\n",
    "\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../models/blend/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/lightgbm/engine.py:107: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0\n",
      "precision: 0\n",
      "recall: 0\n",
      "[1]\tvalid_0's f1-score on sentence-level: 0\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "f1: 0\n",
      "precision: 0\n",
      "recall: 0\n",
      "[2]\tvalid_0's f1-score on sentence-level: 0\n",
      "f1: 0\n",
      "precision: 0\n",
      "recall: 0\n",
      "[3]\tvalid_0's f1-score on sentence-level: 0\n",
      "f1: 0\n",
      "precision: 0\n",
      "recall: 0\n",
      "[4]\tvalid_0's f1-score on sentence-level: 0\n",
      "f1: 0.0022837567799029405\n",
      "precision: 1.0\n",
      "recall: 0.0011431837667905116\n",
      "[5]\tvalid_0's f1-score on sentence-level: 0.00228376\n",
      "f1: 0.5335522265544839\n",
      "precision: 0.9461426491994177\n",
      "recall: 0.37153472420691624\n",
      "[6]\tvalid_0's f1-score on sentence-level: 0.533552\n",
      "f1: 0.7407915024841528\n",
      "precision: 0.9247219846022241\n",
      "recall: 0.6178908259502716\n",
      "[7]\tvalid_0's f1-score on sentence-level: 0.740792\n",
      "f1: 0.8743121922965538\n",
      "precision: 0.8861168183152334\n",
      "recall: 0.8628179479851386\n",
      "[8]\tvalid_0's f1-score on sentence-level: 0.874312\n",
      "f1: 0.8952754801626245\n",
      "precision: 0.8786461199779857\n",
      "recall: 0.9125464418405259\n",
      "[9]\tvalid_0's f1-score on sentence-level: 0.895275\n",
      "f1: 0.9064688916357644\n",
      "precision: 0.8725542041248017\n",
      "recall: 0.943126607602172\n",
      "[10]\tvalid_0's f1-score on sentence-level: 0.906469\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's f1-score on sentence-level: 0.906469\n",
      "Fold  0  f1-score :  0.9064688916357644\n",
      "Fold  1 :\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#from utils import custom_system_f1\n",
    "# class OrderedCounter(Counter, OrderedDict):\n",
    "#     'Counter that remembers the order elements are first encountered'\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return '%s(%r)' % (self.__class__.__name__, OrderedDict(self))\n",
    "\n",
    "#     def __reduce__(self):\n",
    "#         return self.__class__, (OrderedDict(self),)\n",
    "    \n",
    "# def custom_system_f1(y_pred, y, threshold = 0.5, verbose = False):\n",
    "#     '''\n",
    "#     It's a customized evaluation metric for computing f1-score on sentence-level.\n",
    "    \n",
    "#     Args:\n",
    "#     If binary classificiton:\n",
    "#         y_pred: 1-D array, with shape of (num_sample, ), where each elemeent is prob belong to the class 1.\n",
    "#         y: same shape as y_pred.\n",
    "        \n",
    "#     If multi-class classification:\n",
    "#         y_pred: 2-D array, with shape of (nun_sample, num_class), where each class is prob belong to this class.\n",
    "    \n",
    "#     Return:\n",
    "#         f1-score on sentence-level instead of token-level.\n",
    "#     ''' \n",
    "#     # get y_true\n",
    "#     s = time.time()\n",
    "#     y_true = y.get_label().astype(\"int\")\n",
    "#     # get y_pred\n",
    "#     y_pred = np.array([1 if p > threshold else 0 for p in y_pred])\n",
    "#     # get helper dict\n",
    "#     id_lengh_dict = OrderedCounter(list(val_for_f1)) # need Counter is ordered key\n",
    "#     #---------------------\n",
    "#     # initialization\n",
    "#     #---------------------\n",
    "#     ix = 0\n",
    "#     correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "\n",
    "#     for item_id, item_length in id_lengh_dict.items():\n",
    "#         y_t_sentence = list(y_true[ix: ix + item_length])\n",
    "#         y_p_sentence = list(y_pred[ix: ix + item_length])\n",
    "#         #----------\n",
    "#         # core\n",
    "#         #----------\n",
    "#         if all(v == 0 for v in y_t_sentence):\n",
    "#             pass\n",
    "#         else:\n",
    "#             # there is exiting atual y_true\n",
    "#             total_correct += 1.0\n",
    "#             if np.array_equal(y_t_sentence, y_p_sentence):\n",
    "#                 # givne the case that we have actual y_ture and y_ture == y_pred\n",
    "#                 correct_preds += 1.0\n",
    "#         if all(v == 0 for v in y_p_sentence):\n",
    "#             pass\n",
    "#         else:\n",
    "#             total_preds += 1.0\n",
    "#         ix += item_length\n",
    "#     #----------\n",
    "#     # output\n",
    "#     #----------\n",
    "#     p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "#     r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "#     f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "#     if verbose == True:\n",
    "#         print('f1: {}'.format(f1))\n",
    "#         print('precision: {}'.format(p))\n",
    "#         print('recall: {}'.format(r))\n",
    "#     return 'f1-score on sentence-level', f1, True\n",
    "\n",
    "test = None\n",
    "prepare_stacking = False\n",
    "# the best para will be determined by bayesian optimization\n",
    "fit_params = {\n",
    "              \"early_stopping_rounds\": 50,\n",
    "              \"verbose\": 100, \n",
    "              'objective': 'binary',\n",
    "              \"num_leaves\":NUM_LEAVES,\n",
    "              'metric': 'None' # Please remember do specify metric == None for using custom evaluation metrci to do early stopping.\n",
    "    \n",
    "              }\n",
    "\n",
    "Kfolder.validate(df, test, features, target_col,'lgb', \"lgbpred\", prepare_stacking, fit_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_class':3,\n",
    "#     #'metric': 'multi_logloss',\n",
    "#     'metric': 'None' # Please remember do specify metric == None for using custom evaluation metrci to do early stopping.\n",
    "    \n",
    "# }\n",
    "\n",
    "# evals_result = {} # This dictionary used to store all evaluation results of all the items in valid_sets\n",
    "# num_round = 50000\n",
    "# lgbmodel = lgb.train(params, \n",
    "#                       dtrain, \n",
    "#                       num_round, \n",
    "#                       early_stopping_rounds = 200,\n",
    "#                       valid_sets = dvalid, \n",
    "#                       evals_result = evals_result,\n",
    "#                       feval = custom_system_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBMClassifier cannot be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# CPU_USE_RATE = 0.5\n",
    "NUM_LEAVES = 31\n",
    "# COLSAMPLE_BYTREE = 1.0\n",
    "# SUBSAMPLE = 1.0\n",
    "# SUBSAMPLE_FREQ = 0\n",
    "# MAX_DEPTH = -1\n",
    "# REG_LAMBDA = 0.0\n",
    "# REG_LAMBDA = 0.0\n",
    "# REG_ALPHA = 0.0\n",
    "# MIN_SPLIT_GAIN = 0\n",
    "# MIN_CHILD_WEIGHT = 0.001\n",
    "# MAX_BIN = 255\n",
    "\n",
    "# fit_params = {\n",
    "#               \"num_boost_round\":15000\n",
    "#               \"early_stopping_rounds\": 50,\n",
    "#               \"verbose\": 100, \n",
    "#               \"eval_metric\": \"multi_logloss\",\n",
    "#               }\n",
    "# train_parameters = {\n",
    "#     \"num_leaves\":NUM_LEAVES\n",
    "# }\n",
    "# lgbmodel = LGBMClassifier(\n",
    "#     n_jobs=int(multiprocessing.cpu_count()*CPU_USE_RATE),\n",
    "#     n_estimators=10000,\n",
    "#     learning_rate=0.02,\n",
    "#     num_leaves=NUM_LEAVES,\n",
    "#     colsample_bytree=COLSAMPLE_BYTREE,\n",
    "#     subsample=SUBSAMPLE,\n",
    "#     subsample_freq=SUBSAMPLE_FREQ,\n",
    "#     max_depth=MAX_DEPTH,\n",
    "#     reg_alpha=REG_ALPHA,\n",
    "#     reg_lambda=REG_LAMBDA,\n",
    "#     min_split_gain=MIN_SPLIT_GAIN,\n",
    "#     min_child_weight=MIN_CHILD_WEIGHT,\n",
    "#     max_bin=MAX_BIN,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbmodel = lgb.LGBMRegressor(n_estimators=1000, objective=\"regression\", metric=\"rmse\", num_leaves=31, min_child_samples=100,\n",
    "#                       learning_rate=0.03, bagging_fraction=0.7, feature_fraction=0.5, bagging_frequency=5, \n",
    "#                       bagging_seed=2019, subsample=.9, colsample_bytree=.9, use_best_model=True)\n",
    "\n",
    "\n",
    "# Kfolder.validate(train, test, real_cols + cat_cols, lgbmodel, \"lgbpred\", prepare_stacking=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
