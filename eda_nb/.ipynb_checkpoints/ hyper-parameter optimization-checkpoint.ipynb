{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "sys.path.append('../models/blend/')\n",
    "from utils import customized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# setting\n",
    "#---------------------\n",
    "BINARY_SCENARIO = None\n",
    "#---------------------\n",
    "# load features\n",
    "#---------------------\n",
    "feature_dir = '../features/lazada_and_amazon/all_features.h5'\n",
    "df = pd.read_hdf(feature_dir)\n",
    "#---------------------\n",
    "# label post-processing\n",
    "#---------------------\n",
    "if df.label.nunique() == 2: \n",
    "    BINARY_SCENARIO = True\n",
    "    # binary class\n",
    "    df['label'] = df.label.apply(lambda x: 1 if x == 2 else 0) # for customized f1 score inference of lgb\n",
    "else:\n",
    "    # multi-class(B, I or O)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238668, 575)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# setting\n",
    "#---------------------\n",
    "target_col = 'label'\n",
    "features = df.columns.tolist()[7:]\n",
    "# parameters\n",
    "NUM_LEAVES = 31\n",
    "COLSAMPLE_BYTREE = 1.0\n",
    "SUBSAMPLE = 1.0\n",
    "SUBSAMPLE_FREQ = 0\n",
    "MAX_DEPTH = -1\n",
    "REG_LAMBDA = 0.0\n",
    "REG_LAMBDA = 0.0\n",
    "REG_ALPHA = 0.0\n",
    "MIN_SPLIT_GAIN = 0\n",
    "MIN_CHILD_WEIGHT = 0.001\n",
    "MAX_BIN = 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_fold = True\n",
    "target = 'label'\n",
    "n_splits = 5\n",
    "def lgb_eval(train, features, target_col,\n",
    "             num_leaves,max_depth,lambda_l2,\n",
    "             lambda_l1,min_child_samples,bagging_fraction,\n",
    "             feature_fraction):\n",
    "    '''\n",
    "    Notice that:\n",
    "        Bayesian optimization is designed to find optimal value through maximization.\n",
    "        So, if yur target function is loss function. For example, rmse. the lower, the better. \n",
    "        Don't forget to put negative into the target function.\n",
    "        However, if your target function is evluation metric. For example, f1-score. the higher, the better.\n",
    "        There is no need to put negative when returning.\n",
    "    '''\n",
    "    #--------------------\n",
    "    # setting\n",
    "    #--------------------\n",
    "    params = {\n",
    "    \"objective\": 'binary',\n",
    "    \"metric\": 'None', # Please remember do specify metric == None for using custom evaluation metrci to do early stopping. \n",
    "    \"num_leaves\" : int(num_leaves),\n",
    "    \"max_depth\" : int(max_depth),\n",
    "    \"lambda_l2\" : lambda_l2,\n",
    "    \"lambda_l1\" : lambda_l1,\n",
    "    \"num_threads\" : 4,\n",
    "    \"min_child_samples\" : int(min_child_samples),\n",
    "    \"learning_rate\" : 0.03,\n",
    "    \"bagging_fraction\" : bagging_fraction,\n",
    "    \"feature_fraction\" : feature_fraction,\n",
    "    \"subsample_freq\" : 5,\n",
    "    \"bagging_seed\" : 42,\n",
    "    \"verbosity\" : -1\n",
    "    }\n",
    "    col_need_for_computing_f1 = ['item_id']\n",
    "\n",
    "    #--------------------\n",
    "    # convert data into lgb.dataset\n",
    "    #--------------------\n",
    "    y = train[target_col].copy()\n",
    "    dtrain = lgb.Dataset(train[features], y, free_raw_data=False)\n",
    "    item_id_for_customized_f1 = train[col_need_for_computing_f1].iloc[val_idx].values.reshape(len(val_idx))\n",
    "    #--------------------\n",
    "    # CV\n",
    "    #--------------------\n",
    "    if group_fold == True:\n",
    "        gfolds = GroupKFold(n_splits)\n",
    "        unique_vis = np.array(sorted(train['fullVisitorId'].astype(str).unique()))\n",
    "        #--------------------------------\n",
    "        # Here we need our customized cross-validation result\n",
    "        #--------------------------------\n",
    "        cv_result = lgb.cv(params = params,\n",
    "                           train_set = dtrain,\n",
    "                           num_boost_round = 1,\n",
    "                           folds = gfolds.split(X=unique_vis, y=unique_vis, groups=unique_vis),\n",
    "                           feval = customized_eval(data = item_id_for_customized_f1, threshold = 0.5, verbose = False),\n",
    "                           early_stopping_rounds=150,\n",
    "                           stratified=False,\n",
    "                           nfold=n_splits)\n",
    "        \n",
    "    else:\n",
    "        cv_result = lgb.cv(params = params,\n",
    "                           train_set = dtrain,\n",
    "                           num_boost_round = 10000,\n",
    "                           feval = customized_eval(data = item_id_for_f1, threshold = 0.5, verbose = False),\n",
    "                           early_stopping_rounds=100,\n",
    "                           stratified=False,\n",
    "                           nfold=n_splits)\n",
    "    return cv_result['f1-score-on-sentence-level-mean'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# setting\n",
    "#--------------------\n",
    "\n",
    "group_fold = True\n",
    "target_col = 'label'\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "params = {\n",
    "\"objective\": 'binary',\n",
    "\"metric\": 'None', # Please remember do specify metric == None for using custom evaluation metrci to do early stopping. \n",
    "\"num_leaves\" : int(num_leaves),\n",
    "\"max_depth\" : int(max_depth),\n",
    "\"lambda_l2\" : lambda_l2,\n",
    "\"lambda_l1\" : lambda_l1,\n",
    "\"num_threads\" : 4,\n",
    "\"min_child_samples\" : int(min_child_samples),\n",
    "\"learning_rate\" : 0.03,\n",
    "\"bagging_fraction\" : bagging_fraction,\n",
    "\"feature_fraction\" : feature_fraction,\n",
    "\"subsample_freq\" : 5,\n",
    "\"bagging_seed\" : 42,\n",
    "\"verbosity\" : -1\n",
    "}\n",
    "\n",
    "#--------------------\n",
    "# CV\n",
    "#--------------------\n",
    "\n",
    "col_need_for_computing_f1 = ['item_id']\n",
    "\n",
    "\n",
    "\n",
    "unique_vis = np.array(sorted(data[group_by].astype(str).unique()))\n",
    "folds = GroupKFold(n_splits)\n",
    "ids = np.arange(data.shape[0]) # index of row for whole data\n",
    "\n",
    "self.fold_ids = [] # saving training and validating index of row for each fold\n",
    "for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "    # trn_vis: 1-D array with index of training row\n",
    "    # val_vis: 1-D array with index of validating row\n",
    "    self.fold_ids.append([\n",
    "            ids[data[group_by].astype(str).isin(unique_vis[trn_vis])],\n",
    "            ids[data[group_by].astype(str).isin(unique_vis[val_vis])]\n",
    "        ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
