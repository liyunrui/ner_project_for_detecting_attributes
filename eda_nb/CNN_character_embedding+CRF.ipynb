{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "sys.path.append('../models')\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel # for building our customized\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data_test)\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "        self.train_df = DataFrame(columns=data_cols, data=data_train)\n",
    "        self.val_df = DataFrame(columns=data_cols, data=data_val)\n",
    "\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = 3)\n",
    "\n",
    "        \n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        '''All row in our dataframe need to predicted as input of second-level model'''\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    \n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-based char-leve extraction reference: 1. https://github.com/IsaacChanghau/neural_sequence_labeling/blob/master/models/nns.py#L64\n",
    "# https://github.com/IsaacChanghau/neural_sequence_labeling/blob/master/models/blstm_cnn_crf_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 36), (2, 36), (2,), (2, 36, 54), (2, 36))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id = np.load(os.path.join('../models/data/train/', '{}.npy'.format('word_id')))\n",
    "history_length = np.load(os.path.join('../models/data/train/', '{}.npy'.format('history_length')))\n",
    "label = np.load(os.path.join('../models/data/train/', '{}.npy'.format('label')))\n",
    "item_id = np.load(os.path.join('../models/data/train/','{}.npy'.format('item_id')))\n",
    "char_id = np.load(os.path.join('../models/data/train/','{}.npy'.format('char_id')))\n",
    "word_lengths = np.load(os.path.join('../models/data/train/','{}.npy'.format('word_length')))\n",
    "sentence = word_id[:2]\n",
    "y_true = label[:2]\n",
    "length = history_length[:2]\n",
    "char_id_____ = char_id[:2]\n",
    "word_lengths______ = word_lengths[:2]\n",
    "\n",
    "sentence.shape, y_true.shape, length.shape, char_id_____.shape, word_lengths______.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 11], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, optimizer='adam'):\n",
    "    '''\n",
    "    It's for choosing optimizer given learning rate.\n",
    "    '''\n",
    "    if optimizer == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    elif soptimizer == 'rms':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        # assert is a good way to tell other how to use this function for bug happening.\n",
    "        #-------\n",
    "        # standard way to pring the error\n",
    "        #-------\n",
    "        assert False, 'optimizer must be adam, adagrad, sgd, or rms'\n",
    "\n",
    "def update_parameters(loss, optimizer = 'adam'):\n",
    "    '''\n",
    "    It's for optimizing and logging training parameters\n",
    "    \n",
    "    1.using gradient clipping to avoid gradient explosion and vanishment.\n",
    "    \n",
    "    Gradient clipping is most common in recurrent neural networks. \n",
    "    When gradients are being propagated back in time, they can vanish \n",
    "    because they they are continuously multiplied by numbers less than one.\n",
    "    This is called the vanishing gradient problem. \n",
    "    This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, \n",
    "    This is solved by residual connections. \n",
    "    On the other hand, you can have exploding gradients too. \n",
    "    This is when they get exponentially large from being multiplied by numbers larger \n",
    "    than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    grad_clip = 5 # Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "    regularization_constant = 0.1 # Regularization constant applied to all trainable parameters.\n",
    "    enable_parameter_averaging = False # If true, model saves exponential weighted averages of parameters to separate checkpoint file.\n",
    "    global_step = tf.Variable(0, trainable = False) # Optional Variable to increment by one after the variables have been updated.\n",
    "    learning_rate_var = tf.Variable(0.0, trainable = False)\n",
    "    \n",
    "    #----------------\n",
    "    # for understanding regularization\n",
    "    #----------------\n",
    "    trainable_variables_1 = tf.trainable_variables()[0]\n",
    "    square_1 = tf.square(trainable_variables_1)\n",
    "    sum_1 = tf.reduce_sum(square_1)\n",
    "    sqrt = tf.sqrt(sum_1)\n",
    "    #-----------------\n",
    "    # we can customized our regularization on the parameters we like\n",
    "    #-----------------\n",
    "    if regularization_constant != 0:\n",
    "        # l2_norm: is a 0-D tensor. \n",
    "        # we do l2-norm on each trainable's parameters.\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()]) # Returns list including all variables created with trainable=True\n",
    "        # the smaller the loss is, the better do finish overfitting \n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "    #-----------------\n",
    "    # optimizing\n",
    "    #-----------------\n",
    "    # define the optimizer\n",
    "    optimizer = get_optimizer(learning_rate_var, optimizer=optimizer)\n",
    "    # compute grads: return A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    # standard way to do gradient clipping\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "    step = optimizer.apply_gradients(clipped, global_step = global_step)\n",
    "    print ('step - whtat optimizer.apply_gradients returns', step)\n",
    "    #-----------------\n",
    "    # if using moving average techniques\n",
    "    #-----------------\n",
    "    if enable_parameter_averaging:\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "        maintain_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([step]):\n",
    "            step = tf.group(maintain_averages_op)\n",
    "    else:\n",
    "        step = step\n",
    "    return learning_rate_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_utils import temporal_convolution_layer\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from tf_utils import sequence_evaluation_metric\n",
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "from tf_utils import shape_of_tensor\n",
    "\n",
    "\n",
    "dim_word = 300 # lstm on word embeddings\n",
    "\n",
    "trainable_embedding = False\n",
    "USE_PRETRAINED = True\n",
    "USE_CHARS = True\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "filename_chars_voc = \"../models/data/wordvec/chars_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "nchars = len(load_vocab_and_return_word_to_id_dict(filename_chars_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "\n",
    "#----------\n",
    "# for debugging\n",
    "#-------------\n",
    "max_seq_length = 36\n",
    "max_word_length = 54\n",
    "num_layers = 2\n",
    "hidden_size_cnn = 300\n",
    "dim_char = 100\n",
    "k = 3\n",
    "ntags = 3\n",
    "# num residual block we used\n",
    "hidden_size_char = 200\n",
    "num_channels = [hidden_size_char,hidden_size_char] # 200 is hidden ouput unit of char\n",
    "# causal\n",
    "causal  = False\n",
    "filter_widths = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_crf_loss(y, y_hat, sequence_lengths, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of tag sequences in a CRF on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length], which should be indices of label.\n",
    "        y_hat: Prediction tensor, [batch size, max_sequence_length, num_class], which should be unscaled score.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        batch_softmax_loss. 0-dimensional tensor.\n",
    "        transition matrix.\n",
    "    Reference of sparse_softmax_cross_entropy_with_logits:\n",
    "        -Official: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "        -Blog : https://blog.csdn.net/yc461515457/article/details/77861695\n",
    "    \"\"\"\n",
    "    # compute log-likelihood between y(y_true) and y_hat(y_pred)\n",
    "    log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(y_hat, y, sequence_lengths) # log_likelihood with shape of [batch_size]\n",
    "    batch_crf_loss = tf.reduce_mean(-log_likelihood)\n",
    "    return batch_crf_loss, trans_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no need to use additional 1x1 convolution to solve discrepant input-output widths\n",
      "Original features : (?, 36, 500)\n",
      "CNN-0 layer : (?, 36, 300)\n",
      "CNN-1 layer : (?, 36, 300)\n",
      "Output layer : (?, 36, 3)\n",
      "y_true : (?, 36)\n",
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tf_utils import TemporalConvNet\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    ####################################\n",
    "    # Step1: get input_sequences \n",
    "    ####################################\n",
    "\n",
    "    #------------\n",
    "    # 1-D  \n",
    "    #------------\n",
    "    item_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "    #------------   \n",
    "    # 2-D  \n",
    "    #------------\n",
    "    word_id = tf.placeholder(tf.int32, [None, max_seq_length]) \n",
    "    label = tf.placeholder(tf.int32, [None, max_seq_length]) # [batch_size, num_class]\n",
    "    if USE_CHARS:\n",
    "        word_lengths = tf.placeholder(tf.int32, shape=[None, max_seq_length])\n",
    "    #------------   \n",
    "    # 3-D  \n",
    "    #------------\n",
    "    if USE_CHARS:\n",
    "        char_ids = tf.placeholder(tf.int32, shape=[None, max_seq_length, max_word_length]) # [batch_size, max_seq_length, max_word_length]\n",
    "\n",
    "    #------------\n",
    "    # boolean parameter\n",
    "    #------------\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #------------\n",
    "    # word_embedding: get word embeddings matrix\n",
    "    #------------\n",
    "    if embeddings is None:\n",
    "        logging.info('WARNING: randomly initializing word vectors')\n",
    "        word_embeddings = tf.get_variable(\n",
    "        shape = [nwords, dim_word],\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32,\n",
    "        )\n",
    "    else:\n",
    "        word_embeddings = tf.get_variable(\n",
    "        initializer = embeddings, # it will hold the embedding\n",
    "        trainable = trainable_embedding,\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32\n",
    "        )\n",
    "    word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = word_id)\n",
    "    #------------\n",
    "    # char_embedding: get char embeddings matrix\n",
    "    #------------\n",
    "    if USE_CHARS:\n",
    "        # get char embeddings matrix\n",
    "        char_embeddings = tf.get_variable(\n",
    "                shape=[nchars, dim_char],\n",
    "                name=\"char_embeddings\",\n",
    "                trainable = True,\n",
    "                dtype=tf.float32,\n",
    "        )\n",
    "        # get char_representation, 4-D, [batch_size, max_seq_length, max_word_length, dim_char]\n",
    "        char_representation = tf.nn.embedding_lookup(params = char_embeddings, ids = char_ids)\n",
    "        # convert 4-D into 3-D: put the timestep on axis=1 and should be charater-level axis\n",
    "        s = tf.shape(char_representation) # 1-D tensor, (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "        char_representation = tf.reshape(char_representation, shape=[ s[0]*s[1], s[-2], dim_char]) # [batch_size * max_seq_length, max_word_length, dim_char]\n",
    "        #---------------\n",
    "        # CNN-based Word level representation from characters embeddings\n",
    "        #---------------\n",
    "        output = TemporalConvNet(inputs=char_representation, num_channels = num_channels, convolution_width= filter_widths, causal = causal)\n",
    "        # max pooling on char-axis\n",
    "        output = tf.reduce_max(output, axis=1)\n",
    "        # reshape to word level representation\n",
    "        word_representation_extracted_from_char = tf.reshape(output, shape=[s[0], s[1], hidden_size_char]) # [batch_size, max_seq_length, hidden_size_char]\n",
    "\n",
    "    x = tf.concat([\n",
    "    word_representation,\n",
    "    word_representation_extracted_from_char\n",
    "        ], axis = 2) # (?, 122, 300)\n",
    "\n",
    "    ####################################\n",
    "    # Step2: calculate_outputs \n",
    "    ####################################\n",
    "    \n",
    "    #-------------------------\n",
    "    # NN architecuture-Simple CNN\n",
    "    #-------------------------\n",
    "    print ('Original features : {}'.format(x.shape))\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            conv = temporal_convolution_layer(x , \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None, \n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "        else:\n",
    "            conv = temporal_convolution_layer(conv, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "            \n",
    "        print ('CNN-{} layer : {}'.format(i, conv.shape))\n",
    "    # output layer (linear)\n",
    "    y_hat = time_distributed_dense_layer(conv, ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "    print ('Output layer : {}'.format(y_hat.shape))\n",
    "    print ('y_true : {}'.format(label.shape))\n",
    "    #--------------\n",
    "    # for second-level model\n",
    "    #--------------\n",
    "    prediction_tensors = {\n",
    "        'item_id':item_id,\n",
    "        'word_id':word_id,\n",
    "        'final_states':conv, # 修改不要全部max_seq_lenghth都存, 只存到history_length的長度(save memory)\n",
    "        'final_predictions':y_hat,\n",
    "    }\n",
    "    \n",
    "    ####################################\n",
    "    # Step3: calculate_loss +evaluation score+ optimizing\n",
    "    ####################################\n",
    "    loss = sequence_softmax_loss(y = label, y_hat = y_hat, sequence_lengths = history_length, max_sequence_length = max_seq_length)\n",
    "    learning_rate_var  = update_parameters(loss)\n",
    "    \n",
    "    \n",
    "    labels_pred = tf.cast(tf.argmax(y_hat, axis= 2),tf.int32) # (?, max_seq_length)\n",
    "    log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(y_hat,label, history_length)\n",
    "    batch_crf_loss = tf.reduce_mean(-log_likelihood)\n",
    "    score = sequence_evaluation_metric(y = label, y_hat = labels_pred, sequence_lengths = history_length, max_sequence_length = max_seq_length)['f1']\n",
    "\n",
    "#     ####################################\n",
    "#     # Step4: saving the model \n",
    "#     ####################################    \n",
    "#     # create saver object\n",
    "#     # max_to_keep: indicates the maximum number of recent checkpoint files to keep.\n",
    "#     saver = tf.train.Saver(max_to_keep = 1)\n",
    "#     if enable_parameter_averaging:\n",
    "#         saver_averaged = tf.train.Saver(ema.variables_to_restore(), max_to_keep=1)    \n",
    "\n",
    "    #-------------------------\n",
    "    # standard\n",
    "    #-------------------------\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque # for computing Train/validation losses are averaged over the last loss_averaging_window\n",
    "import tensorflow as tf\n",
    "warm_start_init_step = 0 # If nonzero, model will resume training a restored model beginning at warm_start_init_step.\n",
    "batch_size = 128\n",
    "loss_averaging_window = 10\n",
    "num_validation_batches = 1\n",
    "num_training_steps = 10\n",
    "learning_rate=0.001\n",
    "log_interval = 1\n",
    "min_steps_to_checkpoint =1\n",
    "early_stopping_steps = 10\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    ####################################\n",
    "    # 1. fit\n",
    "    ####################################\n",
    "    if warm_start_init_step:\n",
    "        # continue the optimization at a recent checkpoint instead of having to restart the optimization from the beginning\n",
    "        restore(warm_start_init_step)\n",
    "        step = warm_start_init_step\n",
    "    else:\n",
    "        # start the optimization from the beginning\n",
    "        sess.run(init) # Run the initializer\n",
    "        step = 0\n",
    "     \n",
    "    log_likelihood_, batch_crf_loss_, trans_params_  = sess.run(fetches = [log_likelihood,batch_crf_loss, trans_params], feed_dict = {\n",
    "                                                 word_id:sentence,\n",
    "                                                 label: y_true,\n",
    "                                                 history_length: length,\n",
    "                                                 char_ids: char_id_____,\n",
    "                                                 #word_lengths:word_lengths______\n",
    "                \n",
    "                                                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.105577, -20.77645 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_params_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15558529, -0.9989271 ,  0.6235478 ],\n",
       "       [-0.96864796, -0.9568076 ,  0.42102146],\n",
       "       [ 0.7179775 ,  0.992568  ,  0.8633115 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.441013"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_crf_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv1d(in_, filter_size, height, padding, is_train=True, drop_rate=0.0, scope=None):\n",
    "#     with tf.variable_scope(scope or \"conv1d\"):\n",
    "#         num_channels = in_.get_shape()[-1]\n",
    "#         filter_ = tf.get_variable(\"filter\", shape=[1, height, num_channels, filter_size], dtype=tf.float32)\n",
    "#         bias = tf.get_variable(\"bias\", shape=[filter_size], dtype=tf.float32)\n",
    "#         strides = [1, 1, 1, 1]\n",
    "#         in_ = tf.layers.dropout(in_, rate=drop_rate, training=is_train)\n",
    "#         # [batch, max_len_sent, max_len_word / filter_stride, char output size]\n",
    "#         xxc = tf.nn.conv2d(in_, filter_, strides, padding) + bias\n",
    "#         out = tf.reduce_max(tf.nn.relu(xxc), axis=2)  # max-pooling, [-1, max_len_sent, char output size]\n",
    "#         return out\n",
    "\n",
    "\n",
    "# def multi_conv1d(in_, filter_sizes, heights, padding=\"VALID\", is_train=True, drop_rate=0.0, scope=None):\n",
    "#     with tf.variable_scope(scope or \"multi_conv1d\"):\n",
    "#         assert len(filter_sizes) == len(heights)\n",
    "#         outs = []\n",
    "#         for i, (filter_size, height) in enumerate(zip(filter_sizes, heights)):\n",
    "#             if filter_size == 0:\n",
    "#                 continue\n",
    "#             out = conv1d(in_, filter_size, height, padding, is_train=is_train, drop_rate=drop_rate,\n",
    "#                          scope=\"conv1d_{}\".format(i))\n",
    "#             outs.append(out)\n",
    "#         concat_out = tf.concat(axis=2, values=outs)\n",
    "#         return concat_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*36*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
