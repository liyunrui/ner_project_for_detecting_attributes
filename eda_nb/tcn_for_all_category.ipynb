{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Nov 8 2018\n",
    "\n",
    "This is an implementation of An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling in TensorFlow.\n",
    "\n",
    "Reference:\n",
    "    -Code: https://github.com/YuanTingHsieh/TF_TCN\n",
    "    -Figure: https://github.com/philipperemy/keras-tcn\n",
    "    -Spatial Droput: https://colab.research.google.com/drive/1la33lW7FQV1RicpfzyLq9H0SH1VSD4LE#scrollTo=YRTsgwSGy-gK\n",
    "\n",
    "@author: Ray\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../models/')\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "# for reading data\n",
    "from data_frame import DataFrame\n",
    "# for building our customized tensorflow model\n",
    "from tf_base_model import TFBaseModel \n",
    "from tf_utils import TemporalConvNet\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from tf_utils import sequence_evaluation_metric\n",
    "# laoding data\n",
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'char_id',\n",
    "            'word_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        data_train = [np.load(os.path.join(data_dir, 'train/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        data_val = [np.load(os.path.join(data_dir, 'val/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        data_test = [np.load(os.path.join(data_dir, 'test/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data_test)\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "        self.train_df = DataFrame(columns=data_cols, data=data_train)\n",
    "        self.val_df = DataFrame(columns=data_cols, data=data_val)\n",
    "\n",
    "        print ('shape of whole data : {}'.format(self.test_df.shapes()))\n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        '''All row in our dataframe need to predicted as input of second-level model'''\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    \n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n",
    "\n",
    "class tcn(TFBaseModel):\n",
    "    \n",
    "    def __init__(self,max_seq_len,filter_widths,ntags,trainable_embedding,dim_word,nwords,metric,use_chars,causal,\n",
    "                 dim_char,max_word_length,nchars,hidden_size_char,num_channels,char_representation_method,\n",
    "                 num_channels_char,filter_widths_char,**kwargs):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.filter_widths = filter_widths\n",
    "        self.ntags = ntags # n_class\n",
    "        self.dim_word = dim_word\n",
    "        self.nwords = nwords\n",
    "        self.trainable_embedding = trainable_embedding\n",
    "        self.metric = metric\n",
    "        self.USE_CHARS = use_chars\n",
    "        self.num_channels = num_channels # list of output unit for each residual block\n",
    "        self.causal = causal\n",
    "        if self.USE_CHARS:\n",
    "            self.char_representation_method = char_representation_method\n",
    "            try:\n",
    "                self.dim_char = dim_char\n",
    "                self.max_word_length = max_word_length\n",
    "                self.nchars = nchars\n",
    "                self.hidden_size_char = hidden_size_char\n",
    "            except:\n",
    "                assert False, 'Please assing dim_char, max_word_length, and nchars as arguments'\n",
    "            if char_representation_method == 'CNN':\n",
    "                self.filter_widths_char = filter_widths_char\n",
    "                self.num_channels_char = num_channels_char\n",
    "        # self.super call  _init_ function of parent class \n",
    "        super(tcn, self).__init__(**kwargs)\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        ####################################\n",
    "        # Step1: get input_sequences \n",
    "        ####################################\n",
    "        #------------\n",
    "        # 1-D  \n",
    "        #------------\n",
    "        self.item_id = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "        #------------   \n",
    "        # 2-D  \n",
    "        #------------\n",
    "        self.word_id = tf.placeholder(tf.int32, [None, self.max_seq_len]) \n",
    "        self.label = tf.placeholder(tf.int32, [None, self.max_seq_len]) # [batch_size, num_class]\n",
    "        if self.USE_CHARS:\n",
    "            if self.char_representation_method == 'BI-LSTM':\n",
    "                self.word_length = tf.placeholder(tf.int32, shape=[None, self.max_seq_len])\n",
    "            else:\n",
    "                pass\n",
    "        #------------   \n",
    "        # 3-D  \n",
    "        #------------\n",
    "        if self.USE_CHARS:\n",
    "            self.char_id = tf.placeholder(tf.int32, shape=[None, self.max_seq_len, self.max_word_length]) # [batch_size, max_seq_length, max_word_length]\n",
    "\n",
    "\n",
    "        #------------\n",
    "        # boolean parameter\n",
    "        #------------\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #------------\n",
    "        # word_embedding: get embeddings matrix\n",
    "        #------------\n",
    "        if embeddings is None:\n",
    "            logging.info('WARNING: randomly initializing word vectors')\n",
    "            word_embeddings = tf.get_variable(\n",
    "            shape = [nwords, dim_word],\n",
    "            name = 'word_embeddings',\n",
    "            dtype = tf.float32,\n",
    "            )\n",
    "        else:\n",
    "            word_embeddings = tf.get_variable(\n",
    "            initializer = embeddings, # it will hold the embedding\n",
    "            #shape = [word2vec.shape[0], word2vec.shape[1]], # [num_vocabulary, embeddings_dim]\n",
    "            trainable = trainable_embedding,\n",
    "            name = 'word_embeddings',\n",
    "            dtype = tf.float32\n",
    "            )\n",
    "        word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = self.word_id)\n",
    "        #------------\n",
    "        # char_embedding: get char embeddings matrix\n",
    "        #------------\n",
    "        if self.USE_CHARS:\n",
    "            if self.char_representation_method == 'BI-LSTM':\n",
    "                # get char embeddings matrix\n",
    "                char_embeddings = tf.get_variable(\n",
    "                        shape=[self.nchars, self.dim_char],\n",
    "                        name=\"char_embeddings\",\n",
    "                        trainable = True,\n",
    "                        dtype=tf.float32,\n",
    "                )\n",
    "                # get char_representation, 4-D, [batch_size, max_seq_length, max_word_length, dim_char]\n",
    "                char_representation = tf.nn.embedding_lookup(params = char_embeddings, ids = self.char_id) \n",
    "                # convert 4-D into 3-D: put the timestep on axis=1 and should be charater-level axis\n",
    "                s = tf.shape(char_representation) # 1-D tensor, (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "                char_representation = tf.reshape(char_representation, shape=[ s[0]*s[1], s[-2], self.dim_char]) # [batch_size * max_seq_length, max_word_length, dim_char]\n",
    "                # for computing bi lstm on chars\n",
    "                word_lengths = tf.reshape(self.word_length, shape=[s[0]*s[1]]) # 1-D tensor\n",
    "                # bi lstm on chars\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        cell_fw, \n",
    "                        cell_bw, \n",
    "                        inputs = char_representation,\n",
    "                        sequence_length = word_lengths, \n",
    "                        dtype=tf.float32) \n",
    "                \"\"\"\n",
    "                Return A tuple (outputs, output_states) \n",
    "                  -outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output\n",
    "                       1.output_fw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char].For example, (72, 54, 100)\n",
    "                       2.output_bw with shape of [batch_szie*max_word_lenght, max_word_length, hidden_size_char]\n",
    "                  -output_states: A tuple (output_state_fw, output_state_bw) containing the forward and the backward final states of bidirectional rnn.\n",
    "                       1.final_state_output_fw with shape of [batch_szie*max_word_lenght, hidden_size_char]. For instance, (72, 100)\n",
    "                       2.final_state_output_bw with shape of [batch_szie*max_word_lenght, hidden_size_char]\n",
    "\n",
    "                \"\"\"\n",
    "                # get word level representation from characters embeddings\n",
    "                _, ((_, output_fw_final_state), (_, output_bw_final_state)) = _output\n",
    "                output = tf.concat([output_fw_final_state, output_bw_final_state], axis=-1)# concat on char_embedding level, [batch_szie*max_word_lenght, 2*hidden_size_char]\n",
    "                # reshape to word level representation\n",
    "                word_representation_extracted_from_char = tf.reshape(output, shape=[s[0], s[1], 2* hidden_size_char]) # [batch_size, max_seq_length, 2*hidden_size_char]\n",
    "            elif self.char_representation_method == 'CNN':\n",
    "                # get char embeddings matrix\n",
    "                char_embeddings = tf.get_variable(\n",
    "                        shape=[self.nchars, self.dim_char],\n",
    "                        name=\"char_embeddings\",\n",
    "                        trainable = True,\n",
    "                        dtype=tf.float32,\n",
    "                )\n",
    "                # get char_representation, 4-D, [batch_size, max_seq_length, max_word_length, dim_char]\n",
    "                char_representation = tf.nn.embedding_lookup(params = char_embeddings, ids = self.char_id)\n",
    "                # convert 4-D into 3-D: put the timestep on axis=1 and should be charater-level axis\n",
    "                s = tf.shape(char_representation) # 1-D tensor, (batch_size, max_seq_length, max_word_length, dim_char)\n",
    "                char_representation = tf.reshape(char_representation, shape=[ s[0]*s[1], s[-2], self.dim_char]) # [batch_size * max_seq_length, max_word_length, dim_char]\n",
    "                #---------------\n",
    "                # CNN-based Word level representation from characters embeddings\n",
    "                #---------------\n",
    "                output = TemporalConvNet(inputs=char_representation, \n",
    "                                         num_channels = self.num_channels_char, \n",
    "                                         convolution_width= self.filter_widths_char, \n",
    "                                         causal = self.causal,\n",
    "                                         scope = 'char_level')\n",
    "                # max pooling on char-axis\n",
    "                output = tf.reduce_max(output, axis=1)\n",
    "                # reshape to word level representation\n",
    "                word_representation_extracted_from_char = tf.reshape(output, shape=[s[0], s[1], self.num_channels_char[-1]]) # [batch_size, max_seq_length, hidden_size_char]\n",
    "            else:\n",
    "                assert False, 'Now, we only provide BI-LSTM and CNN'\n",
    "\n",
    "        if self.USE_CHARS:\n",
    "            x = tf.concat([\n",
    "            word_representation,\n",
    "            word_representation_extracted_from_char\n",
    "                ], axis = 2) # (?, 36, 500 == 300+200)  \n",
    "        else:\n",
    "            x = tf.concat([word_representation\n",
    "                ], axis=2) # (?, 36, 300)\n",
    "        #print ('Original features : {}'.format(x.shape))\n",
    "        return x\n",
    "    \n",
    "    def calculate_outputs(self, x):\n",
    "        ####################################\n",
    "        # Step2: calculate_outputs \n",
    "        ####################################\n",
    "        self.tcn = TemporalConvNet(inputs=x, num_channels = self.num_channels, convolution_width= self.filter_widths, causal = self.causal, scope = 'word_level')\n",
    "        y_hat = time_distributed_dense_layer(self.tcn, self.ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "        #--------------\n",
    "        # for second-level model: \n",
    "        #--------------\n",
    "        # method1: using final_temporal_idx, it's for only get the final step. -->That's not here task I want\n",
    "        # method2: Only saving history_length rather tahn saving max_seq_lenght --> By far, don't know how to do that.\n",
    "        self.prediction_tensors = {\n",
    "            'item_id':self.item_id, # (?, )\n",
    "            'word_id':self.word_id, # (?, max_seq_length)\n",
    "            'history_length':self.history_length,\n",
    "            'final_states':self.tcn, # (?, max_seq_length, num_hidden_units)\n",
    "            'final_predictions':y_hat, # (?, max_seq_length, ntags)\n",
    "        }\n",
    "        return y_hat\n",
    "    \n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        self.preds = self.calculate_outputs(x)\n",
    "        loss = sequence_softmax_loss(y = self.label, \n",
    "                                     y_hat = self.preds, \n",
    "                                     sequence_lengths = self.history_length, \n",
    "                                     max_sequence_length = self.max_seq_len)\n",
    "        return loss\n",
    "    \n",
    "    def calculate_evaluation_metric(self):\n",
    "        labels_pred = tf.cast(tf.argmax(self.preds, axis= 2),tf.int32) # (?, max_seq_length)\n",
    "        score = sequence_evaluation_metric(y = self.label,\n",
    "                                           y_hat = labels_pred, \n",
    "                                           sequence_lengths = self.history_length,\n",
    "                                           max_sequence_length = self.max_seq_len\n",
    "                                          )[self.metric]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lips', 'dress', 'face', 'women_top', 'mobile']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/data/ner_task/dress/shopee_data_tagging_result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======category======= lips\n",
      "loaded data\n",
      "shape of whole data : item_id                  (44041,)\n",
      "word_id               (44041, 27)\n",
      "history_length           (44041,)\n",
      "char_id           (44041, 27, 71)\n",
      "word_length           (44041, 27)\n",
      "label                 (44041, 27)\n",
      "dtype: object\n",
      "number of training example: 30643\n",
      "number of validating example: 4548\n",
      "number of testing example: 44041\n",
      "=======category======= dress\n",
      "loaded data\n",
      "shape of whole data : item_id                  (3845,)\n",
      "word_id               (3845, 19)\n",
      "history_length           (3845,)\n",
      "char_id           (3845, 19, 78)\n",
      "word_length           (3845, 19)\n",
      "label                 (3845, 19)\n",
      "dtype: object\n",
      "number of training example: 2881\n",
      "number of validating example: 500\n",
      "number of testing example: 3845\n",
      "=======category======= face\n",
      "loaded data\n",
      "shape of whole data : item_id                  (73833,)\n",
      "word_id               (73833, 28)\n",
      "history_length           (73833,)\n",
      "char_id           (73833, 28, 48)\n",
      "word_length           (73833, 28)\n",
      "label                 (73833, 28)\n",
      "dtype: object\n",
      "number of training example: 50806\n",
      "number of validating example: 7931\n",
      "number of testing example: 73833\n",
      "=======category======= women_top\n",
      "loaded data\n",
      "shape of whole data : item_id                  (7807,)\n",
      "word_id               (7807, 20)\n",
      "history_length           (7807,)\n",
      "char_id           (7807, 20, 28)\n",
      "word_length           (7807, 20)\n",
      "label                 (7807, 20)\n",
      "dtype: object\n",
      "number of training example: 5730\n",
      "number of validating example: 755\n",
      "number of testing example: 7807\n",
      "=======category======= mobile\n",
      "loaded data\n",
      "shape of whole data : item_id                  (186409,)\n",
      "word_id               (186409, 32)\n",
      "history_length           (186409,)\n",
      "char_id           (186409, 32, 54)\n",
      "word_length           (186409, 32)\n",
      "label                 (186409, 32)\n",
      "dtype: object\n",
      "number of training example: 145267\n",
      "number of validating example: 14510\n",
      "number of testing example: 186409\n"
     ]
    }
   ],
   "source": [
    "for category in os.listdir('/data/ner_task/dress/shopee_data_tagging_result/'):\n",
    "    print ('=======category=======', category)\n",
    "    #--------------------------\n",
    "    # setting\n",
    "    #--------------------------\n",
    "    dim_word = 300\n",
    "    trainable_embedding = False\n",
    "    USE_PRETRAINED = True\n",
    "    filename_words_vec = \"../models/data/wordvec/word2vec.npz\"\n",
    "    filename_words_voc = \"../models/data/wordvec/{}_training/words_vocab.txt\".format(category)\n",
    "    filename_chars_voc = \"../models/data/wordvec/{}_training/chars_vocab.txt\".format(category)\n",
    "    base_dir = ''\n",
    "\n",
    "    nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "    embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    enable_parameter_averaging = False\n",
    "    USE_CHARS = True\n",
    "    if USE_CHARS:\n",
    "        hidden_size_char = 100\n",
    "        nchars = len(load_vocab_and_return_word_to_id_dict(filename_chars_voc))\n",
    "    char_representation_method = 'CNN'\n",
    "    filter_widths_char = 5\n",
    "    num_channels_char = [200]\n",
    "    # num residual block we used\n",
    "    num_channels = [300, 250, 200, 150, 100, 50]\n",
    "    # causal\n",
    "    causal  = False\n",
    "    # read data\n",
    "    dr = DataReader(data_dir ='/data/ner_task/data_for_brand_detection_model/{}/'.format(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.88934627374013"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2213.360776424408 /60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "145267+14510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
