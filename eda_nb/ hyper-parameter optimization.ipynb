{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "sys.path.append('../models/blend/')\n",
    "from utils import customized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# setting\n",
    "#---------------------\n",
    "BINARY_SCENARIO = None\n",
    "#---------------------\n",
    "# load features\n",
    "#---------------------\n",
    "feature_dir = '../features/lazada_and_amazon/all_features.h5'\n",
    "df = pd.read_hdf(feature_dir)\n",
    "#---------------------\n",
    "# label post-processing\n",
    "#---------------------\n",
    "if df.label.nunique() == 2: \n",
    "    BINARY_SCENARIO = True\n",
    "    # binary class\n",
    "    df['label'] = df.label.apply(lambda x: 1 if x == 2 else 0) # for customized f1 score inference of lgb\n",
    "else:\n",
    "    # multi-class(B, I or O)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238668, 575)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# setting\n",
    "#---------------------\n",
    "target_col = 'label'\n",
    "features = df.columns.tolist()[7:]\n",
    "# parameters\n",
    "NUM_LEAVES = 31\n",
    "COLSAMPLE_BYTREE = 1.0\n",
    "SUBSAMPLE = 1.0\n",
    "SUBSAMPLE_FREQ = 0\n",
    "MAX_DEPTH = -1\n",
    "REG_LAMBDA = 0.0\n",
    "REG_LAMBDA = 0.0\n",
    "REG_ALPHA = 0.0\n",
    "MIN_SPLIT_GAIN = 0\n",
    "MIN_CHILD_WEIGHT = 0.001\n",
    "MAX_BIN = 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_fold = True\n",
    "target = 'label'\n",
    "n_splits = 5\n",
    "def lgb_eval(train, features, target_col,\n",
    "             num_leaves,max_depth,lambda_l2,\n",
    "             lambda_l1,min_child_samples,bagging_fraction,\n",
    "             feature_fraction):\n",
    "    '''\n",
    "    Notice that:\n",
    "        Bayesian optimization is designed to find optimal value through maximization.\n",
    "        So, if yur target function is loss function. For example, rmse. the lower, the better. \n",
    "        Don't forget to put negative into the target function.\n",
    "        However, if your target function is evluation metric. For example, f1-score. the higher, the better.\n",
    "        There is no need to put negative when returning.\n",
    "    '''\n",
    "    #--------------------\n",
    "    # setting\n",
    "    #--------------------\n",
    "    fit_params = {\n",
    "    \"objective\": 'binary',\n",
    "    \"metric\": 'None', # Please remember do specify metric == None for using custom evaluation metrci to do early stopping. \n",
    "    \"num_leaves\" : int(num_leaves),\n",
    "    \"max_depth\" : int(max_depth),\n",
    "    \"lambda_l2\" : lambda_l2,\n",
    "    \"lambda_l1\" : lambda_l1,\n",
    "    \"num_threads\" : 4,\n",
    "    \"min_child_samples\" : int(min_child_samples),\n",
    "    \"learning_rate\" : 0.03,\n",
    "    \"bagging_fraction\" : bagging_fraction,\n",
    "    \"feature_fraction\" : feature_fraction,\n",
    "    \"subsample_freq\" : 5,\n",
    "    \"bagging_seed\" : 42,\n",
    "    \"verbosity\" : -1\n",
    "    }\n",
    "    col_need_for_computing_f1 = ['item_id']\n",
    "\n",
    "    #--------------------\n",
    "    # convert data into lgb.dataset\n",
    "    #--------------------\n",
    "    y = train[target_col].copy()\n",
    "    dtrain = lgb.Dataset(train[features], y, free_raw_data=False)\n",
    "    item_id_for_customized_f1 = train[col_need_for_computing_f1].iloc[val_idx].values.reshape(len(val_idx))\n",
    "    #--------------------\n",
    "    # CV\n",
    "    #--------------------\n",
    "    if group_fold == True:\n",
    "        gfolds = GroupKFold(n_splits)\n",
    "        unique_vis = np.array(sorted(train['fullVisitorId'].astype(str).unique()))\n",
    "        #--------------------------------\n",
    "        # Here we need our customized cross-validation result\n",
    "        #--------------------------------\n",
    "        cv_result = lgb.cv(params = params,\n",
    "                           train_set = dtrain,\n",
    "                           num_boost_round = 1,\n",
    "                           folds = gfolds.split(X=unique_vis, y=unique_vis, groups=unique_vis),\n",
    "                           feval = customized_eval(data = item_id_for_customized_f1, threshold = 0.5, verbose = False),\n",
    "                           early_stopping_rounds=150,\n",
    "                           stratified=False,\n",
    "                           nfold=n_splits)\n",
    "        \n",
    "    else:\n",
    "        cv_result = lgb.cv(params = params,\n",
    "                           train_set = dtrain,\n",
    "                           num_boost_round = 10000,\n",
    "                           feval = customized_eval(data = item_id_for_f1, threshold = 0.5, verbose = False),\n",
    "                           early_stopping_rounds=100,\n",
    "                           stratified=False,\n",
    "                           nfold=n_splits)\n",
    "    return cv_result['f1-score-on-sentence-level-mean'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# setting\n",
    "#--------------------\n",
    "\n",
    "group_fold = True\n",
    "target_col = 'label'\n",
    "n_splits = 2\n",
    "group_by = 'item_id'\n",
    "data = df\n",
    "fit_params = {\n",
    "\"objective\": 'binary',\n",
    "\"metric\": 'None', # Please remember do specify metric == None for using custom evaluation metrci to do early stopping. \n",
    "# \"num_leaves\" : int(31),\n",
    "# \"max_depth\" : int(1),\n",
    "# \"lambda_l2\" : 1,\n",
    "# \"lambda_l1\" : 1,\n",
    "# \"num_threads\" : 4,\n",
    "# \"min_child_samples\" : int(1),\n",
    "# \"learning_rate\" : 0.03,\n",
    "# \"bagging_fraction\" : 1,\n",
    "# \"feature_fraction\" : 1,\n",
    "# \"subsample_freq\" : 5,\n",
    "# \"bagging_seed\" : 42,\n",
    "# \"verbosity\" : -1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0 :\n",
      "[1]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[2]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[3]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[4]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[5]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[6]\tvalid_0's f1-score-on-sentence-level: 0.558525\n",
      "[7]\tvalid_0's f1-score-on-sentence-level: 0.738095\n",
      "[8]\tvalid_0's f1-score-on-sentence-level: 0.868014\n",
      "[9]\tvalid_0's f1-score-on-sentence-level: 0.898331\n",
      "[10]\tvalid_0's f1-score-on-sentence-level: 0.901428\n",
      "Fold  1 :\n",
      "[1]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[2]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[3]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[4]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[5]\tvalid_0's f1-score-on-sentence-level: 0\n",
      "[6]\tvalid_0's f1-score-on-sentence-level: 0.538222\n",
      "[7]\tvalid_0's f1-score-on-sentence-level: 0.741328\n",
      "[8]\tvalid_0's f1-score-on-sentence-level: 0.865926\n",
      "[9]\tvalid_0's f1-score-on-sentence-level: 0.895096\n",
      "[10]\tvalid_0's f1-score-on-sentence-level: 0.9026\n",
      "mean_cv_result 0.902014378837163\n"
     ]
    }
   ],
   "source": [
    "use_which_model = 'lgb'\n",
    "col_need_for_computing_f1 = ['item_id']\n",
    "cv_result = []\n",
    "#--------------------\n",
    "# CV\n",
    "#--------------------\n",
    "\n",
    "\n",
    "unique_vis = np.array(sorted(data[group_by].astype(str).unique()))\n",
    "folds = GroupKFold(n_splits)\n",
    "ids = np.arange(data.shape[0]) # index of row for whole data\n",
    "\n",
    "fold_ids = [] # saving training and validating index of row for each fold\n",
    "for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "    # trn_vis: 1-D array with index of training row\n",
    "    # val_vis: 1-D array with index of validating row\n",
    "    fold_ids.append([\n",
    "            ids[data[group_by].astype(str).isin(unique_vis[trn_vis])],\n",
    "            ids[data[group_by].astype(str).isin(unique_vis[val_vis])]\n",
    "        ])\n",
    "\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(fold_ids):\n",
    "    #---------------------\n",
    "    # train-val split\n",
    "    #---------------------\n",
    "    devel = data[features].iloc[trn_idx]\n",
    "    y_devel = data[target_col].iloc[trn_idx]\n",
    "    valid = data[features].iloc[val_idx]\n",
    "    y_valid = data[target_col].iloc[val_idx]\n",
    "\n",
    "    # for custom_f1: 1-D array with shape of (num_samples,), which each element is item_id\n",
    "    item_id_for_f1 = data[col_need_for_computing_f1].iloc[val_idx].values.reshape(len(val_idx))                 \n",
    "    print(\"Fold \", fold_id, \":\")            \n",
    "    #--------------------\n",
    "    # covert pd.DataFrame into lgb.Dataset\n",
    "    #--------------------\n",
    "\n",
    "    if use_which_model == 'lgb':\n",
    "        dtrain = lgb.Dataset(devel, label= y_devel, free_raw_data = False)\n",
    "        dvalid = lgb.Dataset(valid, label= y_valid, free_raw_data = False, reference= dtrain)\n",
    "\n",
    "        evals_result = {} # for saving the evaluation metric of validating set during training\n",
    "        model = lgb.train(params = fit_params, \n",
    "                              train_set = dtrain, \n",
    "                              num_boost_round = 10,\n",
    "                              valid_sets = dvalid, \n",
    "                              evals_result = evals_result,\n",
    "                              feval = customized_eval(data = item_id_for_f1,threshold = 0.5, verbose = False), \n",
    "                             )\n",
    "        res_score_ls = evals_result['valid_0']['f1-score-on-sentence-level']\n",
    "        cv_result.append(max(res_score_ls))\n",
    "mean_cv_result = np.mean(cv_result)\n",
    "print ('mean_cv_result',mean_cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
