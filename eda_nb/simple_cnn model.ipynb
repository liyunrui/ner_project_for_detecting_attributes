{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../models/'))\n",
    "sys.path.append('../models')\n",
    "from data_frame import DataFrame\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 3:\n",
    "    print (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data = [np.load(os.path.join(data_dir, '{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        #\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print ('shape of whole data : {}'.format(self.test_df.shapes()))\n",
    "        print ('loaded data')\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = int(time.time()))\n",
    "\n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size,shuffle = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_CODE = False\n",
    "reader = DataReader(data_dir ='../models/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = np.load(os.path.join('../models/data/', '{}.npy'.format('word_id')))\n",
    "history_length = np.load(os.path.join('../models/data/', '{}.npy'.format('history_length')))\n",
    "label = np.load(os.path.join('../models/data/', '{}.npy'.format('label')))\n",
    "item_id = np.load(os.path.join('../models/data/','{}.npy'.format('item_id')))\n",
    "eval_set = np.load(os.path.join('../models/data/','{}.npy'.format('eval_set')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (label.shape)\n",
    "print (word_id.shape)\n",
    "print (history_length.shape)\n",
    "print (item_id.shape)\n",
    "print (eval_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = word_id[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = label[0]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = history_length[0]\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/mobile_training_w_word_id.csv')\n",
    "df[df.item_id == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "def shape_of_tensor(tensor, dim=None):\n",
    "    \"\"\"\n",
    "    Get tensor shape/dimension as list/int\n",
    "    ======\n",
    "    Args:\n",
    "        tensor: tensor in tensorflow\n",
    "        dim: int, the dimension of this tensor\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        # return list\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        # t\n",
    "        return tensor.shape.as_list()[dim]\n",
    "    \n",
    "def temporal_convolution_layer(inputs, output_units, convolution_width = 3, dilated = False,\n",
    "                               causal=False, dilation_rate=[1], bias=True, activation=None, \n",
    "                               dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilated: Simple CNN or Dilated CNN\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "        scope: str.\n",
    "        reuse: boolean. If we would like to sharing this weight.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "         if dilated == True:\n",
    "            #-------------------\n",
    "            # dilated CNN\n",
    "            #-------------------\n",
    "            if causal:\n",
    "                # padding zero in the left side of sequence with (k-1) *d\n",
    "                shift = int((convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)) # modified to int\n",
    "                pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "                inputs = tf.concat([pad, inputs], axis=1)\n",
    "            \n",
    "            # filter weight\n",
    "            W = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "            )\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "            # adding bias if True\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "            # adding non-linear output if True\n",
    "            z = activation(z) if activation else z\n",
    "            # adding dropout if True\n",
    "            z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "            # output tensor of this hidden layer\n",
    "            z = z[:, :-shift, :] if causal else z\n",
    "        else:\n",
    "            #-------------------\n",
    "            # simple CNN\n",
    "            #-------------------\n",
    "            \n",
    "            # filter weight\n",
    "            W = tf.get_variable(\n",
    "                name='weights',\n",
    "                initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                # shape = spatial_filter_shape + [in_channels, out_channels]\n",
    "                shape=[convolution_width, shape_of_tensor(inputs, 2), output_units]\n",
    "            )\n",
    "            # convolution = matrix multiplication + element-wise addition\n",
    "            z = tf.nn.convolution(input = inputs, \n",
    "                          filter = W, \n",
    "                          padding='SAME', \n",
    "                          )\n",
    "            if bias:\n",
    "                b = tf.get_variable(\n",
    "                    name='biases',\n",
    "                    initializer=tf.constant_initializer(),\n",
    "                    shape=[output_units]\n",
    "                )\n",
    "                z = z + b\n",
    "\n",
    "            # adding non-linear output if True\n",
    "            z = activation(z) if activation else z\n",
    "            # adding dropout if True\n",
    "            z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "            # output tensor of this hidden layer\n",
    "            z = z[:, :-shift, :] if causal else z\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    \n",
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "\n",
    "    output_units aka residual_channels: Number of channels to use for residual connections.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            shape=[shape_of_tensor(inputs, -1), output_units]\n",
    "        )\n",
    "        # matrix multiplication\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        # doing batch_norm before activation is better for training nn.\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "        #--------\n",
    "        # In practice, activation fisst and then dropout\n",
    "        #----------\n",
    "        # adding non-linear output if True\n",
    "        z = activation(z) if activation else z\n",
    "        # adding dropout if True\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n",
    "\n",
    "def sequence_softmax_loss(y, y_hat, sequence_lengths, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Calculates average softmax cross entropy on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length], which should be index of label.\n",
    "        y_hat: Prediction tensor, [batch size, max_sequence_length, num_class], which should be unscaled score.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        batch_softmax_loss. 0-dimensional tensor.\n",
    "        \n",
    "    Reference:\n",
    "        -Official: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "        -Blog : https://blog.csdn.net/yc461515457/article/details/77861695\n",
    "    \"\"\"\n",
    "    # softmax cross-entropy between y(y_true) and y_hat(y_pred)\n",
    "    softmax_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= y_hat) # (?, max_sequence_length)\n",
    "    # returns a boolean mask tensor for the first N positions of each cell.\n",
    "    sequence_mask = tf.sequence_mask(lengths = sequence_lengths, maxlen=max_sequence_length) # (?, max_sequence_length)\n",
    "    # convert boolean into 1 or 0\n",
    "    sequence_mask = tf.cast(sequence_mask, tf.float32) # (?, max_sequence_length)\n",
    "    # compute sum of loss over each timestep / number of real training example of this batch (aka batch loss)\n",
    "    batch_softmax_loss = tf.reduce_sum(softmax_losses*sequence_mask) / tf.cast(tf.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return batch_softmax_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, optimizer='adam'):\n",
    "    '''\n",
    "    It's for choosing optimizer given learning rate.\n",
    "    '''\n",
    "    if optimizer == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    elif soptimizer == 'rms':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        # assert is a good way to tell other how to use this function for bug happening.\n",
    "        #-------\n",
    "        # standard way to pring the error\n",
    "        #-------\n",
    "        assert False, 'optimizer must be adam, adagrad, sgd, or rms'\n",
    "\n",
    "def update_parameters(loss, optimizer = 'adam'):\n",
    "    '''\n",
    "    It's for optimizing and logging training parameters\n",
    "    \n",
    "    1.using gradient clipping to avoid gradient explosion and vanishment.\n",
    "    \n",
    "    Gradient clipping is most common in recurrent neural networks. \n",
    "    When gradients are being propagated back in time, they can vanish \n",
    "    because they they are continuously multiplied by numbers less than one.\n",
    "    This is called the vanishing gradient problem. \n",
    "    This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, \n",
    "    This is solved by residual connections. \n",
    "    On the other hand, you can have exploding gradients too. \n",
    "    This is when they get exponentially large from being multiplied by numbers larger \n",
    "    than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    grad_clip = 5 # Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "    regularization_constant = 0.1 # Regularization constant applied to all trainable parameters.\n",
    "    enable_parameter_averaging = False # If true, model saves exponential weighted averages of parameters to separate checkpoint file.\n",
    "    global_step = tf.Variable(0, trainable = False) # Optional Variable to increment by one after the variables have been updated.\n",
    "    learning_rate_var = tf.Variable(0.0, trainable = False)\n",
    "    \n",
    "    #----------------\n",
    "    # for understanding regularization\n",
    "    #----------------\n",
    "    trainable_variables_1 = tf.trainable_variables()[0]\n",
    "    square_1 = tf.square(trainable_variables_1)\n",
    "    sum_1 = tf.reduce_sum(square_1)\n",
    "    sqrt = tf.sqrt(sum_1)\n",
    "    #-----------------\n",
    "    # we can customized our regularization on the parameters we like\n",
    "    #-----------------\n",
    "    if regularization_constant != 0:\n",
    "        # l2_norm: is a 0-D tensor. \n",
    "        # we do l2-norm on each trainable's parameters.\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()]) # Returns list including all variables created with trainable=True\n",
    "        # the smaller the loss is, the better do finish overfitting \n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "    #-----------------\n",
    "    # optimizing\n",
    "    #-----------------\n",
    "    # define the optimizer\n",
    "    optimizer = get_optimizer(learning_rate_var, optimizer=optimizer)\n",
    "    # compute grads: return A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    # standard way to do gradient clipping\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "    step = optimizer.apply_gradients(clipped, global_step = global_step)\n",
    "    print ('step - whtat optimizer.apply_gradients returns', step)\n",
    "    #-----------------\n",
    "    # if using moving average techniques\n",
    "    #-----------------\n",
    "    if enable_parameter_averaging:\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "        maintain_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([step]):\n",
    "            step = tf.group(maintain_averages_op)\n",
    "    else:\n",
    "        step = step\n",
    "    #--------------\n",
    "    # logging\n",
    "    #--------------\n",
    "    logging.info('all parameters:')\n",
    "    logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.global_variables()]))\n",
    "\n",
    "    logging.info('trainable parameters:')\n",
    "    logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.trainable_variables()]))\n",
    "\n",
    "    logging.info('trainable parameter count:')\n",
    "    logging.info(str(np.sum(np.prod(shape_of_tensor(var)) for var in tf.trainable_variables())))\n",
    "    return grads, clipped , step, global_step, learning_rate_var\n",
    "\n",
    "def save(step, averaged=False):\n",
    "    '''\n",
    "    save model\n",
    "    \n",
    "    Args:\n",
    "        step: number checkpoint filenames by passing a value of step\n",
    "        averaged:\n",
    "    '''\n",
    "    global saver\n",
    "    global checkpoint_dir\n",
    "    #---------\n",
    "    # determine using which saver object\n",
    "    #---------\n",
    "    saver = saver_averaged if averaged else saver\n",
    "    checkpoint_dir = checkpoint_dir_averaged if averaged else checkpoint_dir\n",
    "    #---------\n",
    "    #checkpoint_dir\n",
    "    #---------\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        logging.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    model_path = os.path.join(checkpoint_dir, 'model')\n",
    "    print ('========model_path in save========', model_path)\n",
    "    logging.info('saving model to {}'.format(model_path))\n",
    "    #saving\n",
    "    saver.save(sess, model_path, global_step=step)\n",
    "    \n",
    "def restore(step=None, averaged=False):\n",
    "    '''\n",
    "    For using warm start technique\n",
    "    '''\n",
    "    global saver\n",
    "    # checkpoint_dir: Directory where checkpoints are saved\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "\n",
    "    saver = saver_averaged if averaged else saver\n",
    "    checkpoint_dir = checkpoint_dir_averaged if averaged else checkpoint_dir\n",
    "    if not step: \n",
    "        model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        print ('========model_path in restore========', model_path)\n",
    "        logging.info('restoring model parameters from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n",
    "    else:\n",
    "        model_path = os.path.join(\n",
    "            checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "        )\n",
    "\n",
    "        logging.info('restoring model from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Case1:Softmax\n",
    "1.輸出3維度的藍色點點\n",
    "3.Softmax\n",
    "\n",
    "\n",
    "Case2:CRF\n",
    "1.輸出3為度的藍色點點\n",
    "2.CRF\n",
    "\n",
    "Prediction\n",
    "取最大維度的值\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "sys.path.append('/home/ld-sgdev/yunrui_li/ner_project/brand_recognition_bio_FE/py_model')\n",
    "from utils import init_logging\n",
    "import logging\n",
    "log_dir = 'log/' # log path\n",
    "init_logging(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_word = 300\n",
    "max_seq_length = 122\n",
    "num_layers = 2\n",
    "hidden_size_cnn = 100\n",
    "k = 3\n",
    "ntags = 3 # n_class\n",
    "USE_PRETRAINED = True\n",
    "trainable_embedding = False\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "gc.collect()\n",
    "with tf.Graph().as_default() as g:\n",
    "    ####################################\n",
    "    # Step1: get input_sequences \n",
    "    ####################################\n",
    "\n",
    "    #------------\n",
    "    # 1-D  \n",
    "    #------------\n",
    "    item_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "    #------------   \n",
    "    # 2-D  \n",
    "    #------------\n",
    "    word_id = tf.placeholder(tf.int32, [None, max_seq_length]) \n",
    "    label = tf.placeholder(tf.int32, [None, max_seq_length]) # [batch_size, num_class]\n",
    "\n",
    "    #------------\n",
    "    # boolean parameter\n",
    "    #------------\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #------------\n",
    "    # word_embedding: get char embeddings matrix\n",
    "    #------------\n",
    "    if embeddings is None:\n",
    "        logging.info('WARNING: randomly initializing word vectors')\n",
    "        word_embeddings = tf.get_variable(\n",
    "        shape = [nwords, dim_word],\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32,\n",
    "        )\n",
    "    else:\n",
    "        word_embeddings = tf.get_variable(\n",
    "        initializer = embeddings, # it will hold the embedding\n",
    "        #shape = [word2vec.shape[0], word2vec.shape[1]], # [num_vocabulary, embeddings_dim]\n",
    "        trainable = trainable_embedding,\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32\n",
    "        )\n",
    "    word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = word_id)\n",
    "    x_word = tf.concat([\n",
    "    word_representation,\n",
    "    # tf_idf:for product_name\n",
    "        ], axis=1) # (?, 122, 300)\n",
    "    \n",
    "    ####################################\n",
    "    # Step2: calculate_outputs \n",
    "    ####################################\n",
    "    \n",
    "    #-------------------------\n",
    "    # NN architecuture-Simple CNN\n",
    "    #-------------------------\n",
    "    print ('Original features : {}'.format(x_word.shape))\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            conv = temporal_convolution_layer(x_word, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "        else:\n",
    "            conv = temporal_convolution_layer(conv, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "            \n",
    "        print ('CNN-{} layer : {}'.format(i, conv.shape))\n",
    "    # output layer (linear)\n",
    "    y_hat = time_distributed_dense_layer(conv, ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "    print ('Output layer : {}'.format(y_hat.shape))\n",
    "    print ('y_true : {}'.format(label.shape))\n",
    "    #--------------\n",
    "    # for second-level model\n",
    "    #--------------\n",
    "    prediction_tensors = {\n",
    "        'item_id':item_id,\n",
    "        'word_id':word_id,\n",
    "        'final_states':conv, # 修改不要全部max_seq_lenghth都存, 只存到history_length的長度(save memory)\n",
    "        'final_predictions':y_hat,\n",
    "    }\n",
    "    \n",
    "    ####################################\n",
    "    # Step3: calculate_loss + optimizing\n",
    "    ####################################\n",
    "    loss,softmax_losses,sequence_mask = sequence_softmax_loss(y = label, y_hat = y_hat, sequence_lengths = history_length, max_sequence_length = max_seq_length)\n",
    "    grads, clipped , step, global_step, learning_rate_var  = update_parameters(loss)\n",
    "    ####################################\n",
    "    # Step4: saving the model \n",
    "    ####################################    \n",
    "    # create saver object\n",
    "    # max_to_keep: indicates the maximum number of recent checkpoint files to keep.\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    if enable_parameter_averaging:\n",
    "        saver_averaged = tf.train.Saver(ema.variables_to_restore(), max_to_keep=1)    \n",
    "\n",
    "    #-------------------------\n",
    "    # standard\n",
    "    #-------------------------\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-test time_distributed_dense_layer\n",
    "-test sequence_softmax_loss\n",
    "-understanding how to determine grad_clip==> empirical\n",
    "-understanding save and restore and fit function, and make them start training\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque # for computing Train/validation losses are averaged over the last loss_averaging_window\n",
    "\n",
    "warm_start_init_step = 0 # If nonzero, model will resume training a restored model beginning at warm_start_init_step.\n",
    "batch_size = 128\n",
    "loss_averaging_window = 10\n",
    "num_validation_batches = 1\n",
    "num_training_steps = 10\n",
    "learning_rate=0.001\n",
    "log_interval = 1\n",
    "min_steps_to_checkpoint =5\n",
    "early_stopping_steps = 10\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    ####################################\n",
    "    # 1. fit\n",
    "    ####################################\n",
    "    if warm_start_init_step:\n",
    "        # continue the optimization at a recent checkpoint instead of having to restart the optimization from the beginning\n",
    "        restore(warm_start_init_step)\n",
    "        step = warm_start_init_step\n",
    "    else:\n",
    "        # start the optimization from the beginning\n",
    "        sess.run(init) # Run the initializer\n",
    "        step = 0\n",
    "        \n",
    "    sess.run(init)\n",
    "    #--------------\n",
    "    # create generator for batch training\n",
    "    #--------------\n",
    "    train_generator = reader.train_batch_generator(batch_size) # it will yield our customized dataframe\n",
    "    val_generator = reader.val_batch_generator(num_validation_batches*batch_size)\n",
    "    \n",
    "    # create deque object (list-like): for only considering last 100 training steps\n",
    "    train_loss_history = deque(maxlen = loss_averaging_window)\n",
    "    val_loss_history = deque(maxlen= loss_averaging_window)\n",
    "\n",
    "    best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "    restarts = 0\n",
    "    while step < num_training_steps:\n",
    "        #-----------------------\n",
    "        # validating \n",
    "        #-----------------------\n",
    "        val_batch_df = next(val_generator) # it's also a generator.\n",
    "\n",
    "        val_feed_dict = {}\n",
    "\n",
    "            \n",
    "        for placeholder_name, data in val_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                if placeholder_name in vars():\n",
    "                    val_feed_dict.update({globals()[placeholder_name]: data})\n",
    "        val_feed_dict.update({learning_rate_var: learning_rate})\n",
    "\n",
    "    \n",
    "        if keep_prob is not None:\n",
    "            val_feed_dict.update({keep_prob: 1.0}) # 在validating 和testing時, keep_prob:1.0\n",
    "\n",
    "        if is_training is not None:\n",
    "            val_feed_dict.update({is_training: False})\n",
    "        # feeding data into the graph we designed\n",
    "        [val_loss] = sess.run(\n",
    "            fetches=[loss],\n",
    "            feed_dict=val_feed_dict\n",
    "        )\n",
    "        print ('val_loss', val_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "     \n",
    "        #-----------------------\n",
    "        # training\n",
    "        #-----------------------\n",
    "        train_batch_df = next(train_generator)\n",
    "        train_feed_dict = {}\n",
    "        \n",
    "        for placeholder_name, data in train_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                #print ('placeholder_name === train', placeholder_name)\n",
    "                if placeholder_name in vars():\n",
    "                    #print ('placeholder_name === train',globals()[placeholder_name])\n",
    "                    train_feed_dict.update({globals()[placeholder_name]: data})\n",
    "\n",
    "        train_feed_dict.update({learning_rate_var: learning_rate})\n",
    "        if keep_prob is not None:\n",
    "            train_feed_dict.update({keep_prob: 1.0})\n",
    "        if is_training is not None:\n",
    "            train_feed_dict.update({is_training: True})\n",
    "\n",
    "        [train_loss] = sess.run(\n",
    "            fetches=[loss],\n",
    "            feed_dict=train_feed_dict\n",
    "        )\n",
    "        print ('train_loss', train_loss)\n",
    "        train_loss_history.append(train_loss)\n",
    "    \n",
    "        #----------\n",
    "        # logging: log the training and validating loss every log_interval training steps\n",
    "        #----------\n",
    "        if step % log_interval == 0:\n",
    "            # compute average batch training loss over the last loss_averaging_window steps\n",
    "            avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "            print ('avg_train_loss', avg_train_loss)\n",
    "            avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "            print ('avg_val_loss', avg_val_loss)\n",
    "            metric_log = (\n",
    "                \"[[step {:>8}]]     \"\n",
    "                \"[[train]]     loss: {:<12}     \"\n",
    "                \"[[val]]     loss: {:<12}     \"\n",
    "            ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "            logging.info(metric_log)\n",
    "\n",
    "            #------------------\n",
    "            # early stopping\n",
    "            #------------------\n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                # update the best_validation_loss\n",
    "                best_validation_loss = avg_val_loss\n",
    "                best_validation_tstep = step # best_validation_tstep: for recording the model_path in restore function\n",
    "                if step > min_steps_to_checkpoint:\n",
    "                    # saving \n",
    "                    save(step)\n",
    "                    if enable_parameter_averaging:\n",
    "                        save(step, averaged=True)\n",
    "\n",
    "            if step - best_validation_tstep > early_stopping_steps:\n",
    "                #----------------\n",
    "                # handling loss plateaus:  halving earning rate will be repeated more times. \n",
    "                #----------------\n",
    "                if num_restarts is None or restarts >=  num_restarts:\n",
    "                    # stop training\n",
    "                    logging.info('best validation loss of {} at training step {}'.format(\n",
    "                        best_validation_loss, best_validation_tstep))\n",
    "                    logging.info('early stopping - ending training.')\n",
    "\n",
    "                if restarts < num_restarts:\n",
    "                    # 1.the best checkpoint will be restored.\n",
    "                    # 2.keep traning but halving the learning rate and early_stopping_steps\n",
    "                    restore(best_validation_tstep)\n",
    "                    logging.info('halving learning rate')\n",
    "                    learning_rate /= 2.0\n",
    "                    early_stopping_steps /= 2\n",
    "                    step = best_validation_tstep # start from the best_validation_tstep\n",
    "                    restarts += 1\n",
    "        # for terminating while\n",
    "        step += 1\n",
    "        print ('**********step**********', step)\n",
    "    #----------------\n",
    "    # for the case: num_training_steps < = min_steps_to_checkpoint(Basically, it won't happen)\n",
    "    #----------------\n",
    "    if step <= min_steps_to_checkpoint:\n",
    "        best_validation_tstep = step\n",
    "        save(step)\n",
    "        if enable_parameter_averaging:\n",
    "            save(step, averaged=True)\n",
    "\n",
    "    logging.info('num_training_steps reached - ending training')\n",
    "\n",
    "    ####################################\n",
    "    # 2.restore\n",
    "    ####################################\n",
    "    restore()\n",
    "    ####################################\n",
    "    # 3.predict\n",
    "    ####################################\n",
    "    \n",
    "    chunk_size = 3\n",
    "    prediction_dir = 'predictions'\n",
    "    if not os.path.isdir(prediction_dir):\n",
    "        os.makedirs(prediction_dir)\n",
    "    \n",
    "    #--------------------\n",
    "    # for input of 2nd-level\n",
    "    #--------------------\n",
    "    prediction_dict = {}\n",
    "    for tensor_name, value in prediction_tensors.items():\n",
    "        prediction_dict.update({tensor_name: []})\n",
    "        \n",
    "    test_generator = reader.test_batch_generator(chunk_size)\n",
    "    for i, test_batch_df in enumerate(test_generator):\n",
    "        if i % 100 == 0:\n",
    "            print (i*chunk_size)\n",
    "\n",
    "        #--------------\n",
    "        # preparing feed_dict\n",
    "        #--------------\n",
    "        test_feed_dict = {}\n",
    "        for placeholder_name, data in test_batch_df:\n",
    "            if placeholder_name is not None:\n",
    "                #print ('placeholder_name === val', placeholder_name)\n",
    "                if placeholder_name in vars():\n",
    "                    #print ('placeholder_name === val',globals()[placeholder_name])\n",
    "                    test_feed_dict.update({globals()[placeholder_name]: data})\n",
    "        test_feed_dict.update({learning_rate_var: learning_rate})\n",
    "\n",
    "        if keep_prob in vars():\n",
    "            # Pracitcally, After finishing the training, it is important to turn off the dropout during development and testing.\n",
    "            # Otherwise, the prediction of this model is not stable since dropout add uncertainties to it.\n",
    "            test_feed_dict.update({keep_prob: 1.0})\n",
    "        if is_training in vars():\n",
    "            test_feed_dict.update({is_training: False})\n",
    "        print ('test_feed_dict',test_feed_dict)\n",
    "        #-----------\n",
    "        # feeding the data into session\n",
    "        #-----------\n",
    "        tensor_names, tf_tensors = zip(*prediction_tensors.items()) \n",
    "        np_tensors = sess.run(\n",
    "            fetches = list(tf_tensors),\n",
    "            feed_dict=test_feed_dict\n",
    "        )  # return list\n",
    "        \n",
    "        for tensor_name, tensor in zip(list(tensor_names), np_tensors):\n",
    "            print ('=====tensor_name in test =====', tensor_name)\n",
    "            print ('=====tensor in test =====', tensor.shape, tensor)\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "    #-------------\n",
    "    # save the prediction result\n",
    "    #-------------\n",
    "    for tensor_name, tensor in prediction_dict.items():\n",
    "        # tensor: list of array w shape of (batch_size, )\n",
    "        # tensor_name: str\n",
    "        # \n",
    "        np_tensor = np.concatenate(tensor, axis = 0)\n",
    "        save_file = os.path.join(prediction_dir, '{}.npy'.format(tensor_name))\n",
    "        print ('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "        logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "        np.save(save_file, np_tensor)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping based on f1-scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.design a customized plot funtion to observe training/validation loss versus epoch\n",
    "# 2.build tensorflow base model\n",
    "# 3.suvey about how to tune deep learning model using Bayesian Optimization\n",
    "### reference: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以透過is_training 傳入variable決定這個batch要怎麼運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 4:\n",
    "    print (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = set() & set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
