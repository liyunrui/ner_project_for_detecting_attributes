{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "#from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "sys.path.append('../models')\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel # for building our customized\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data = [np.load(os.path.join(data_dir, '{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print ('shape of whole data : {}'.format(self.test_df.shapes()))\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = int(time.time()))\n",
    "\n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, shuffle = True):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size,shuffle = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of whole data : item_id               (24028,)\n",
      "word_id           (24028, 122)\n",
      "history_length        (24028,)\n",
      "label             (24028, 122)\n",
      "dtype: object\n",
      "loaded data\n",
      "number of training example: 21625\n",
      "number of validating example: 2403\n",
      "number of testing example: 24028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "TRACE_CODE = False\n",
    "dr = DataReader(data_dir ='../models/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_utils import temporal_convolution_layer\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../models/cnn') \n",
    "from simple_cnn import simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new run with parameters:\n",
      "{'batch_size': 128,\n",
      " 'checkpoint_dir': 'checkpoints',\n",
      " 'dim_word': 300,\n",
      " 'early_stopping_steps': 30000,\n",
      " 'embeddings': array([[-4.391240e-01,  4.646779e+00, -1.602939e+00, ..., -9.194760e-01,\n",
      "        -8.168350e-01,  3.141200e-02],\n",
      "       [-5.499886e+00,  1.631320e+00, -1.703313e+00, ...,  1.883183e+00,\n",
      "         4.813218e+00, -2.075190e-01],\n",
      "       [ 3.052127e+00,  1.373138e+00,  1.502869e+00, ..., -9.166140e-01,\n",
      "        -1.385339e+00,  9.958170e-01],\n",
      "       ...,\n",
      "       [-2.782620e+00, -4.530440e-01,  3.562689e+00, ...,  3.935201e+00,\n",
      "         9.740690e-01, -1.321913e+00],\n",
      "       [ 1.867000e-03, -2.541924e+00, -4.685680e-01, ...,  1.460600e-02,\n",
      "        -4.422500e-02,  2.060565e+00],\n",
      "       [ 1.605634e+00,  2.821861e+00, -3.930379e+00, ...,  1.326763e+00,\n",
      "         2.236029e+00,  2.455661e+00]], dtype=float32),\n",
      " 'enable_parameter_averaging': False,\n",
      " 'filter_widths': 3,\n",
      " 'grad_clip': 5,\n",
      " 'hidden_size_cnn': 300,\n",
      " 'keep_prob_scalar': 1.0,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_dir': 'logs',\n",
      " 'log_interval': 20,\n",
      " 'loss_averaging_window': 100,\n",
      " 'min_steps_to_checkpoint': 5000,\n",
      " 'ntags': 3,\n",
      " 'num_hidden_layers': 2,\n",
      " 'num_restarts': 2,\n",
      " 'num_training_steps': 200000,\n",
      " 'num_validation_batches': 4,\n",
      " 'nwords': 10577,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': 'predictions',\n",
      " 'reader': <__main__.DataReader object at 0x7f4713ec99b0>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'trainable_embedding': False,\n",
      " 'warm_start_init_step': 0}\n",
      "all parameters:\n",
      "[('word_embeddings:0', [10577, 300]),\n",
      " ('cnn-0/weights:0', [3, 300, 300]),\n",
      " ('cnn-0/biases:0', [300]),\n",
      " ('cnn-1/weights:0', [3, 300, 300]),\n",
      " ('cnn-1/biases:0', [300]),\n",
      " ('output-layer/weights:0', [300, 3]),\n",
      " ('output-layer/biases:0', [3]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('cnn-0/weights/Adam:0', [3, 300, 300]),\n",
      " ('cnn-0/weights/Adam_1:0', [3, 300, 300]),\n",
      " ('cnn-0/biases/Adam:0', [300]),\n",
      " ('cnn-0/biases/Adam_1:0', [300]),\n",
      " ('cnn-1/weights/Adam:0', [3, 300, 300]),\n",
      " ('cnn-1/weights/Adam_1:0', [3, 300, 300]),\n",
      " ('cnn-1/biases/Adam:0', [300]),\n",
      " ('cnn-1/biases/Adam_1:0', [300]),\n",
      " ('output-layer/weights/Adam:0', [300, 3]),\n",
      " ('output-layer/weights/Adam_1:0', [300, 3]),\n",
      " ('output-layer/biases/Adam:0', [3]),\n",
      " ('output-layer/biases/Adam_1:0', [3])]\n",
      "trainable parameters:\n",
      "[('cnn-0/weights:0', [3, 300, 300]),\n",
      " ('cnn-0/biases:0', [300]),\n",
      " ('cnn-1/weights:0', [3, 300, 300]),\n",
      " ('cnn-1/biases:0', [300]),\n",
      " ('output-layer/weights:0', [300, 3]),\n",
      " ('output-layer/biases:0', [3])]\n",
      "trainable parameter count:\n",
      "541503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n",
      "built graph\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "TRACE_CODE = False\n",
    "dim_word = 300\n",
    "trainable_embedding = False\n",
    "USE_PRETRAINED = True\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "base_dir = ''\n",
    "# load pre-trained word embedding\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n",
    "\n",
    "nn = simple_cnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "    optimizer='adam',\n",
    "    learning_rate =0.001,\n",
    "    hidden_size_cnn = 300,\n",
    "    filter_widths= 3,\n",
    "    num_hidden_layers =2,\n",
    "    ntags = 3,\n",
    "    batch_size = 128,\n",
    "    dim_word = 300,\n",
    "    nwords = nwords,\n",
    "    trainable_embedding = False,\n",
    "    embeddings = embeddings,\n",
    "    num_training_steps=200000,\n",
    "    early_stopping_steps=30000,\n",
    "    warm_start_init_step = 0, # for some case, we don't want to train the model from the beginning\n",
    "    regularization_constant = 0.0,\n",
    "    keep_prob = 1.0,\n",
    "    enable_parameter_averaging = False,\n",
    "    num_restarts = 2,\n",
    "    min_steps_to_checkpoint = 5000,\n",
    "    log_interval = 20,\n",
    "    num_validation_batches = 4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1:We first need to define the valid search-ranges or search-dimensions for each of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "# We then combine all these search-dimensions into a list.\n",
    "search_space = [\n",
    "    dim_learning_rate,\n",
    "#     dim_num_dense_layers,\n",
    "#     dim_num_dense_nodes,\n",
    "#     dim_activation\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: choosing starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is helpful to start the search for hyper-parameters with a decent choice that we have found by hand-tuning. \n",
    "default_parameters = [1e-5, 1, 16, 'relu']\n",
    "default_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Define the function that evaluates its performance on the validation-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(learning_rate):\n",
    "    '''log the training-progress for all parameter-combinations'''\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./logs/lr_{}_layers_{}_nodes_{}_{}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(\n",
    "                       learning_rate,\n",
    "#                        num_dense_layers,\n",
    "#                        num_dense_nodes,\n",
    "#                        activation\n",
    "    )\n",
    "\n",
    "    return log_dir\n",
    "\n",
    "# function decorator @use_named_args which wraps the fitness function so that it can be called with all the parameters as a single list, for example: fitness(x=[1e-4, 3, 256, 'relu']). \n",
    "# This is the calling-style skopt uses internally.\n",
    "@use_named_args(search_space)\n",
    "def fitness(learning_rate):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {}'.format(learning_rate))\n",
    "#     print('num_dense_layers:', num_dense_layers)\n",
    "#     print('num_dense_nodes:', num_dense_nodes)\n",
    "#     print('activation:', activation)\n",
    "    #-------------------------\n",
    "    # Customized model: Create the neural network with these hyper-parameters.\n",
    "    #-------------------------\n",
    "    # trying to using tensorflow model\n",
    "#     model = create_model(learning_rate=learning_rate,\n",
    "#                          num_dense_layers=num_dense_layers,\n",
    "#                          num_dense_nodes=num_dense_nodes,\n",
    "#                          activation=activation)\n",
    "    nn = simple_cnn(\n",
    "        reader=dr,\n",
    "        log_dir=os.path.join(base_dir, 'logs'),\n",
    "        checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "        prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "        optimizer='adam',\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_size_cnn = 300,\n",
    "        filter_widths= 3,\n",
    "        num_hidden_layers =2,\n",
    "        ntags = 3,\n",
    "        batch_size = 128,\n",
    "        dim_word = 300,\n",
    "        nwords = nwords,\n",
    "        trainable_embedding = False,\n",
    "        embeddings = embeddings,\n",
    "        num_training_steps=200000,\n",
    "        early_stopping_steps=30000,\n",
    "        warm_start_init_step = 0, # for some case, we don't want to train the model from the beginning\n",
    "        regularization_constant = 0.0,\n",
    "        keep_prob = 1.0,\n",
    "        enable_parameter_averaging = False,\n",
    "        num_restarts = 2,\n",
    "        min_steps_to_checkpoint = 5000,\n",
    "        log_interval = 20,\n",
    "        num_validation_batches = 4,\n",
    "    )\n",
    "\n",
    "#     # Dir-name for the TensorBoard log-files.\n",
    "#     log_dir = log_dir_name(learning_rate, num_dense_layers,\n",
    "#                            num_dense_nodes, activation)\n",
    "    log_dir = log_dir_name(learning_rate)\n",
    "    #-------------------------\n",
    "    # model training \n",
    "    #-------------------------\n",
    "#     history = model.fit(x=data.train.images,\n",
    "#                         y=data.train.labels,\n",
    "#                         epochs=3,\n",
    "#                         batch_size=128,\n",
    "#                         validation_data=validation_data,\n",
    "#                         callbacks=[callback_log])\n",
    "    nn.fit() # need a method return loss on the validation-set create evaluatio(val_set)\n",
    "#     #-------------------------\n",
    "#     # Get the loss(classification accuracy) on the validation-set\n",
    "#     # loss = objective_fun()\n",
    "#     #-------------------------\n",
    "#     accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "#     # Print the classification accuracy.\n",
    "#     print()\n",
    "#     print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "#     print()\n",
    "\n",
    "#     # Save the model if it improves on the best-found performance.\n",
    "#     # We use the global keyword so we update the variable outside\n",
    "#     # of this function.\n",
    "#     global best_accuracy # best_loss\n",
    "\n",
    "#     # If the classification accuracy of the saved model is improved ...\n",
    "#     if accuracy > best_accuracy:\n",
    "#         # Save the new model to harddisk.\n",
    "#         model.save(path_best_model)\n",
    "        \n",
    "#         # Update the classification accuracy.\n",
    "#         best_accuracy = accuracy # best_loss\n",
    "\n",
    "#     # Delete the Keras model with these hyper-parameters from memory.\n",
    "#     del model\n",
    "    \n",
    "#     return -accuracy # loss\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
