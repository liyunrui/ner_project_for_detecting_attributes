{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "sys.path.append('../models')\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel # for building our customized\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "class DataReader(object):\n",
    "    '''for reading data'''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'item_id',\n",
    "            'word_id',\n",
    "            'history_length',\n",
    "            'label'\n",
    "        ]\n",
    "        #-----------------\n",
    "        # loading data\n",
    "        #-----------------\n",
    "        if TRACE_CODE == True:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}_0.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        else:\n",
    "            data_train = [np.load(os.path.join(data_dir, 'train/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_val = [np.load(os.path.join(data_dir, 'val/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "            data_test = [np.load(os.path.join(data_dir, 'test/{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        #------------------\n",
    "        # For Testing-phase\n",
    "        #------------------\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data_test)\n",
    "        print ('loaded data')\n",
    "        #------------------\n",
    "        # For Training-phase\n",
    "        #------------------\n",
    "        self.train_df = DataFrame(columns=data_cols, data=data_train)\n",
    "        self.val_df = DataFrame(columns=data_cols, data=data_val)\n",
    "\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        #self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9, random_state = 3)\n",
    "\n",
    "        \n",
    "        print ('number of training example: {}'.format(len(self.train_df)))\n",
    "        print ('number of validating example: {}'.format(len(self.val_df)))\n",
    "        print ('number of testing example: {}'.format(len(self.test_df)))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size, num_epochs=100000, shuffle = True, is_test = False):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            is_test=is_test\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        '''All row in our dataframe need to predicted as input of second-level model'''\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "    \n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        '''\n",
    "        df: customized DataFrame object,\n",
    "        '''\n",
    "        # call our customized DataFrame object method batch_generator\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle = shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        # batch_gen is a generator\n",
    "        for batch in batch_gen:\n",
    "            # what batch_gen yield is also a customized Dataframe object.\n",
    "            if not is_test:\n",
    "                pass\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRACE_CODE = False\n",
    "# dr = DataReader(data_dir ='../models/data/')\n",
    "word_id = np.load(os.path.join('../models/data/train/', '{}.npy'.format('word_id')))\n",
    "history_length = np.load(os.path.join('../models/data/train/', '{}.npy'.format('history_length')))\n",
    "label = np.load(os.path.join('../models/data/train/', '{}.npy'.format('label')))\n",
    "item_id = np.load(os.path.join('../models/data/train/','{}.npy'.format('item_id')))\n",
    "eval_set = np.load(os.path.join('../models/data/train/','{}.npy'.format('eval_set')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180311, 36)\n",
      "(180311, 36)\n",
      "(180311,)\n",
      "(180311,)\n",
      "(180311,)\n"
     ]
    }
   ],
   "source": [
    "print (label.shape)\n",
    "print (word_id.shape)\n",
    "print (history_length.shape)\n",
    "print (item_id.shape)\n",
    "print (eval_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = word_id[:2]\n",
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = label[:2]\n",
    "y_true.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = history_length[:2]\n",
    "length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3969, 12161, 23290,  9553,  3353,  3969, 21633,  3882,  9858,\n",
       "         9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,\n",
       "         9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,\n",
       "         9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858],\n",
       "       [ 3969,  7651,  6606, 25149,  8289, 21633, 12640, 23290, 24435,\n",
       "        17761, 21855,  9858,  9858,  9858,  9858,  9858,  9858,  9858,\n",
       "         9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,\n",
       "         9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858,  9858]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   266,    267,    268, ..., 212782, 212783, 212784], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate, optimizer='adam'):\n",
    "    '''\n",
    "    It's for choosing optimizer given learning rate.\n",
    "    '''\n",
    "    if optimizer == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    elif soptimizer == 'rms':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "    else:\n",
    "        # assert is a good way to tell other how to use this function for bug happening.\n",
    "        #-------\n",
    "        # standard way to pring the error\n",
    "        #-------\n",
    "        assert False, 'optimizer must be adam, adagrad, sgd, or rms'\n",
    "\n",
    "def update_parameters(loss, optimizer = 'adam'):\n",
    "    '''\n",
    "    It's for optimizing and logging training parameters\n",
    "    \n",
    "    1.using gradient clipping to avoid gradient explosion and vanishment.\n",
    "    \n",
    "    Gradient clipping is most common in recurrent neural networks. \n",
    "    When gradients are being propagated back in time, they can vanish \n",
    "    because they they are continuously multiplied by numbers less than one.\n",
    "    This is called the vanishing gradient problem. \n",
    "    This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, \n",
    "    This is solved by residual connections. \n",
    "    On the other hand, you can have exploding gradients too. \n",
    "    This is when they get exponentially large from being multiplied by numbers larger \n",
    "    than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    grad_clip = 5 # Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "    regularization_constant = 0.1 # Regularization constant applied to all trainable parameters.\n",
    "    enable_parameter_averaging = False # If true, model saves exponential weighted averages of parameters to separate checkpoint file.\n",
    "    global_step = tf.Variable(0, trainable = False) # Optional Variable to increment by one after the variables have been updated.\n",
    "    learning_rate_var = tf.Variable(0.0, trainable = False)\n",
    "    \n",
    "    #----------------\n",
    "    # for understanding regularization\n",
    "    #----------------\n",
    "    trainable_variables_1 = tf.trainable_variables()[0]\n",
    "    square_1 = tf.square(trainable_variables_1)\n",
    "    sum_1 = tf.reduce_sum(square_1)\n",
    "    sqrt = tf.sqrt(sum_1)\n",
    "    #-----------------\n",
    "    # we can customized our regularization on the parameters we like\n",
    "    #-----------------\n",
    "    if regularization_constant != 0:\n",
    "        # l2_norm: is a 0-D tensor. \n",
    "        # we do l2-norm on each trainable's parameters.\n",
    "        l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()]) # Returns list including all variables created with trainable=True\n",
    "        # the smaller the loss is, the better do finish overfitting \n",
    "        loss = loss + regularization_constant*l2_norm\n",
    "    #-----------------\n",
    "    # optimizing\n",
    "    #-----------------\n",
    "    # define the optimizer\n",
    "    optimizer = get_optimizer(learning_rate_var, optimizer=optimizer)\n",
    "    # compute grads: return A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "    grads = optimizer.compute_gradients(loss)\n",
    "    # standard way to do gradient clipping\n",
    "    clipped = [(tf.clip_by_value(g, -grad_clip, grad_clip), v_) for g, v_ in grads]\n",
    "    step = optimizer.apply_gradients(clipped, global_step = global_step)\n",
    "    print ('step - whtat optimizer.apply_gradients returns', step)\n",
    "    #-----------------\n",
    "    # if using moving average techniques\n",
    "    #-----------------\n",
    "    if enable_parameter_averaging:\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "        maintain_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([step]):\n",
    "            step = tf.group(maintain_averages_op)\n",
    "    else:\n",
    "        step = step\n",
    "#     #--------------\n",
    "#     # logging\n",
    "#     #--------------\n",
    "#     logging.info('all parameters:')\n",
    "#     logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.global_variables()]))\n",
    "\n",
    "#     logging.info('trainable parameters:')\n",
    "#     logging.info(pp.pformat([(var.name, shape_of_tensor(var)) for var in tf.trainable_variables()]))\n",
    "\n",
    "#     logging.info('trainable parameter count:')\n",
    "#     logging.info(str(np.sum(np.prod(shape_of_tensor(var)) for var in tf.trainable_variables())))\n",
    "    return learning_rate_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_utils import temporal_convolution_layer\n",
    "from tf_utils import time_distributed_dense_layer\n",
    "from tf_utils import sequence_softmax_loss\n",
    "from tf_utils import sequence_evaluation_metric\n",
    "from data_utils import get_glove_vectors\n",
    "from data_utils import load_vocab_and_return_word_to_id_dict\n",
    "from tf_utils import shape_of_tensor\n",
    "\n",
    "\n",
    "dim_word = 300\n",
    "trainable_embedding = False\n",
    "USE_PRETRAINED = True\n",
    "filename_words_vec = \"../models/data/wordvec/word2vec.npz\".format(dim_word)\n",
    "filename_words_voc = \"../models/data/wordvec/words_vocab.txt\"\n",
    "\n",
    "nwords = len(load_vocab_and_return_word_to_id_dict(filename_words_voc))\n",
    "embeddings = (get_glove_vectors(filename_words_vec) if USE_PRETRAINED else None)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "enable_parameter_averaging = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------\n",
    "# customized evaluation metric\n",
    "#------------------\n",
    "\n",
    "def sequence_evaluation_metric(y, y_hat, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Calculates average evaluation metric on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length], which should be index of label.(y_true)\n",
    "        y_hat: Prediction tensor, [batch size, max_sequence_length], which should be index of predicted label.(y_pred)\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "    Returns:\n",
    "        metrics: dict. metrics[\"f1\"] = 0.72 on each batch\n",
    "    \"\"\"\n",
    "    #---------------\n",
    "    # calculate\n",
    "    #---------------\n",
    "    # make y_true where tagging of each timestep is zero become False. Otherwise, True.\n",
    "    y_ = tf.reduce_sum(y, axis = 1)\n",
    "    y_ = tf.not_equal(y_, tf.zeros(tf.shape(y_), dtype = tf.int32)) # batch_szie\n",
    "\n",
    "    # returns a boolean mask tensor for the first N positions of each cell.\n",
    "    sequence_mask = tf.sequence_mask(lengths = sequence_lengths, maxlen=max_seq_length) # (?, max_sequence_length)\n",
    "    # convert boolean into 1 or 0\n",
    "    sequence_mask = tf.cast(sequence_mask, tf.float32) # (?, max_sequence_length)\n",
    "\n",
    "    correct_preds = tf.equal(y, y_hat) # Returns boolean tensor where the truth value of (x == y) element-wise.\n",
    "    # correct_preds\n",
    "    correct_preds = tf.cast(tf.logical_and(y_, tf.reduce_all(correct_preds, axis = 1)), tf.float32)\n",
    "    # calculate scalar\n",
    "    correct_preds = tf.cast(tf.count_nonzero(correct_preds, axis = None), tf.float32)\n",
    "    total_correct = tf.cast(tf.count_nonzero(tf.count_nonzero(y, axis = 1), axis = None), tf.float32)\n",
    "    total_preds = tf.cast(tf.count_nonzero(tf.count_nonzero(y_hat, axis = 1), axis = None), tf.float32) # 0-D, number of our rediction which is non-zero\n",
    "\n",
    "    #---------------\n",
    "    # output\n",
    "    #---------------\n",
    "    p = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_preds, lambda: tf.zeros(shape=[]))\n",
    "    r = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_correct, lambda: tf.zeros(shape=[]))\n",
    "    f1 = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : 2 * p * r / (p + r), lambda: tf.zeros(shape=[]))\n",
    "    acc = correct_preds/ tf.cast(tf.shape(y_)[0], dtype = tf.float32) # correct_preds/ batch_size\n",
    "\n",
    "    #return { \"prediction\": p, \"recall\": r, \"f1\": f1}\n",
    "    return f1,p,r,y_,correct_preds,total_correct,total_preds,acc, sequence_mask,y\n",
    "    #return {\"acc\": 100*acc, \"prediction\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def sequence_evaluation_metric(y, y_hat, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Calculates average evaluation metric on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length], which should be index of label.(y_true)\n",
    "        y_hat: Prediction tensor, [batch size, max_sequence_length], which should be index of predicted label.(y_pred)\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "    Returns:\n",
    "        metrics: dict. metrics[\"f1\"] = 0.72 on each batch\n",
    "    \"\"\"\n",
    "    #---------------\n",
    "    # calculate\n",
    "    #---------------\n",
    "    \n",
    "    # step1: element-wise comparison between y_true and y_pred\n",
    "    correct_preds = tf.cast(tf.equal(y, y_hat), tf.float32) # Returns boolean tensor where the truth value of (x == y) element-wise.\n",
    "    # returns a boolean mask tensor for the first N positions of each cell.\n",
    "    sequence_mask = tf.sequence_mask(lengths = sequence_lengths, maxlen=max_seq_length) # (?, max_sequence_length)\n",
    "    # convert boolean into 1 or 0\n",
    "    sequence_mask = tf.cast(sequence_mask, tf.float32) # (?, max_sequence_length)\n",
    "\n",
    "    # step2: dynamically consider the case their timestep do not pass his history lenghth\n",
    "    correct_preds = correct_preds*sequence_mask\n",
    "    \n",
    "    # step3: tf.reduce_sum on temporal axi\n",
    "    correct_preds = tf.reduce_sum(correct_preds, axis = 1)\n",
    "    # step4: element-wise comparison between correct_preds and sequence_lengths\n",
    "    correct_preds = tf.cast(tf.equal(tf.cast(correct_preds, tf.int32), sequence_lengths), tf.float32) # (batch_size,), which element representing if this sentence is correct or not\n",
    "    # calculate scalar\n",
    "    correct_preds = tf.cast(tf.count_nonzero(correct_preds, axis = None), tf.float32)\n",
    "    total_correct = tf.cast(tf.count_nonzero(tf.count_nonzero(y, axis = 1), axis = None), tf.float32)\n",
    "    total_preds = tf.cast(tf.count_nonzero(tf.count_nonzero(y_hat, axis = 1), axis = None), tf.float32) # 0-D, number of our rediction which is non-zero\n",
    "\n",
    "    #---------------\n",
    "    # output\n",
    "    #---------------\n",
    "    p = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_preds, lambda: tf.zeros(shape=[]))\n",
    "    r = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : correct_preds/total_correct, lambda: tf.zeros(shape=[]))\n",
    "    f1 = tf.cond(tf.greater(correct_preds, tf.zeros(shape=[])), lambda : 2 * p * r / (p + r), lambda: tf.zeros(shape=[]))\n",
    "    acc = correct_preds/ tf.cast(tf.shape(y)[0], dtype = tf.float32) # correct_preds/ batch_size\n",
    "    return {\"acc\": 100*acc, \"prediction\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "    #return y, y_hat, correct_preds, sequence_mask, p,r,f1,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------\n",
    "# for debugging\n",
    "#-------------\n",
    "max_seq_length = 36\n",
    "num_layers = 2\n",
    "hidden_size_cnn = 300\n",
    "k = 3\n",
    "ntags = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features : (?, 36, 300)\n",
      "CNN-0 layer : (?, 36, 300)\n",
      "CNN-1 layer : (?, 36, 300)\n",
      "Output layer : (?, 36, 3)\n",
      "y_true : (?, 36)\n",
      "step - whtat optimizer.apply_gradients returns name: \"Adam\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Variable\"\n",
      "input: \"Adam/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Variable\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    ####################################\n",
    "    # Step1: get input_sequences \n",
    "    ####################################\n",
    "\n",
    "    #------------\n",
    "    # 1-D  \n",
    "    #------------\n",
    "    item_id = tf.placeholder(tf.int32, [None])\n",
    "    history_length = tf.placeholder(tf.int32, [None]) # It's for arg of lstm model: sequence_length, == len(is_ordered_history)\n",
    "    #------------   \n",
    "    # 2-D  \n",
    "    #------------\n",
    "    word_id = tf.placeholder(tf.int32, [None, max_seq_length]) \n",
    "    label = tf.placeholder(tf.int32, [None, max_seq_length]) # [batch_size, num_class]\n",
    "\n",
    "    #------------\n",
    "    # boolean parameter\n",
    "    #------------\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #------------\n",
    "    # word_embedding: get char embeddings matrix\n",
    "    #------------\n",
    "    if embeddings is None:\n",
    "        logging.info('WARNING: randomly initializing word vectors')\n",
    "        word_embeddings = tf.get_variable(\n",
    "        shape = [nwords, dim_word],\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32,\n",
    "        )\n",
    "    else:\n",
    "        word_embeddings = tf.get_variable(\n",
    "        initializer = embeddings, # it will hold the embedding\n",
    "        #shape = [word2vec.shape[0], word2vec.shape[1]], # [num_vocabulary, embeddings_dim]\n",
    "        trainable = trainable_embedding,\n",
    "        name = 'word_embeddings',\n",
    "        dtype = tf.float32\n",
    "        )\n",
    "    word_representation = tf.nn.embedding_lookup(params = word_embeddings, ids = word_id)\n",
    "    x_word = tf.concat([\n",
    "    word_representation,\n",
    "    # tf_idf:for product_name\n",
    "        ], axis=1) # (?, 122, 300)\n",
    "    \n",
    "    ####################################\n",
    "    # Step2: calculate_outputs \n",
    "    ####################################\n",
    "    \n",
    "    #-------------------------\n",
    "    # NN architecuture-Simple CNN\n",
    "    #-------------------------\n",
    "    print ('Original features : {}'.format(x_word.shape))\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            conv = temporal_convolution_layer(x_word, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "        else:\n",
    "            conv = temporal_convolution_layer(conv, \n",
    "                                       output_units = hidden_size_cnn,\n",
    "                                       convolution_width = k,\n",
    "                                       dilated = False,\n",
    "                                       causal = False,\n",
    "                                       bias=True,\n",
    "                                       activation=None, \n",
    "                                       dropout=None,\n",
    "                                       scope='cnn-{}'.format(i),\n",
    "                                       reuse = False,\n",
    "                                      )\n",
    "            \n",
    "        print ('CNN-{} layer : {}'.format(i, conv.shape))\n",
    "    # output layer (linear)\n",
    "    y_hat = time_distributed_dense_layer(conv, ntags, activation=None, scope='output-layer') # (?, 122, 3)\n",
    "    print ('Output layer : {}'.format(y_hat.shape))\n",
    "    print ('y_true : {}'.format(label.shape))\n",
    "    #--------------\n",
    "    # for second-level model\n",
    "    #--------------\n",
    "    prediction_tensors = {\n",
    "        'item_id':item_id,\n",
    "        'word_id':word_id,\n",
    "        'final_states':conv, # 修改不要全部max_seq_lenghth都存, 只存到history_length的長度(save memory)\n",
    "        'final_predictions':y_hat,\n",
    "    }\n",
    "    \n",
    "    ####################################\n",
    "    # Step3: calculate_loss +evaluation score+ optimizing\n",
    "    ####################################\n",
    "    loss = sequence_softmax_loss(y = label, y_hat = y_hat, sequence_lengths = history_length, max_sequence_length = max_seq_length)\n",
    "    learning_rate_var  = update_parameters(loss)\n",
    "    \n",
    "    \n",
    "    labels_pred = tf.cast(tf.argmax(y_hat, axis= 2),tf.int32) # (?, max_seq_length)\n",
    "    score = sequence_evaluation_metric(y = label, y_hat = labels_pred, sequence_lengths = history_length)['f1']\n",
    "    #score,p,r,y_,correct_preds,total_correct,total_preds,acc,sequence_mask,y = sequence_evaluation_metric(y = label, y_hat = labels_pred, sequence_lengths = history_length)\n",
    "    #y_ture_input, y_pred_input, correct_preds,sequence_mask, p,r,f1,acc = sequence_evaluation_metric_1(y = label, y_hat = labels_pred, sequence_lengths = history_length)\n",
    "\n",
    "#     ####################################\n",
    "#     # Step4: saving the model \n",
    "#     ####################################    \n",
    "#     # create saver object\n",
    "#     # max_to_keep: indicates the maximum number of recent checkpoint files to keep.\n",
    "#     saver = tf.train.Saver(max_to_keep = 1)\n",
    "#     if enable_parameter_averaging:\n",
    "#         saver_averaged = tf.train.Saver(ema.variables_to_restore(), max_to_keep=1)    \n",
    "\n",
    "    #-------------------------\n",
    "    # standard\n",
    "    #-------------------------\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque # for computing Train/validation losses are averaged over the last loss_averaging_window\n",
    "import tensorflow as tf\n",
    "warm_start_init_step = 0 # If nonzero, model will resume training a restored model beginning at warm_start_init_step.\n",
    "batch_size = 128\n",
    "loss_averaging_window = 10\n",
    "num_validation_batches = 1\n",
    "num_training_steps = 10\n",
    "learning_rate=0.001\n",
    "log_interval = 1\n",
    "min_steps_to_checkpoint =1\n",
    "early_stopping_steps = 10\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "with tf.Session(graph=g, config = config) as sess:\n",
    "    ####################################\n",
    "    # 1. fit\n",
    "    ####################################\n",
    "    if warm_start_init_step:\n",
    "        # continue the optimization at a recent checkpoint instead of having to restart the optimization from the beginning\n",
    "        restore(warm_start_init_step)\n",
    "        step = warm_start_init_step\n",
    "    else:\n",
    "        # start the optimization from the beginning\n",
    "        sess.run(init) # Run the initializer\n",
    "        step = 0\n",
    "     \n",
    "    score_ = sess.run(fetches = score, feed_dict = {word_id:sentence,\n",
    "                                                 label: y_true,\n",
    "                                                 history_length: length})\n",
    "#     y_ture_input_, y_pred_input_,correct_preds_,sequence_mask_, history_length_, p_,r_,f1_,acc_ = sess.run(fetches = [y_ture_input, y_pred_input,correct_preds,sequence_mask,history_length, p,r,f1,acc], \n",
    "#                                                  feed_dict = {word_id:sentence,\n",
    "#                                                  label: y_true,\n",
    "#                                                  history_length: length}) \n",
    "#                                                                                                            labels_pred, \n",
    "#     label_, labels_pred_, score,p_,r_,y__,correct_preds_,total_correct_,total_preds_,acc_,history_length,sequence_mask_,y_ = sess.run(fetches = [label, \n",
    "#                                                                                                            labels_pred, \n",
    "#                                                                                                            score,\n",
    "#                                                                                                            p,\n",
    "#                                                                                                            r,\n",
    "#                                                                                                            y_,\n",
    "#                                                                                                            correct_preds,\n",
    "#                                                                                                            total_correct,\n",
    "#                                                                                                            total_preds,\n",
    "#                                                                                                            acc, \n",
    "#                                                                                                            history_length,\n",
    "#                                                                                                            sequence_mask,y\n",
    "                                                                                                        \n",
    "#                                                                                                                                              ], \n",
    "#                                     feed_dict = {word_id:sentence,\n",
    "#                                                  label: y_true,\n",
    "#                                                  history_length: length}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.if 要把n_tag改成2, 因為class越多, 本來難度就越高啊!!!\n",
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ture_input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
